#include "torch/csrc/autograd/VariableTypeUtils.h"
#include "torch/csrc/autograd/generated/VariableType.h"
#include "torch/csrc/autograd/FunctionsManual.h"

#include <ATen/RedispatchFunctions.h>
#include <c10/core/impl/TorchDispatchModeTLS.h>
#include <ATen/core/TorchDispatchUtils.h>
#include <torch/library.h>

#include <ATen/SparseCsrTensorUtils.h>


// @generated by torchgen/gen.py from VariableType.cpp

// NOTE [Sharded File]: on this file's split-into-shards state
//
// Back in the good old days, VariableType.cpp was generated as one
// file with every function in it, and everything was great and
// simple.
//
// However, this file was also very large (over 36,000 lines), and
// compiling it was very slow, and in fact was a significant
// bottleneck for incremental rebuilds. To address this, we now
// generate the file split across multiple shards, named
// VariableType_0.cpp and so on, which can be compiled in parallel.
//
// For ease of inspection and debugging, so that it's not necessary to
// go rooting around in multiple files, we also generate all the
// functions together in VariableTypeEverything.cpp. This generated
// file is only for convenience; it's not actually used in the
// build. If the file you're looking at now is one of the shards, you
// may want to switch over to the Everything variant to make you
// grepping smoother.

using namespace at;
using namespace torch::autograd::generated;
using namespace torch::autograd::generated::details;


namespace torch::autograd {

namespace VariableType {
namespace{
  C10_UNUSED void reset_grad_accumulator(Variable & self) {
    AutogradMeta* meta = torch::autograd::impl::get_autograd_meta(self);
    if (meta != nullptr) {
      meta->grad_accumulator_.reset();
    }
  }
}

namespace {


at::Tensor _test_autograd_multiple_dispatch_view_copy_AutogradCUDA(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<TestAutogradMultipleDispatchViewBackwardAutogradCUDA0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TestAutogradMultipleDispatchViewBackwardAutogradCUDA0_copy>(new TestAutogradMultipleDispatchViewBackwardAutogradCUDA0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_test_autograd_multiple_dispatch_view_copy", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_test_autograd_multiple_dispatch_view_copy", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_test_autograd_multiple_dispatch_view_copy(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _test_autograd_multiple_dispatch_view_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _test_autograd_multiple_dispatch_view_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_test_autograd_multiple_dispatch_view_copy");
  return result;
}

at::Tensor select_int_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, c10::SymInt index) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<SelectBackwardAutogradNestedTensor0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SelectBackwardAutogradNestedTensor0>(new SelectBackwardAutogradNestedTensor0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->index = index;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::select", "int");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("select", *opt_op, ks, self, dim, index);
    } else {
      at::AutoDispatchBelowAutograd guard;
      return at::redispatch::select_symint(ks & c10::after_autograd_keyset, self_, dim, index);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: select_int");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  return result;
}
void split_with_sizes_copy_out_out_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim, at::TensorList out) {
  auto& self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::split_with_sizes_copy_symint_outf(ks & c10::after_autograd_keyset, self_, split_sizes, dim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with split_with_sizes_copy_out that does not support it because it is an out= function");
}
at::Tensor squeeze_copy_dim_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<SqueezeBackwardAutogradNestedTensor0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqueezeBackwardAutogradNestedTensor0_copy>(new SqueezeBackwardAutogradNestedTensor0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::squeeze_copy", "dim");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("squeeze_copy", *opt_op, ks, self, dim);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::squeeze_copy(ks & c10::after_autograd_keyset, self_, dim);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: squeeze_copy_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: squeeze_copy_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "squeeze_copy");
  return result;
}
at::Tensor squeeze_copy_dims_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<SqueezeBackwardAutogradNestedTensor1_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqueezeBackwardAutogradNestedTensor1_copy>(new SqueezeBackwardAutogradNestedTensor1_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim.vec();
    grad_fn->self_dim = self.dim();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::squeeze_copy", "dims");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("squeeze_copy", *opt_op, ks, self, dim);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::squeeze_copy(ks & c10::after_autograd_keyset, self_, dim);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: squeeze_copy_dims");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: squeeze_copy_dims");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "squeeze_copy");
  return result;
}
at::Tensor & sum_out_IntList_out_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("sum");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("sum");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::sum_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, dtype, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with sum_out that does not support it because it is an out= function");
  return out;
}
at::Tensor view_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ViewBackwardAutogradNestedTensor0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ViewBackwardAutogradNestedTensor0>(new ViewBackwardAutogradNestedTensor0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::view_symint(ks & c10::after_autograd_keyset, self_, size);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: view");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t.view_symint(size);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}

at::Tensor _adaptive_avg_pool3d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<AdaptiveAvgPool3DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AdaptiveAvgPool3DBackward0>(new AdaptiveAvgPool3DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_adaptive_avg_pool3d_symint(ks & c10::after_autograd_keyset, self_, output_size);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _adaptive_avg_pool3d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _adaptive_avg_pool3d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_adaptive_avg_pool3d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_adaptive_avg_pool3d_symint(self_t, output_size);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _coalesce(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<CoalesceBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CoalesceBackward0>(new CoalesceBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_coalesce", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_coalesce", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_coalesce(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _coalesce");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _coalesce");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,::std::vector<at::Tensor>> _cudnn_rnn_backward(c10::DispatchKeySet ks, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, c10::SymIntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask) {
  auto& input_ = unpack(input, "input", 0);
  auto weight_ = unpack(weight, "weight", 1);
  auto& weight_buf_ = unpack(weight_buf, "weight_buf", 3);
  auto& hx_ = unpack(hx, "hx", 4);
  auto& output_ = unpack(output, "output", 6);
  auto& reserve_ = unpack(reserve, "reserve", 20);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, hx, cx, output, grad_output, grad_hy, grad_cy );
  
  check_no_requires_grad(weight_buf, "weight_buf", "_cudnn_rnn_backward");
  check_no_requires_grad(reserve, "reserve", "_cudnn_rnn_backward");
  std::shared_ptr<CudnnRnnBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CudnnRnnBackwardBackward0>(new CudnnRnnBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, hx, cx, output, grad_output, grad_hy, grad_cy ));
    grad_fn->weight_size_ = weight.size();
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  ::std::vector<at::Tensor> result3;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> weight__storage_saved(weight_.size());
  for (const Tensor& tensor : weight_)
    weight__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> weight__impl_saved(weight_.size());
  for (size_t i=0; i<weight_.size(); i++)
    if (weight_[i].defined()) weight__impl_saved[i] = weight_[i].getIntrusivePtr();
  c10::optional<Storage> weight_buf__storage_saved =
    weight_buf_.has_storage() ? c10::optional<Storage>(weight_buf_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight_buf__impl_saved;
  if (weight_buf_.defined()) weight_buf__impl_saved = weight_buf_.getIntrusivePtr();
  c10::optional<Storage> hx__storage_saved =
    hx_.has_storage() ? c10::optional<Storage>(hx_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> hx__impl_saved;
  if (hx_.defined()) hx__impl_saved = hx_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> reserve__storage_saved =
    reserve_.has_storage() ? c10::optional<Storage>(reserve_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> reserve__impl_saved;
  if (reserve_.defined()) reserve__impl_saved = reserve_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input) || isFwGradDefinedTensorList(weight) || isFwGradDefined(weight_buf) || isFwGradDefined(hx) || isFwGradDefined(cx) || isFwGradDefined(output) || isFwGradDefined(grad_output) || isFwGradDefined(grad_hy) || isFwGradDefined(grad_cy) || isFwGradDefined(reserve))) {
      static c10::OperatorName full_name("aten::_cudnn_rnn_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor, ::std::vector<at::Tensor>>>("_cudnn_rnn_backward", *opt_op, ks, input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_cudnn_rnn_backward_symint(ks & c10::after_autograd_keyset, input_, weight_, weight_stride0, weight_buf_, hx_, cx, output_, grad_output, grad_hy, grad_cy, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve_, output_mask);
    }
  })();
  std::tie(result0, result1, result2, result3) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  for (size_t i=0; i<weight_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weight__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(weight_))
      TORCH_INTERNAL_ASSERT(weight__storage_saved[i].value().is_alias_of(weight_[i].storage()));
  }
  for (size_t i=0; i<weight_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weight__impl_saved[i] && !at::impl::tensorlist_has_dispatch(weight_))
      TORCH_INTERNAL_ASSERT(weight__impl_saved[i] == weight_[i].getIntrusivePtr());
  }
  if (weight_buf__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_buf_))
    TORCH_INTERNAL_ASSERT(weight_buf__storage_saved.value().is_alias_of(weight_buf_.storage()));
  if (weight_buf__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_buf_))
    TORCH_INTERNAL_ASSERT(weight_buf__impl_saved == weight_buf_.getIntrusivePtr());
  if (hx__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__storage_saved.value().is_alias_of(hx_.storage()));
  if (hx__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__impl_saved == hx_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (reserve__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(reserve_))
    TORCH_INTERNAL_ASSERT(reserve__storage_saved.value().is_alias_of(reserve_.storage()));
  if (reserve__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(reserve_))
    TORCH_INTERNAL_ASSERT(reserve__impl_saved == reserve_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2, result3 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_cudnn_rnn_backward");
  throw_error_for_complex_autograd(result1, "_cudnn_rnn_backward");
  throw_error_for_complex_autograd(result2, "_cudnn_rnn_backward");
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3));
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,c10::SymInt,c10::SymInt> _efficient_attention_forward(c10::DispatchKeySet ks, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & cu_seqlens_q, const c10::optional<at::Tensor> & cu_seqlens_k, c10::optional<int64_t> max_seqlen_q, c10::optional<int64_t> max_seqlen_k, double dropout_p, int64_t custom_mask_type, bool compute_log_sumexp, c10::optional<double> scale, const c10::optional<at::Tensor> & causal_diagonal, const c10::optional<at::Tensor> & seqlen_k) {
  auto& query_ = unpack(query, "query", 0);
  auto& key_ = unpack(key, "key", 1);
  auto& value_ = unpack(value, "value", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( query, key, value, bias );
  
  check_no_requires_grad(cu_seqlens_q, "cu_seqlens_q", "_efficient_attention_forward");
  check_no_requires_grad(cu_seqlens_k, "cu_seqlens_k", "_efficient_attention_forward");
  check_no_requires_grad(causal_diagonal, "causal_diagonal", "_efficient_attention_forward");
  check_no_requires_grad(seqlen_k, "seqlen_k", "_efficient_attention_forward");
  std::shared_ptr<EfficientAttentionBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<EfficientAttentionBackward0>(new EfficientAttentionBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( query, key, value, bias ));
    grad_fn->bias_ = SavedVariable(bias, false);
    grad_fn->cu_seqlens_k_ = SavedVariable(cu_seqlens_k, false);
    grad_fn->cu_seqlens_q_ = SavedVariable(cu_seqlens_q, false);
    grad_fn->custom_mask_type = custom_mask_type;
    grad_fn->dropout_p = dropout_p;
    grad_fn->key_ = SavedVariable(key, false);
    grad_fn->query_ = SavedVariable(query, false);
    grad_fn->scale = scale;
    grad_fn->value_ = SavedVariable(value, false);
  }
  at::Tensor output;
  at::Tensor logsumexp;
  at::Tensor philox_seed;
  at::Tensor philox_offset;
  c10::SymInt max_seqlen_batch_q;
  c10::SymInt max_seqlen_batch_k;
  #ifndef NDEBUG
  c10::optional<Storage> query__storage_saved =
    query_.has_storage() ? c10::optional<Storage>(query_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> query__impl_saved;
  if (query_.defined()) query__impl_saved = query_.getIntrusivePtr();
  c10::optional<Storage> key__storage_saved =
    key_.has_storage() ? c10::optional<Storage>(key_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> key__impl_saved;
  if (key_.defined()) key__impl_saved = key_.getIntrusivePtr();
  c10::optional<Storage> value__storage_saved =
    value_.has_storage() ? c10::optional<Storage>(value_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> value__impl_saved;
  if (value_.defined()) value__impl_saved = value_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(query) || isFwGradDefined(key) || isFwGradDefined(value) || isFwGradDefined(bias) || isFwGradDefined(cu_seqlens_q) || isFwGradDefined(cu_seqlens_k) || isFwGradDefined(causal_diagonal) || isFwGradDefined(seqlen_k))) {
      static c10::OperatorName full_name("aten::_efficient_attention_forward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::SymInt, c10::SymInt>>("_efficient_attention_forward", *opt_op, ks, query, key, value, bias, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, custom_mask_type, compute_log_sumexp, scale, causal_diagonal, seqlen_k);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_efficient_attention_forward(ks & c10::after_autograd_keyset, query_, key_, value_, bias, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, custom_mask_type, compute_log_sumexp, scale, causal_diagonal, seqlen_k);
    }
  })();
  std::tie(output, logsumexp, philox_seed, philox_offset, max_seqlen_batch_q, max_seqlen_batch_k) = std::move(_tmp);
  #ifndef NDEBUG
  if (query__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(query_))
    TORCH_INTERNAL_ASSERT(query__storage_saved.value().is_alias_of(query_.storage()));
  if (query__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(query_))
    TORCH_INTERNAL_ASSERT(query__impl_saved == query_.getIntrusivePtr());
  if (key__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(key_))
    TORCH_INTERNAL_ASSERT(key__storage_saved.value().is_alias_of(key_.storage()));
  if (key__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(key_))
    TORCH_INTERNAL_ASSERT(key__impl_saved == key_.getIntrusivePtr());
  if (value__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__storage_saved.value().is_alias_of(value_.storage()));
  if (value__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__impl_saved == value_.getIntrusivePtr());
  if (output.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output)) {
    TORCH_INTERNAL_ASSERT(output.storage().use_count() == 1, "function: _efficient_attention_forward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output))
    TORCH_INTERNAL_ASSERT(output.use_count() <= 1, "function: _efficient_attention_forward");
  if (logsumexp.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(logsumexp)) {
    TORCH_INTERNAL_ASSERT(logsumexp.storage().use_count() == 1, "function: _efficient_attention_forward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(logsumexp))
    TORCH_INTERNAL_ASSERT(logsumexp.use_count() <= 1, "function: _efficient_attention_forward");
  if (philox_seed.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_seed)) {
    TORCH_INTERNAL_ASSERT(philox_seed.storage().use_count() == 1, "function: _efficient_attention_forward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_seed))
    TORCH_INTERNAL_ASSERT(philox_seed.use_count() <= 1, "function: _efficient_attention_forward");
  if (philox_offset.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_offset)) {
    TORCH_INTERNAL_ASSERT(philox_offset.storage().use_count() == 1, "function: _efficient_attention_forward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_offset))
    TORCH_INTERNAL_ASSERT(philox_offset.use_count() <= 1, "function: _efficient_attention_forward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( output ), grad_fn);
  }
  throw_error_for_complex_autograd(output, "_efficient_attention_forward");
  if (grad_fn) {
    grad_fn->logsumexp_ = SavedVariable(logsumexp, true);
    grad_fn->max_seqlen_batch_k = max_seqlen_batch_k;
    grad_fn->max_seqlen_batch_q = max_seqlen_batch_q;
    grad_fn->output_ = SavedVariable(output, true);
    grad_fn->philox_offset_ = SavedVariable(philox_offset, true);
    grad_fn->philox_seed_ = SavedVariable(philox_seed, true);
  }
  return std::make_tuple(std::move(output), std::move(logsumexp), std::move(philox_seed), std::move(philox_offset), std::move(max_seqlen_batch_q), std::move(max_seqlen_batch_k));
}
at::Tensor _fft_r2c(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<FftR2CBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FftR2CBackward0>(new FftR2CBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim.vec();
    grad_fn->normalization = normalization;
    grad_fn->onesided = onesided;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_fft_r2c(ks & c10::after_autograd_keyset, self_, dim, normalization, onesided);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _fft_r2c");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _fft_r2c");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_fft_r2c(self_t, dim, normalization, onesided);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::vector<at::Tensor> _foreach_addcmul_Scalar(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
  auto self_ = unpack(self, "self", 0);
  auto tensor1_ = unpack(tensor1, "tensor1", 1);
  auto tensor2_ = unpack(tensor2, "tensor2", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, tensor1, tensor2 );
  
  TORCH_CHECK(
      self.size() == tensor1.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      tensor1.size());
  TORCH_CHECK(
      self.size() == tensor2.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      tensor2.size());
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(tensor1[i]) || isFwGradDefined(tensor2[i]);
  }
  std::shared_ptr<ForeachAddcmulBackward0Scalar> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachAddcmulBackward0Scalar>(new ForeachAddcmulBackward0Scalar(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, tensor1, tensor2 ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->tensor1_ = make_saved_variable_list(tensor1, false);
    grad_fn->tensor2_ = make_saved_variable_list(tensor2, false);
    grad_fn->value = value;
    grad_fn->self_size_ = self.size();
    grad_fn->tensor1_size_ = tensor1.size();
    grad_fn->tensor2_size_ = tensor2.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor1__storage_saved(tensor1_.size());
  for (const Tensor& tensor : tensor1_)
    tensor1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor1__impl_saved(tensor1_.size());
  for (size_t i=0; i<tensor1_.size(); i++)
    if (tensor1_[i].defined()) tensor1__impl_saved[i] = tensor1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor2__storage_saved(tensor2_.size());
  for (const Tensor& tensor : tensor2_)
    tensor2__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor2__impl_saved(tensor2_.size());
  for (size_t i=0; i<tensor2_.size(); i++)
    if (tensor2_[i].defined()) tensor2__impl_saved[i] = tensor2_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_addcmul(ks & c10::after_autograd_keyset, self_, tensor1_, tensor2_, value);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__storage_saved[i].value().is_alias_of(tensor1_[i].storage()));
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__impl_saved[i] == tensor1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__storage_saved[i].value().is_alias_of(tensor2_[i].storage()));
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__impl_saved[i] == tensor2_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto tensor1_t_raw = toNonOptFwGrad(tensor1[i]);
        auto tensor1_tensor = toNonOptTensor(tensor1[i]);
        auto tensor1_t = (tensor1_t_raw.defined() || !tensor1_tensor.defined())
          ? tensor1_t_raw : at::_efficientzerotensor(tensor1_tensor.sizes(), tensor1_tensor.options());
        auto tensor1_p = toNonOptPrimal(tensor1[i]);
        auto tensor2_t_raw = toNonOptFwGrad(tensor2[i]);
        auto tensor2_tensor = toNonOptTensor(tensor2[i]);
        auto tensor2_t = (tensor2_t_raw.defined() || !tensor2_tensor.defined())
          ? tensor2_t_raw : at::_efficientzerotensor(tensor2_tensor.sizes(), tensor2_tensor.options());
        auto tensor2_p = toNonOptPrimal(tensor2[i]);
        result_new_fw_grad_opts[i] = self_t + maybe_multiply(tensor1_t * tensor2_p, value) + maybe_multiply(tensor2_t * tensor1_p, value);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_addcmul_ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  auto tensor1_ = unpack(tensor1, "tensor1", 1);
  auto tensor2_ = unpack(tensor2, "tensor2", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, tensor1, tensor2 );
  
  TORCH_CHECK(
      self.size() == tensor1.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      tensor1.size());
  TORCH_CHECK(
      self.size() == tensor2.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      tensor2.size());
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(tensor1[i]) || isFwGradDefined(tensor2[i]);
  }
  std::shared_ptr<ForeachAddcmulBackward0ScalarList> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachAddcmulBackward0ScalarList>(new ForeachAddcmulBackward0ScalarList(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, tensor1, tensor2 ));
    grad_fn->scalars = scalars.vec();
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->tensor1_ = make_saved_variable_list(tensor1, false);
    grad_fn->tensor2_ = make_saved_variable_list(tensor2, false);
    grad_fn->self_size_ = self.size();
    grad_fn->tensor1_size_ = tensor1.size();
    grad_fn->tensor2_size_ = tensor2.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor1__storage_saved(tensor1_.size());
  for (const Tensor& tensor : tensor1_)
    tensor1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor1__impl_saved(tensor1_.size());
  for (size_t i=0; i<tensor1_.size(); i++)
    if (tensor1_[i].defined()) tensor1__impl_saved[i] = tensor1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor2__storage_saved(tensor2_.size());
  for (const Tensor& tensor : tensor2_)
    tensor2__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor2__impl_saved(tensor2_.size());
  for (size_t i=0; i<tensor2_.size(); i++)
    if (tensor2_[i].defined()) tensor2__impl_saved[i] = tensor2_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_addcmul(ks & c10::after_autograd_keyset, self_, tensor1_, tensor2_, scalars);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__storage_saved[i].value().is_alias_of(tensor1_[i].storage()));
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__impl_saved[i] == tensor1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__storage_saved[i].value().is_alias_of(tensor2_[i].storage()));
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__impl_saved[i] == tensor2_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto tensor1_t_raw = toNonOptFwGrad(tensor1[i]);
        auto tensor1_tensor = toNonOptTensor(tensor1[i]);
        auto tensor1_t = (tensor1_t_raw.defined() || !tensor1_tensor.defined())
          ? tensor1_t_raw : at::_efficientzerotensor(tensor1_tensor.sizes(), tensor1_tensor.options());
        auto tensor1_p = toNonOptPrimal(tensor1[i]);
        auto tensor2_t_raw = toNonOptFwGrad(tensor2[i]);
        auto tensor2_tensor = toNonOptTensor(tensor2[i]);
        auto tensor2_t = (tensor2_t_raw.defined() || !tensor2_tensor.defined())
          ? tensor2_t_raw : at::_efficientzerotensor(tensor2_tensor.sizes(), tensor2_tensor.options());
        auto tensor2_p = toNonOptPrimal(tensor2[i]);
        result_new_fw_grad_opts[i] = self_t + maybe_multiply(tensor1_t * tensor2_p, scalars[i]) + maybe_multiply(tensor2_t * tensor1_p, scalars[i]);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_asin(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachAsinBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachAsinBackward0>(new ForeachAsinBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_asin(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (self_t.conj() * (-self_p * self_p + 1).rsqrt().conj()).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_asin_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_asin_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_asin_out that does not support it because it is an out= function");
}
void _foreach_ceil_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<CeilBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<CeilBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<CeilBackward0>(new CeilBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_ceil_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((zeros_like(self_t.conj())).conj()) : (zeros_like(self_t.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_ceil_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_ceil_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_ceil_out that does not support it because it is an out= function");
}
::std::vector<at::Tensor> _foreach_cos(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachCosBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachCosBackward0>(new ForeachCosBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_cos(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (self_t.conj() * -self_p.sin().conj()).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_cos_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<CosBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<CosBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<CosBackward0>(new CosBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_cos_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * -original_self_p.sin().conj()).conj()) : (original_self_t.conj() * -original_self_p.sin().conj()).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
::std::vector<at::Tensor> _foreach_div_Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachDivBackward1Scalar> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachDivBackward1Scalar>(new ForeachDivBackward1Scalar(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->scalar = scalar;
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_div(ks & c10::after_autograd_keyset, self_, scalar);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = self_t / scalar;
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_div_List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::shared_ptr<ForeachDivBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachDivBackward0>(new ForeachDivBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = make_saved_variable_list(other, false);
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
    grad_fn->other_size_ = other.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_div(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other[i]);
        result_new_fw_grad_opts[i] = (self_t - other_t * result[i]) / other_p;
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_div_ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachDivBackward1ScalarList> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachDivBackward1ScalarList>(new ForeachDivBackward1ScalarList(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->scalars = scalars.vec();
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_div(ks & c10::after_autograd_keyset, self_, scalars);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = self_t / scalars[i];
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_div_Tensor(c10::DispatchKeySet ks, at::TensorList self, const at::Tensor & other) {
  auto self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(other);
  }
  std::shared_ptr<ForeachDivBackward0Tensor> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachDivBackward0Tensor>(new ForeachDivBackward0Tensor(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_div(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto other_t_raw = toNonOptFwGrad(other);
        auto other_tensor = toNonOptTensor(other);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other);
        result_new_fw_grad_opts[i] = (self_t - other_t * result[i]) / other_p;
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_div__Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<DivBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<DivBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<DivBackward1>(new DivBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->other = scalar;
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_div_(ks & c10::after_autograd_keyset, self_, scalar);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(self_t / scalar) : self_t / scalar;
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_div__List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<DivBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<DivBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<DivBackward0>(new DivBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->other_ = SavedVariable(other[i], false);
                if (grad_fn->should_compute_output(1)) {
                  if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                  grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
                }
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_div_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other[i]);
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((self_t - other_t * self_p) / other_p) : (self_t - other_t * self_p) / other_p;
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_div__ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<DivBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<DivBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<DivBackward1>(new DivBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->other = scalars[i];
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_div_(ks & c10::after_autograd_keyset, self_, scalars);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(self_t / scalars[i]) : self_t / scalars[i];
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_div__Tensor(c10::DispatchKeySet ks, at::TensorList self, const at::Tensor & other) {
  auto self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(other);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<DivBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<DivBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<DivBackward0>(new DivBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->other_ = SavedVariable(other, false);
                if (grad_fn->should_compute_output(1)) {
                  if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                  grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
                }
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_div_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other);
        auto other_tensor = toNonOptTensor(other);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other);
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((self_t - other_t * self_p) / other_p) : (self_t - other_t * self_p) / other_p;
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_div_out_Scalar_out(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_div_outf(ks & c10::after_autograd_keyset, self_, scalar, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_div_out that does not support it because it is an out= function");
}
void _foreach_div_out_List_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_div_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(other) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_div_out that does not support it because it is an out= function");
}
void _foreach_div_out_ScalarList_out(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_div_outf(ks & c10::after_autograd_keyset, self_, scalars, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_div_out that does not support it because it is an out= function");
}
void _foreach_div_out_Tensor_out(c10::DispatchKeySet ks, at::TensorList self, const at::Tensor & other, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_div_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefined(other) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_div_out that does not support it because it is an out= function");
}
void _foreach_erfc_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_erfc_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_erfc_out that does not support it because it is an out= function");
}
void _foreach_expm1_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_expm1_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_expm1_out that does not support it because it is an out= function");
}
::std::vector<at::Tensor> _foreach_floor(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachFloorBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachFloorBackward0>(new ForeachFloorBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_floor(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = (zeros_like(self_t.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_floor_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<FloorBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<FloorBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<FloorBackward0>(new FloorBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_floor_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((zeros_like(self_t.conj())).conj()) : (zeros_like(self_t.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_lgamma_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<LgammaBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<LgammaBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<LgammaBackward0>(new LgammaBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_lgamma_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * digamma(original_self_p)).conj()) : (original_self_t.conj() * digamma(original_self_p)).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_lgamma_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_lgamma_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_lgamma_out that does not support it because it is an out= function");
}
void _foreach_log2_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<Log2Backward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<Log2Backward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<Log2Backward0>(new Log2Backward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_log2_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() / (original_self_p.conj() * 0.6931471805599453)).conj()) : (original_self_t.conj() / (original_self_p.conj() * 0.6931471805599453)).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
::std::vector<at::Tensor> _foreach_mul_Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachMulBackward1Scalar> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachMulBackward1Scalar>(new ForeachMulBackward1Scalar(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->scalar = scalar;
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_mul(ks & c10::after_autograd_keyset, self_, scalar);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = self_t * scalar;
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_mul_List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::shared_ptr<ForeachMulBackward0List> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachMulBackward0List>(new ForeachMulBackward0List(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = make_saved_variable_list(other, false);
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
    grad_fn->other_size_ = other.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_mul(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other[i]);
        result_new_fw_grad_opts[i] = other_t * self_p + self_t * other_p;
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_mul_ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachMulBackward1ScalarList> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachMulBackward1ScalarList>(new ForeachMulBackward1ScalarList(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->scalars = scalars.vec();
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_mul(ks & c10::after_autograd_keyset, self_, scalars);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = self_t * scalars[i];
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_mul_Tensor(c10::DispatchKeySet ks, at::TensorList self, const at::Tensor & other) {
  auto self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(other);
  }
  std::shared_ptr<ForeachMulBackward0Tensor> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachMulBackward0Tensor>(new ForeachMulBackward0Tensor(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_mul(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other);
        auto other_tensor = toNonOptTensor(other);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other);
        result_new_fw_grad_opts[i] = other_t * self_p + self_t * other_p;
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_mul__Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<MulBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<MulBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<MulBackward1>(new MulBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->other = scalar;
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_mul_(ks & c10::after_autograd_keyset, self_, scalar);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(self_t * scalar) : self_t * scalar;
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_mul__List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<MulBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<MulBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<MulBackward0>(new MulBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (grad_fn->should_compute_output(0)) {
                  grad_fn->other_ = SavedVariable(other[i], false);
                }
                grad_fn->other_scalar_type = other[i].scalar_type();
                if (grad_fn->should_compute_output(1)) {
                  if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                  grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
                }
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_mul_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(other_t * original_self_p + original_self_t * other_p) : other_t * original_self_p + original_self_t * other_p;
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_mul__ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<MulBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<MulBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<MulBackward1>(new MulBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->other = scalars[i];
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_mul_(ks & c10::after_autograd_keyset, self_, scalars);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(self_t * scalars[i]) : self_t * scalars[i];
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_mul__Tensor(c10::DispatchKeySet ks, at::TensorList self, const at::Tensor & other) {
  auto self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(other);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<MulBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<MulBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<MulBackward0>(new MulBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (grad_fn->should_compute_output(0)) {
                  grad_fn->other_ = SavedVariable(other, false);
                }
                grad_fn->other_scalar_type = other.scalar_type();
                if (grad_fn->should_compute_output(1)) {
                  if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                  grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
                }
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_mul_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other);
        auto other_tensor = toNonOptTensor(other);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(other_t * original_self_p + original_self_t * other_p) : other_t * original_self_p + original_self_t * other_p;
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_mul_out_Scalar_out(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_mul_outf(ks & c10::after_autograd_keyset, self_, scalar, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_mul_out that does not support it because it is an out= function");
}
void _foreach_mul_out_List_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_mul_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(other) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_mul_out that does not support it because it is an out= function");
}
void _foreach_mul_out_ScalarList_out(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_mul_outf(ks & c10::after_autograd_keyset, self_, scalars, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_mul_out that does not support it because it is an out= function");
}
void _foreach_mul_out_Tensor_out(c10::DispatchKeySet ks, at::TensorList self, const at::Tensor & other, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_mul_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefined(other) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_mul_out that does not support it because it is an out= function");
}
void _foreach_pow__List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList exponent) {
  auto self_ = unpack(self, "self", 0);
  auto exponent_ = unpack(exponent, "exponent", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, exponent );
  
  TORCH_CHECK(
      self.size() == exponent.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      exponent.size());
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(exponent[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<PowBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], exponent[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<PowBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<PowBackward1>(new PowBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], exponent[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->exponent_ = SavedVariable(exponent[i], false);
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> exponent__storage_saved(exponent_.size());
  for (const Tensor& tensor : exponent_)
    exponent__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> exponent__impl_saved(exponent_.size());
  for (size_t i=0; i<exponent_.size(); i++)
    if (exponent_[i].defined()) exponent__impl_saved[i] = exponent_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_pow_(ks & c10::after_autograd_keyset, self_, exponent_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<exponent_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exponent__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(exponent_))
      TORCH_INTERNAL_ASSERT(exponent__storage_saved[i].value().is_alias_of(exponent_[i].storage()));
  }
  for (size_t i=0; i<exponent_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exponent__impl_saved[i] && !at::impl::tensorlist_has_dispatch(exponent_))
      TORCH_INTERNAL_ASSERT(exponent__impl_saved[i] == exponent_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto exponent_t_raw = toNonOptFwGrad(exponent[i]);
        auto exponent_tensor = toNonOptTensor(exponent[i]);
        auto exponent_t = (exponent_t_raw.defined() || !exponent_tensor.defined())
          ? exponent_t_raw : at::_efficientzerotensor(exponent_tensor.sizes(), exponent_tensor.options());
        auto exponent_p = toNonOptPrimal(exponent[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((pow_backward_self(original_self_t.conj(), original_self_p, exponent_p) + pow_backward_exponent(exponent_t.conj(), original_self_p, exponent_p, self_p)).conj()) : (pow_backward_self(original_self_t.conj(), original_self_p, exponent_p) + pow_backward_exponent(exponent_t.conj(), original_self_p, exponent_p, self_p)).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
  if (!grad_fns.empty()) {
  
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              grad_fn->result_ = SavedVariable(self[i], true, self[i].is_view());
          }
      }
  }
}
void _foreach_pow__Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & exponent) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<PowBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<PowBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<PowBackward0>(new PowBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->exponent = exponent;
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_pow_(ks & c10::after_autograd_keyset, self_, exponent);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((pow_backward(original_self_t.conj(), original_self_p, exponent)).conj()) : (pow_backward(original_self_t.conj(), original_self_p, exponent)).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_pow__ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> exponent) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<PowBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<PowBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<PowBackward0>(new PowBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->exponent = exponent[i];
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_pow_(ks & c10::after_autograd_keyset, self_, exponent);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((pow_backward(original_self_t.conj(), original_self_p, exponent[i])).conj()) : (pow_backward(original_self_t.conj(), original_self_p, exponent[i])).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_round_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_round_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_round_out that does not support it because it is an out= function");
}
void _foreach_sin_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<SinBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<SinBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<SinBackward0>(new SinBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sin_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * original_self_p.cos().conj()).conj()) : (original_self_t.conj() * original_self_p.cos().conj()).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
::std::vector<at::Tensor> _foreach_tanh(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachTanhBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachTanhBackward0>(new ForeachTanhBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_tanh(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = (tanh_backward(self_t.conj(), result[i])).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  if (grad_fn) {
    grad_fn->result_ = make_saved_variable_list(result, true);
  }
  return result;
}
void _foreach_zero_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<ZeroBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<ZeroBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<ZeroBackward0>(new ZeroBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_zero_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(at::zero_(self_t)) : at::zero_(self_t);
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _histogramdd_bin_edges_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density, at::TensorList out) {
  auto& self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 5);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_histogramdd_bin_edges_outf(ks & c10::after_autograd_keyset, self_, bins, range, weight, density, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _histogramdd_bin_edges_out that does not support it because it is an out= function");
}
at::Tensor _is_any_true(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_is_any_true(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _is_any_true");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _is_any_true");
  #endif
  return result;
}
void _linalg_check_errors(c10::DispatchKeySet ks, const at::Tensor & info, c10::string_view api_name, bool is_matrix) {
  auto& info_ = unpack(info, "info", 0);
  #ifndef NDEBUG
  c10::optional<Storage> info__storage_saved =
    info_.has_storage() ? c10::optional<Storage>(info_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> info__impl_saved;
  if (info_.defined()) info__impl_saved = info_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowADInplaceOrView guard;
    at::redispatch::_linalg_check_errors(ks & c10::after_autograd_keyset, info_, api_name, is_matrix);
  }
  #ifndef NDEBUG
  if (info__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(info_))
    TORCH_INTERNAL_ASSERT(info__storage_saved.value().is_alias_of(info_.storage()));
  if (info__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(info_))
    TORCH_INTERNAL_ASSERT(info__impl_saved == info_.getIntrusivePtr());
  #endif
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _linalg_det(c10::DispatchKeySet ks, const at::Tensor & A) {
  auto& A_ = unpack(A, "A", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(A));
  std::shared_ptr<LinalgDetBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgDetBackward0>(new LinalgDetBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( A ));
    grad_fn->A_ = SavedVariable(A, false);
  }
  at::Tensor result;
  at::Tensor LU;
  at::Tensor pivots;
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_linalg_det(ks & c10::after_autograd_keyset, A_);
  })();
  std::tie(result, LU, pivots) = std::move(_tmp);
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _linalg_det");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _linalg_det");
  if (LU.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU)) {
    TORCH_INTERNAL_ASSERT(LU.storage().use_count() == 1, "function: _linalg_det");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU))
    TORCH_INTERNAL_ASSERT(LU.use_count() <= 1, "function: _linalg_det");
  if (pivots.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots)) {
    TORCH_INTERNAL_ASSERT(pivots.storage().use_count() == 1, "function: _linalg_det");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots))
    TORCH_INTERNAL_ASSERT(pivots.use_count() <= 1, "function: _linalg_det");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      auto A_p = toNonOptPrimal(A);
      result_new_fw_grad_opt = linalg_det_jvp(A_t, result, LU, pivots, A_p.is_contiguous() && !A_p.is_complex());
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->LU_ = SavedVariable(LU, true);
    grad_fn->pivots_ = SavedVariable(pivots, true);
    grad_fn->result_ = SavedVariable(result, true);
  }
  return std::make_tuple(std::move(result), std::move(LU), std::move(pivots));
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _linalg_slogdet_out_sign(c10::DispatchKeySet ks, const at::Tensor & A, at::Tensor & sign, at::Tensor & logabsdet, at::Tensor & LU, at::Tensor & pivots) {
  auto& A_ = unpack(A, "A", 0);
  auto& sign_ = unpack(sign, "sign", 1);
  auto& logabsdet_ = unpack(logabsdet, "logabsdet", 2);
  auto& LU_ = unpack(LU, "LU", 3);
  auto& pivots_ = unpack(pivots, "pivots", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_sign_logabsdet = (isFwGradDefined(A));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( A )) {
    throw_error_out_requires_grad("_linalg_slogdet");
  }
  if (compute_requires_grad( sign, logabsdet )) {
    throw_error_out_requires_grad("_linalg_slogdet");
  }
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  c10::optional<Storage> sign__storage_saved =
    sign_.has_storage() ? c10::optional<Storage>(sign_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> sign__impl_saved;
  if (sign_.defined()) sign__impl_saved = sign_.getIntrusivePtr();
  c10::optional<Storage> logabsdet__storage_saved =
    logabsdet_.has_storage() ? c10::optional<Storage>(logabsdet_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> logabsdet__impl_saved;
  if (logabsdet_.defined()) logabsdet__impl_saved = logabsdet_.getIntrusivePtr();
  c10::optional<Storage> LU__storage_saved =
    LU_.has_storage() ? c10::optional<Storage>(LU_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> LU__impl_saved;
  if (LU_.defined()) LU__impl_saved = LU_.getIntrusivePtr();
  c10::optional<Storage> pivots__storage_saved =
    pivots_.has_storage() ? c10::optional<Storage>(pivots_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> pivots__impl_saved;
  if (pivots_.defined()) pivots__impl_saved = pivots_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_linalg_slogdet_outf(ks & c10::after_autograd_keyset, A_, sign_, logabsdet_, LU_, pivots_);
  }
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (sign__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(sign_))
    TORCH_INTERNAL_ASSERT(sign__storage_saved.value().is_alias_of(sign_.storage()));
  if (sign__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(sign_))
    TORCH_INTERNAL_ASSERT(sign__impl_saved == sign_.getIntrusivePtr());
  if (logabsdet__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(logabsdet_))
    TORCH_INTERNAL_ASSERT(logabsdet__storage_saved.value().is_alias_of(logabsdet_.storage()));
  if (logabsdet__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(logabsdet_))
    TORCH_INTERNAL_ASSERT(logabsdet__impl_saved == logabsdet_.getIntrusivePtr());
  if (LU__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(LU_))
    TORCH_INTERNAL_ASSERT(LU__storage_saved.value().is_alias_of(LU_.storage()));
  if (LU__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU_))
    TORCH_INTERNAL_ASSERT(LU__impl_saved == LU_.getIntrusivePtr());
  if (pivots__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(pivots_))
    TORCH_INTERNAL_ASSERT(pivots__storage_saved.value().is_alias_of(pivots_.storage()));
  if (pivots__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots_))
    TORCH_INTERNAL_ASSERT(pivots__impl_saved == pivots_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( sign, logabsdet ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(A) || isFwGradDefined(sign) || isFwGradDefined(logabsdet) || isFwGradDefined(LU) || isFwGradDefined(pivots))), "Trying to use forward AD with _linalg_slogdet_out that does not support it because it is an out= function");
  return std::forward_as_tuple(sign, logabsdet, LU, pivots);
}
at::Tensor & _log_softmax_backward_data_out_out(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & out) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& output_ = unpack(output, "output", 1);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, output );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, output )) {
    throw_error_out_requires_grad("_log_softmax_backward_data");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("_log_softmax_backward_data");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_log_softmax_backward_data_outf(ks & c10::after_autograd_keyset, grad_output_, output_, dim, input_dtype, out_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(output) || isFwGradDefined(out))), "Trying to use forward AD with _log_softmax_backward_data_out that does not support it because it is an out= function");
  return out;
}
at::Tensor _masked_softmax(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mask, c10::optional<int64_t> dim, c10::optional<int64_t> mask_type) {
  auto& self_ = unpack(self, "self", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<MaskedSoftmaxBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaskedSoftmaxBackward0>(new MaskedSoftmaxBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->mask_ = SavedVariable(mask, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_masked_softmax", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_masked_softmax", *opt_op, ks, self, mask, dim, mask_type);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_masked_softmax(ks & c10::after_autograd_keyset, self_, mask_, dim, mask_type);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _masked_softmax");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _masked_softmax");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_masked_softmax");
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _native_batch_norm_legit_functional(c10::DispatchKeySet ks, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & running_mean, const at::Tensor & running_var, bool training, double momentum, double eps) {
  auto& input_ = unpack(input, "input", 0);
  auto& running_mean_ = unpack(running_mean, "running_mean", 3);
  auto& running_var_ = unpack(running_var, "running_var", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias));
  check_no_requires_grad(running_mean, "running_mean", "_native_batch_norm_legit_functional");
  check_no_requires_grad(running_var, "running_var", "_native_batch_norm_legit_functional");
  std::shared_ptr<NativeBatchNormLegitBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NativeBatchNormLegitBackward0>(new NativeBatchNormLegitBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->eps = eps;
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->running_mean_ = SavedVariable(running_mean, false);
    grad_fn->running_var_ = SavedVariable(running_var, false);
    grad_fn->training = training;
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  at::Tensor running_mean_out;
  at::Tensor running_var_out;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> running_mean__storage_saved =
    running_mean_.has_storage() ? c10::optional<Storage>(running_mean_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> running_mean__impl_saved;
  if (running_mean_.defined()) running_mean__impl_saved = running_mean_.getIntrusivePtr();
  c10::optional<Storage> running_var__storage_saved =
    running_var_.has_storage() ? c10::optional<Storage>(running_var_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> running_var__impl_saved;
  if (running_var_.defined()) running_var__impl_saved = running_var_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_native_batch_norm_legit_functional(ks & c10::after_autograd_keyset, input_, weight, bias, running_mean_, running_var_, training, momentum, eps);
  })();
  std::tie(result0, result1, result2, running_mean_out, running_var_out) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (running_mean__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(running_mean_))
    TORCH_INTERNAL_ASSERT(running_mean__storage_saved.value().is_alias_of(running_mean_.storage()));
  if (running_mean__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(running_mean_))
    TORCH_INTERNAL_ASSERT(running_mean__impl_saved == running_mean_.getIntrusivePtr());
  if (running_var__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(running_var_))
    TORCH_INTERNAL_ASSERT(running_var__storage_saved.value().is_alias_of(running_var_.storage()));
  if (running_var__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(running_var_))
    TORCH_INTERNAL_ASSERT(running_var__impl_saved == running_var_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _native_batch_norm_legit_functional");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _native_batch_norm_legit_functional");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _native_batch_norm_legit_functional");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _native_batch_norm_legit_functional");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: _native_batch_norm_legit_functional");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: _native_batch_norm_legit_functional");
  if (running_mean_out.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(running_mean_out)) {
    TORCH_INTERNAL_ASSERT(running_mean_out.storage().use_count() == 1, "function: _native_batch_norm_legit_functional");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(running_mean_out))
    TORCH_INTERNAL_ASSERT(running_mean_out.use_count() <= 1, "function: _native_batch_norm_legit_functional");
  if (running_var_out.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(running_var_out)) {
    TORCH_INTERNAL_ASSERT(running_var_out.storage().use_count() == 1, "function: _native_batch_norm_legit_functional");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(running_var_out))
    TORCH_INTERNAL_ASSERT(running_var_out.use_count() <= 1, "function: _native_batch_norm_legit_functional");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_native_batch_norm_legit");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto input_t_raw = toNonOptFwGrad(input);
      auto input_tensor = toNonOptTensor(input);
      auto input_t = (input_t_raw.defined() || !input_tensor.defined())
        ? input_t_raw : at::_efficientzerotensor(input_tensor.sizes(), input_tensor.options());
      auto input_p = toNonOptPrimal(input);
      auto weight_t_raw = toNonOptFwGrad(weight);
      auto weight_tensor = toNonOptTensor(weight);
      auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
        ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
      auto weight_p = toNonOptPrimal(weight);
      auto bias_t_raw = toNonOptFwGrad(bias);
      auto bias_tensor = toNonOptTensor(bias);
      auto bias_t = (bias_t_raw.defined() || !bias_tensor.defined())
        ? bias_t_raw : at::_efficientzerotensor(bias_tensor.sizes(), bias_tensor.options());
      auto bias_p = toNonOptPrimal(bias);
      result0_new_fw_grad_opt = batch_norm_jvp(input_p, input_t, weight_p, weight_t, bias_p, bias_t, running_mean, running_var, result1, result2, training, eps);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(running_mean_out), std::move(running_var_out));
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _native_batch_norm_legit_no_training(c10::DispatchKeySet ks, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & running_mean, const at::Tensor & running_var, double momentum, double eps) {
  auto& input_ = unpack(input, "input", 0);
  auto& running_mean_ = unpack(running_mean, "running_mean", 3);
  auto& running_var_ = unpack(running_var, "running_var", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias));
  check_no_requires_grad(running_mean, "running_mean", "_native_batch_norm_legit_no_training");
  check_no_requires_grad(running_var, "running_var", "_native_batch_norm_legit_no_training");
  std::shared_ptr<NativeBatchNormLegitNoTrainingBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NativeBatchNormLegitNoTrainingBackward0>(new NativeBatchNormLegitNoTrainingBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->eps = eps;
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->running_mean_ = SavedVariable(running_mean, false);
    grad_fn->running_var_ = SavedVariable(running_var, false);
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> running_mean__storage_saved =
    running_mean_.has_storage() ? c10::optional<Storage>(running_mean_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> running_mean__impl_saved;
  if (running_mean_.defined()) running_mean__impl_saved = running_mean_.getIntrusivePtr();
  c10::optional<Storage> running_var__storage_saved =
    running_var_.has_storage() ? c10::optional<Storage>(running_var_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> running_var__impl_saved;
  if (running_var_.defined()) running_var__impl_saved = running_var_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_native_batch_norm_legit_no_training(ks & c10::after_autograd_keyset, input_, weight, bias, running_mean_, running_var_, momentum, eps);
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (running_mean__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(running_mean_))
    TORCH_INTERNAL_ASSERT(running_mean__storage_saved.value().is_alias_of(running_mean_.storage()));
  if (running_mean__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(running_mean_))
    TORCH_INTERNAL_ASSERT(running_mean__impl_saved == running_mean_.getIntrusivePtr());
  if (running_var__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(running_var_))
    TORCH_INTERNAL_ASSERT(running_var__storage_saved.value().is_alias_of(running_var_.storage()));
  if (running_var__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(running_var_))
    TORCH_INTERNAL_ASSERT(running_var__impl_saved == running_var_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _native_batch_norm_legit_no_training");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _native_batch_norm_legit_no_training");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _native_batch_norm_legit_no_training");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _native_batch_norm_legit_no_training");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: _native_batch_norm_legit_no_training");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: _native_batch_norm_legit_no_training");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_native_batch_norm_legit_no_training");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto input_t_raw = toNonOptFwGrad(input);
      auto input_tensor = toNonOptTensor(input);
      auto input_t = (input_t_raw.defined() || !input_tensor.defined())
        ? input_t_raw : at::_efficientzerotensor(input_tensor.sizes(), input_tensor.options());
      auto input_p = toNonOptPrimal(input);
      auto weight_t_raw = toNonOptFwGrad(weight);
      auto weight_tensor = toNonOptTensor(weight);
      auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
        ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
      auto weight_p = toNonOptPrimal(weight);
      auto bias_t_raw = toNonOptFwGrad(bias);
      auto bias_tensor = toNonOptTensor(bias);
      auto bias_t = (bias_t_raw.defined() || !bias_tensor.defined())
        ? bias_t_raw : at::_efficientzerotensor(bias_tensor.sizes(), bias_tensor.options());
      auto bias_p = toNonOptPrimal(bias);
      result0_new_fw_grad_opt = batch_norm_jvp(input_p, input_t, weight_p, weight_t, bias_p, bias_t, running_mean, running_var, result1, result2, /*training=*/false, eps);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _native_batch_norm_legit_out_no_stats_out(c10::DispatchKeySet ks, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
  auto& input_ = unpack(input, "input", 0);
  auto& out_ = unpack(out, "out", 6);
  auto& save_mean_ = unpack(save_mean, "save_mean", 7);
  auto& save_invstd_ = unpack(save_invstd, "save_invstd", 8);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( input, weight, bias )) {
    throw_error_out_requires_grad("_native_batch_norm_legit");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("_native_batch_norm_legit");
  }
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  c10::optional<Storage> save_mean__storage_saved =
    save_mean_.has_storage() ? c10::optional<Storage>(save_mean_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> save_mean__impl_saved;
  if (save_mean_.defined()) save_mean__impl_saved = save_mean_.getIntrusivePtr();
  c10::optional<Storage> save_invstd__storage_saved =
    save_invstd_.has_storage() ? c10::optional<Storage>(save_invstd_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> save_invstd__impl_saved;
  if (save_invstd_.defined()) save_invstd__impl_saved = save_invstd_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_native_batch_norm_legit_outf(ks & c10::after_autograd_keyset, input_, weight, bias, training, momentum, eps, out_, save_mean_, save_invstd_);
  }
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  if (save_mean__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(save_mean_))
    TORCH_INTERNAL_ASSERT(save_mean__storage_saved.value().is_alias_of(save_mean_.storage()));
  if (save_mean__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(save_mean_))
    TORCH_INTERNAL_ASSERT(save_mean__impl_saved == save_mean_.getIntrusivePtr());
  if (save_invstd__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(save_invstd_))
    TORCH_INTERNAL_ASSERT(save_invstd__storage_saved.value().is_alias_of(save_invstd_.storage()));
  if (save_invstd__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(save_invstd_))
    TORCH_INTERNAL_ASSERT(save_invstd__impl_saved == save_invstd_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias) || isFwGradDefined(out) || isFwGradDefined(save_mean) || isFwGradDefined(save_invstd))), "Trying to use forward AD with _native_batch_norm_legit_out that does not support it because it is an out= function");
  return std::forward_as_tuple(out, save_mean, save_invstd);
}
at::Tensor _nested_from_padded(c10::DispatchKeySet ks, const at::Tensor & padded, const at::Tensor & cpu_nested_shape_example, bool fuse_transform_0213) {
  auto& padded_ = unpack(padded, "padded", 0);
  auto& cpu_nested_shape_example_ = unpack(cpu_nested_shape_example, "cpu_nested_shape_example", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( padded );
  
  std::shared_ptr<NestedFromPaddedBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NestedFromPaddedBackward0>(new NestedFromPaddedBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( padded ));
    grad_fn->fuse_transform_0213 = fuse_transform_0213;
    grad_fn->padded_ = SavedVariable(padded, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> padded__storage_saved =
    padded_.has_storage() ? c10::optional<Storage>(padded_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> padded__impl_saved;
  if (padded_.defined()) padded__impl_saved = padded_.getIntrusivePtr();
  c10::optional<Storage> cpu_nested_shape_example__storage_saved =
    cpu_nested_shape_example_.has_storage() ? c10::optional<Storage>(cpu_nested_shape_example_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> cpu_nested_shape_example__impl_saved;
  if (cpu_nested_shape_example_.defined()) cpu_nested_shape_example__impl_saved = cpu_nested_shape_example_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(padded))) {
      static c10::OperatorName full_name("aten::_nested_from_padded", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_nested_from_padded", *opt_op, ks, padded, cpu_nested_shape_example, fuse_transform_0213);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_nested_from_padded(ks & c10::after_autograd_keyset, padded_, cpu_nested_shape_example_, fuse_transform_0213);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (padded__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(padded_))
    TORCH_INTERNAL_ASSERT(padded__storage_saved.value().is_alias_of(padded_.storage()));
  if (padded__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(padded_))
    TORCH_INTERNAL_ASSERT(padded__impl_saved == padded_.getIntrusivePtr());
  if (cpu_nested_shape_example__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(cpu_nested_shape_example_))
    TORCH_INTERNAL_ASSERT(cpu_nested_shape_example__storage_saved.value().is_alias_of(cpu_nested_shape_example_.storage()));
  if (cpu_nested_shape_example__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(cpu_nested_shape_example_))
    TORCH_INTERNAL_ASSERT(cpu_nested_shape_example__impl_saved == cpu_nested_shape_example_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _nested_from_padded");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _nested_from_padded");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_nested_from_padded");
  return result;
}
at::Tensor & _nested_tensor_size_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_nested_tensor_size_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with _nested_tensor_size_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & _nested_tensor_strides_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_nested_tensor_strides_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with _nested_tensor_strides_out that does not support it because it is an out= function");
  return out;
}
at::Tensor _nested_view_from_buffer(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & nested_size, const at::Tensor & nested_strides, const at::Tensor & offsets) {
  auto& self_ = unpack(self, "self", 0);
  auto& nested_size_ = unpack(nested_size, "nested_size", 1);
  auto& nested_strides_ = unpack(nested_strides, "nested_strides", 2);
  auto& offsets_ = unpack(offsets, "offsets", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  check_no_requires_grad(offsets, "offsets", "_nested_view_from_buffer");
  std::shared_ptr<NestedViewFromBufferBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NestedViewFromBufferBackward0>(new NestedViewFromBufferBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> nested_size__storage_saved =
    nested_size_.has_storage() ? c10::optional<Storage>(nested_size_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> nested_size__impl_saved;
  if (nested_size_.defined()) nested_size__impl_saved = nested_size_.getIntrusivePtr();
  c10::optional<Storage> nested_strides__storage_saved =
    nested_strides_.has_storage() ? c10::optional<Storage>(nested_strides_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> nested_strides__impl_saved;
  if (nested_strides_.defined()) nested_strides__impl_saved = nested_strides_.getIntrusivePtr();
  c10::optional<Storage> offsets__storage_saved =
    offsets_.has_storage() ? c10::optional<Storage>(offsets_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> offsets__impl_saved;
  if (offsets_.defined()) offsets__impl_saved = offsets_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(offsets))) {
      static c10::OperatorName full_name("aten::_nested_view_from_buffer", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_nested_view_from_buffer", *opt_op, ks, self, nested_size, nested_strides, offsets);
    } else {
      at::AutoDispatchBelowAutograd guard;
      return at::redispatch::_nested_view_from_buffer(ks & c10::after_autograd_keyset, self_, nested_size_, nested_strides_, offsets_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (nested_size__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(nested_size_))
    TORCH_INTERNAL_ASSERT(nested_size__storage_saved.value().is_alias_of(nested_size_.storage()));
  if (nested_size__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(nested_size_))
    TORCH_INTERNAL_ASSERT(nested_size__impl_saved == nested_size_.getIntrusivePtr());
  if (nested_strides__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(nested_strides_))
    TORCH_INTERNAL_ASSERT(nested_strides__storage_saved.value().is_alias_of(nested_strides_.storage()));
  if (nested_strides__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(nested_strides_))
    TORCH_INTERNAL_ASSERT(nested_strides__impl_saved == nested_strides_.getIntrusivePtr());
  if (offsets__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(offsets_))
    TORCH_INTERNAL_ASSERT(offsets__storage_saved.value().is_alias_of(offsets_.storage()));
  if (offsets__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(offsets_))
    TORCH_INTERNAL_ASSERT(offsets__impl_saved == offsets_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _nested_view_from_buffer");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_nested_view_from_buffer");
  return result;
}
::std::tuple<at::Tensor,at::Tensor> _pack_padded_sequence(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & lengths, bool batch_first) {
  auto& input_ = unpack(input, "input", 0);
  auto& lengths_ = unpack(lengths, "lengths", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input );
  
  check_no_requires_grad(lengths, "lengths", "_pack_padded_sequence");
  std::shared_ptr<PackPaddedSequenceBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PackPaddedSequenceBackward0>(new PackPaddedSequenceBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input ));
    grad_fn->batch_first = batch_first;
    grad_fn->input_sym_sizes = input.sym_sizes().vec();
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> lengths__storage_saved =
    lengths_.has_storage() ? c10::optional<Storage>(lengths_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> lengths__impl_saved;
  if (lengths_.defined()) lengths__impl_saved = lengths_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input) || isFwGradDefined(lengths))) {
      static c10::OperatorName full_name("aten::_pack_padded_sequence", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_pack_padded_sequence", *opt_op, ks, input, lengths, batch_first);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_pack_padded_sequence(ks & c10::after_autograd_keyset, input_, lengths_, batch_first);
    }
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (lengths__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(lengths_))
    TORCH_INTERNAL_ASSERT(lengths__storage_saved.value().is_alias_of(lengths_.storage()));
  if (lengths__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(lengths_))
    TORCH_INTERNAL_ASSERT(lengths__impl_saved == lengths_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _pack_padded_sequence");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _pack_padded_sequence");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _pack_padded_sequence");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _pack_padded_sequence");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_pack_padded_sequence");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor _pdist_backward(c10::DispatchKeySet ks, const at::Tensor & grad, const at::Tensor & self, double p, const at::Tensor & pdist) {
  auto& grad_ = unpack(grad, "grad", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& pdist_ = unpack(pdist, "pdist", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad, self, pdist );
  
  std::shared_ptr<PdistBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PdistBackwardBackward0>(new PdistBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad, self, pdist ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad__storage_saved =
    grad_.has_storage() ? c10::optional<Storage>(grad_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad__impl_saved;
  if (grad_.defined()) grad__impl_saved = grad_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> pdist__storage_saved =
    pdist_.has_storage() ? c10::optional<Storage>(pdist_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> pdist__impl_saved;
  if (pdist_.defined()) pdist__impl_saved = pdist_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(grad) || isFwGradDefined(self) || isFwGradDefined(pdist))) {
      static c10::OperatorName full_name("aten::_pdist_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_pdist_backward", *opt_op, ks, grad, self, p, pdist);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_pdist_backward(ks & c10::after_autograd_keyset, grad_, self_, p, pdist_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_))
    TORCH_INTERNAL_ASSERT(grad__storage_saved.value().is_alias_of(grad_.storage()));
  if (grad__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_))
    TORCH_INTERNAL_ASSERT(grad__impl_saved == grad_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (pdist__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(pdist_))
    TORCH_INTERNAL_ASSERT(pdist__storage_saved.value().is_alias_of(pdist_.storage()));
  if (pdist__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pdist_))
    TORCH_INTERNAL_ASSERT(pdist__impl_saved == pdist_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _pdist_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _pdist_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_pdist_backward");
  return result;
}
at::Tensor _reshape_alias(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ReshapeAliasBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReshapeAliasBackward0>(new ReshapeAliasBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::_reshape_alias_symint(ks & c10::after_autograd_keyset, self_, size, stride);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _reshape_alias");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_reshape_alias_symint(self_t, size, stride);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _reshape_alias_copy(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ReshapeAliasBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReshapeAliasBackward0_copy>(new ReshapeAliasBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_reshape_alias_copy_symint(ks & c10::after_autograd_keyset, self_, size, stride);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _reshape_alias_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _reshape_alias_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_reshape_alias_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_reshape_alias_symint(self_t, size, stride);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _scaled_dot_product_efficient_attention(c10::DispatchKeySet ks, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_bias, bool compute_log_sumexp, double dropout_p, bool is_causal, c10::optional<double> scale) {
  auto& query_ = unpack(query, "query", 0);
  auto& key_ = unpack(key, "key", 1);
  auto& value_ = unpack(value, "value", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( query, key, value, attn_bias );
  
  std::shared_ptr<ScaledDotProductEfficientAttentionBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ScaledDotProductEfficientAttentionBackward0>(new ScaledDotProductEfficientAttentionBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( query, key, value, attn_bias ));
    grad_fn->attn_bias_ = SavedVariable(attn_bias, false);
    grad_fn->dropout_p = dropout_p;
    grad_fn->is_causal = is_causal;
    grad_fn->key_ = SavedVariable(key, false);
    grad_fn->query_ = SavedVariable(query, false);
    grad_fn->scale = scale;
    grad_fn->value_ = SavedVariable(value, false);
  }
  at::Tensor output;
  at::Tensor log_sumexp;
  at::Tensor philox_seed;
  at::Tensor philox_offset;
  #ifndef NDEBUG
  c10::optional<Storage> query__storage_saved =
    query_.has_storage() ? c10::optional<Storage>(query_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> query__impl_saved;
  if (query_.defined()) query__impl_saved = query_.getIntrusivePtr();
  c10::optional<Storage> key__storage_saved =
    key_.has_storage() ? c10::optional<Storage>(key_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> key__impl_saved;
  if (key_.defined()) key__impl_saved = key_.getIntrusivePtr();
  c10::optional<Storage> value__storage_saved =
    value_.has_storage() ? c10::optional<Storage>(value_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> value__impl_saved;
  if (value_.defined()) value__impl_saved = value_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(query) || isFwGradDefined(key) || isFwGradDefined(value) || isFwGradDefined(attn_bias))) {
      static c10::OperatorName full_name("aten::_scaled_dot_product_efficient_attention", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>>("_scaled_dot_product_efficient_attention", *opt_op, ks, query, key, value, attn_bias, compute_log_sumexp, dropout_p, is_causal, scale);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_scaled_dot_product_efficient_attention(ks & c10::after_autograd_keyset, query_, key_, value_, attn_bias, compute_log_sumexp, dropout_p, is_causal, scale);
    }
  })();
  std::tie(output, log_sumexp, philox_seed, philox_offset) = std::move(_tmp);
  #ifndef NDEBUG
  if (query__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(query_))
    TORCH_INTERNAL_ASSERT(query__storage_saved.value().is_alias_of(query_.storage()));
  if (query__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(query_))
    TORCH_INTERNAL_ASSERT(query__impl_saved == query_.getIntrusivePtr());
  if (key__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(key_))
    TORCH_INTERNAL_ASSERT(key__storage_saved.value().is_alias_of(key_.storage()));
  if (key__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(key_))
    TORCH_INTERNAL_ASSERT(key__impl_saved == key_.getIntrusivePtr());
  if (value__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__storage_saved.value().is_alias_of(value_.storage()));
  if (value__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__impl_saved == value_.getIntrusivePtr());
  if (output.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output)) {
    TORCH_INTERNAL_ASSERT(output.storage().use_count() == 1, "function: _scaled_dot_product_efficient_attention");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output))
    TORCH_INTERNAL_ASSERT(output.use_count() <= 1, "function: _scaled_dot_product_efficient_attention");
  if (log_sumexp.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(log_sumexp)) {
    TORCH_INTERNAL_ASSERT(log_sumexp.storage().use_count() == 1, "function: _scaled_dot_product_efficient_attention");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(log_sumexp))
    TORCH_INTERNAL_ASSERT(log_sumexp.use_count() <= 1, "function: _scaled_dot_product_efficient_attention");
  if (philox_seed.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_seed)) {
    TORCH_INTERNAL_ASSERT(philox_seed.storage().use_count() == 1, "function: _scaled_dot_product_efficient_attention");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_seed))
    TORCH_INTERNAL_ASSERT(philox_seed.use_count() <= 1, "function: _scaled_dot_product_efficient_attention");
  if (philox_offset.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_offset)) {
    TORCH_INTERNAL_ASSERT(philox_offset.storage().use_count() == 1, "function: _scaled_dot_product_efficient_attention");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_offset))
    TORCH_INTERNAL_ASSERT(philox_offset.use_count() <= 1, "function: _scaled_dot_product_efficient_attention");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( output ), grad_fn);
  }
  throw_error_for_complex_autograd(output, "_scaled_dot_product_efficient_attention");
  if (grad_fn) {
    grad_fn->log_sumexp_ = SavedVariable(log_sumexp, true);
    grad_fn->output_ = SavedVariable(output, true);
    grad_fn->philox_offset_ = SavedVariable(philox_offset, true);
    grad_fn->philox_seed_ = SavedVariable(philox_seed, true);
  }
  return std::make_tuple(std::move(output), std::move(log_sumexp), std::move(philox_seed), std::move(philox_offset));
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _slow_conv2d_backward_output_mask(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, ::std::array<bool,3> output_mask) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& weight_ = unpack(weight, "weight", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self, weight );
  
  std::shared_ptr<SlowConv2DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SlowConv2DBackwardBackward0>(new SlowConv2DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self, weight ));
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor grad_input;
  at::Tensor grad_weight;
  at::Tensor grad_bias;
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(weight))) {
      static c10::OperatorName full_name("aten::_slow_conv2d_backward", "output_mask");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor>>("_slow_conv2d_backward", *opt_op, ks, grad_output, self, weight, kernel_size, stride, padding, output_mask);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_slow_conv2d_backward_symint(ks & c10::after_autograd_keyset, grad_output_, self_, weight_, kernel_size, stride, padding, output_mask);
    }
  })();
  std::tie(grad_input, grad_weight, grad_bias) = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (grad_input.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input)) {
    TORCH_INTERNAL_ASSERT(grad_input.storage().use_count() == 1, "function: _slow_conv2d_backward_output_mask");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input))
    TORCH_INTERNAL_ASSERT(grad_input.use_count() <= 1, "function: _slow_conv2d_backward_output_mask");
  if (grad_weight.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_weight)) {
    TORCH_INTERNAL_ASSERT(grad_weight.storage().use_count() == 1, "function: _slow_conv2d_backward_output_mask");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_weight))
    TORCH_INTERNAL_ASSERT(grad_weight.use_count() <= 1, "function: _slow_conv2d_backward_output_mask");
  if (grad_bias.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_bias)) {
    TORCH_INTERNAL_ASSERT(grad_bias.storage().use_count() == 1, "function: _slow_conv2d_backward_output_mask");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_bias))
    TORCH_INTERNAL_ASSERT(grad_bias.use_count() <= 1, "function: _slow_conv2d_backward_output_mask");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( grad_input, grad_weight, grad_bias ), grad_fn);
  }
  throw_error_for_complex_autograd(grad_input, "_slow_conv2d_backward");
  throw_error_for_complex_autograd(grad_weight, "_slow_conv2d_backward");
  throw_error_for_complex_autograd(grad_bias, "_slow_conv2d_backward");
  return std::make_tuple(std::move(grad_input), std::move(grad_weight), std::move(grad_bias));
}
at::Tensor _softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SoftmaxBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SoftmaxBackward0>(new SoftmaxBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _softmax");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _softmax");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_softmax");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = result * (self_t - logsumexp_jvp(self_p, self_t, {dim}, true));
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor _sparse_log_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<SparseLogSoftmaxBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SparseLogSoftmaxBackward0>(new SparseLogSoftmaxBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_sparse_log_softmax", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_sparse_log_softmax", *opt_op, ks, self, dim, half_to_float);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_sparse_log_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _sparse_log_softmax");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _sparse_log_softmax");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_sparse_log_softmax");
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor> _sparse_mm_reduce_impl(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, c10::string_view reduce) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  std::shared_ptr<SparseMmReduceImplBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SparseMmReduceImplBackward0>(new SparseMmReduceImplBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->reduce = std::string(reduce);
    grad_fn->self_ = SavedVariable(self, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(other))) {
      static c10::OperatorName full_name("aten::_sparse_mm_reduce_impl", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_sparse_mm_reduce_impl", *opt_op, ks, self, other, reduce);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_sparse_mm_reduce_impl(ks & c10::after_autograd_keyset, self_, other_, reduce);
    }
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _sparse_mm_reduce_impl");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _sparse_mm_reduce_impl");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _sparse_mm_reduce_impl");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _sparse_mm_reduce_impl");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_sparse_mm_reduce_impl");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor _test_autograd_multiple_dispatch_view_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<TestAutogradMultipleDispatchViewBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TestAutogradMultipleDispatchViewBackward0_copy>(new TestAutogradMultipleDispatchViewBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_test_autograd_multiple_dispatch_view_copy", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_test_autograd_multiple_dispatch_view_copy", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_test_autograd_multiple_dispatch_view_copy(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _test_autograd_multiple_dispatch_view_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _test_autograd_multiple_dispatch_view_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_test_autograd_multiple_dispatch_view_copy");
  return result;
}
::std::tuple<at::Tensor,at::Tensor> _thnn_fused_gru_cell(c10::DispatchKeySet ks, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
  auto& input_gates_ = unpack(input_gates, "input_gates", 0);
  auto& hidden_gates_ = unpack(hidden_gates, "hidden_gates", 1);
  auto& hx_ = unpack(hx, "hx", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input_gates, hidden_gates, hx, input_bias, hidden_bias );
  
  std::shared_ptr<ThnnFusedGruCellBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ThnnFusedGruCellBackward0>(new ThnnFusedGruCellBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input_gates, hidden_gates, hx, input_bias, hidden_bias ));
    grad_fn->hidden_bias_ = SavedVariable(hidden_bias, false);
    grad_fn->hidden_gates_ = SavedVariable(hidden_gates, false);
    grad_fn->hx_ = SavedVariable(hx, false);
    grad_fn->input_bias_ = SavedVariable(input_bias, false);
    grad_fn->input_gates_ = SavedVariable(input_gates, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> input_gates__storage_saved =
    input_gates_.has_storage() ? c10::optional<Storage>(input_gates_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input_gates__impl_saved;
  if (input_gates_.defined()) input_gates__impl_saved = input_gates_.getIntrusivePtr();
  c10::optional<Storage> hidden_gates__storage_saved =
    hidden_gates_.has_storage() ? c10::optional<Storage>(hidden_gates_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> hidden_gates__impl_saved;
  if (hidden_gates_.defined()) hidden_gates__impl_saved = hidden_gates_.getIntrusivePtr();
  c10::optional<Storage> hx__storage_saved =
    hx_.has_storage() ? c10::optional<Storage>(hx_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> hx__impl_saved;
  if (hx_.defined()) hx__impl_saved = hx_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input_gates) || isFwGradDefined(hidden_gates) || isFwGradDefined(hx) || isFwGradDefined(input_bias) || isFwGradDefined(hidden_bias))) {
      static c10::OperatorName full_name("aten::_thnn_fused_gru_cell", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_thnn_fused_gru_cell", *opt_op, ks, input_gates, hidden_gates, hx, input_bias, hidden_bias);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_thnn_fused_gru_cell(ks & c10::after_autograd_keyset, input_gates_, hidden_gates_, hx_, input_bias, hidden_bias);
    }
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (input_gates__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_gates_))
    TORCH_INTERNAL_ASSERT(input_gates__storage_saved.value().is_alias_of(input_gates_.storage()));
  if (input_gates__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_gates_))
    TORCH_INTERNAL_ASSERT(input_gates__impl_saved == input_gates_.getIntrusivePtr());
  if (hidden_gates__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(hidden_gates_))
    TORCH_INTERNAL_ASSERT(hidden_gates__storage_saved.value().is_alias_of(hidden_gates_.storage()));
  if (hidden_gates__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(hidden_gates_))
    TORCH_INTERNAL_ASSERT(hidden_gates__impl_saved == hidden_gates_.getIntrusivePtr());
  if (hx__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__storage_saved.value().is_alias_of(hx_.storage()));
  if (hx__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__impl_saved == hx_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _thnn_fused_gru_cell");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _thnn_fused_gru_cell");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _thnn_fused_gru_cell");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _thnn_fused_gru_cell");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_thnn_fused_gru_cell");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor _to_sparse_sparse_dim(c10::DispatchKeySet ks, const at::Tensor & self, int64_t sparse_dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<ToSparseBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ToSparseBackward0>(new ToSparseBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_layout = self.layout();
    grad_fn->self_self_sym_blocksize_opt = at::sparse_csr::getSymIntBlockSize(self);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_to_sparse", "sparse_dim");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_to_sparse", *opt_op, ks, self, sparse_dim);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_to_sparse(ks & c10::after_autograd_keyset, self_, sparse_dim);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _to_sparse_sparse_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _to_sparse_sparse_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_to_sparse");
  return result;
}
at::Tensor _to_sparse(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::Layout> layout, at::OptionalIntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<ToSparseBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ToSparseBackward1>(new ToSparseBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_layout = self.layout();
    grad_fn->self_self_sym_blocksize_opt = at::sparse_csr::getSymIntBlockSize(self);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_to_sparse", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_to_sparse", *opt_op, ks, self, layout, blocksize, dense_dim);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_to_sparse(ks & c10::after_autograd_keyset, self_, layout, blocksize, dense_dim);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _to_sparse");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _to_sparse");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_to_sparse");
  return result;
}
at::Tensor _trilinear(c10::DispatchKeySet ks, const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim) {
  auto& i1_ = unpack(i1, "i1", 0);
  auto& i2_ = unpack(i2, "i2", 1);
  auto& i3_ = unpack(i3, "i3", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( i1, i2, i3 );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(i1) || isFwGradDefined(i2) || isFwGradDefined(i3));
  std::shared_ptr<TrilinearBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TrilinearBackward0>(new TrilinearBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( i1, i2, i3 ));
    grad_fn->expand1 = expand1.vec();
    grad_fn->expand2 = expand2.vec();
    grad_fn->expand3 = expand3.vec();
    if (grad_fn->should_compute_output(1) || grad_fn->should_compute_output(2)) {
      grad_fn->i1_ = SavedVariable(i1, false);
    }
    if (grad_fn->should_compute_output(0) || grad_fn->should_compute_output(2)) {
      grad_fn->i2_ = SavedVariable(i2, false);
    }
    if (grad_fn->should_compute_output(0) || grad_fn->should_compute_output(1)) {
      grad_fn->i3_ = SavedVariable(i3, false);
    }
    grad_fn->sumdim = sumdim.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> i1__storage_saved =
    i1_.has_storage() ? c10::optional<Storage>(i1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> i1__impl_saved;
  if (i1_.defined()) i1__impl_saved = i1_.getIntrusivePtr();
  c10::optional<Storage> i2__storage_saved =
    i2_.has_storage() ? c10::optional<Storage>(i2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> i2__impl_saved;
  if (i2_.defined()) i2__impl_saved = i2_.getIntrusivePtr();
  c10::optional<Storage> i3__storage_saved =
    i3_.has_storage() ? c10::optional<Storage>(i3_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> i3__impl_saved;
  if (i3_.defined()) i3__impl_saved = i3_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_trilinear(ks & c10::after_autograd_keyset, i1_, i2_, i3_, expand1, expand2, expand3, sumdim, unroll_dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (i1__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(i1_))
    TORCH_INTERNAL_ASSERT(i1__storage_saved.value().is_alias_of(i1_.storage()));
  if (i1__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(i1_))
    TORCH_INTERNAL_ASSERT(i1__impl_saved == i1_.getIntrusivePtr());
  if (i2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(i2_))
    TORCH_INTERNAL_ASSERT(i2__storage_saved.value().is_alias_of(i2_.storage()));
  if (i2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(i2_))
    TORCH_INTERNAL_ASSERT(i2__impl_saved == i2_.getIntrusivePtr());
  if (i3__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(i3_))
    TORCH_INTERNAL_ASSERT(i3__storage_saved.value().is_alias_of(i3_.storage()));
  if (i3__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(i3_))
    TORCH_INTERNAL_ASSERT(i3__impl_saved == i3_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _trilinear");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _trilinear");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_trilinear");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto i1_t_raw = toNonOptFwGrad(i1);
      auto i1_tensor = toNonOptTensor(i1);
      auto i1_t = (i1_t_raw.defined() || !i1_tensor.defined())
        ? i1_t_raw : at::_efficientzerotensor(i1_tensor.sizes(), i1_tensor.options());
      auto i1_p = toNonOptPrimal(i1);
      auto i2_t_raw = toNonOptFwGrad(i2);
      auto i2_tensor = toNonOptTensor(i2);
      auto i2_t = (i2_t_raw.defined() || !i2_tensor.defined())
        ? i2_t_raw : at::_efficientzerotensor(i2_tensor.sizes(), i2_tensor.options());
      auto i2_p = toNonOptPrimal(i2);
      auto i3_t_raw = toNonOptFwGrad(i3);
      auto i3_tensor = toNonOptTensor(i3);
      auto i3_t = (i3_t_raw.defined() || !i3_tensor.defined())
        ? i3_t_raw : at::_efficientzerotensor(i3_tensor.sizes(), i3_tensor.options());
      auto i3_p = toNonOptPrimal(i3);
      result_new_fw_grad_opt = _trilinear(i1_t, i2_p, i3_p, expand1, expand2, expand3, sumdim, unroll_dim) + _trilinear(i1_p, i2_t, i3_p, expand1, expand2, expand3, sumdim, unroll_dim) + _trilinear(i1_p, i2_p, i3_t, expand1, expand2, expand3, sumdim, unroll_dim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _unique2(c10::DispatchKeySet ks, const at::Tensor & self, bool sorted, bool return_inverse, bool return_counts) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<Unique2Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Unique2Backward0>(new Unique2Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_unique2", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor>>("_unique2", *opt_op, ks, self, sorted, return_inverse, return_counts);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_unique2(ks & c10::after_autograd_keyset, self_, sorted, return_inverse, return_counts);
    }
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _unique2");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _unique2");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _unique2");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _unique2");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: _unique2");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: _unique2");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_unique2");
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
at::Tensor _unsafe_index_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  check_no_requires_grad(indices, "indices", "_unsafe_index");
  std::shared_ptr<UnsafeIndexBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnsafeIndexBackward0>(new UnsafeIndexBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->indices_ = make_saved_variable_list(indices, false);
    grad_fn->self_options = self.options();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> indices_storage_saved(indices.size());
  for (const c10::optional<Tensor>& tensor : indices)
    indices_storage_saved.push_back(
      tensor.has_value() && tensor->has_storage() ? c10::optional<Storage>(tensor->storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> indices_impl_saved(indices.size());
  for (size_t i=0; i<indices.size(); i++) {
    c10::optional<Tensor> t = indices[i];
    if (t.has_value() && t->defined()) indices_impl_saved[i] = t->getIntrusivePtr();
  }
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_unsafe_index(ks & c10::after_autograd_keyset, self_, indices);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<indices.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (indices_storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(indices))
      TORCH_INTERNAL_ASSERT(indices_storage_saved[i].value().is_alias_of(
          static_cast<c10::optional<Tensor>>(indices[i])->storage()));
  }
  for (size_t i=0; i<indices.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (indices_impl_saved[i])
      TORCH_INTERNAL_ASSERT(
        indices_impl_saved[i] == static_cast<c10::optional<Tensor>>(indices[i])->getIntrusivePtr());
  }
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _unsafe_index_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _unsafe_index_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_unsafe_index");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_unsafe_index(self_t, indices);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & _upsample_bilinear2d_aa_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& grad_input_ = unpack(grad_input, "grad_input", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output )) {
    throw_error_out_requires_grad("_upsample_bilinear2d_aa_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("_upsample_bilinear2d_aa_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_upsample_bilinear2d_aa_backward_symint_outf(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, align_corners, scales_h, scales_w, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(grad_input))), "Trying to use forward AD with _upsample_bilinear2d_aa_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & _upsample_nearest_exact2d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& grad_input_ = unpack(grad_input, "grad_input", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output )) {
    throw_error_out_requires_grad("_upsample_nearest_exact2d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("_upsample_nearest_exact2d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_upsample_nearest_exact2d_backward_symint_outf(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, scales_h, scales_w, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(grad_input))), "Trying to use forward AD with _upsample_nearest_exact2d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor _upsample_nearest_exact3d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<UpsampleNearestExact3DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleNearestExact3DBackwardBackward0>(new UpsampleNearestExact3DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output ));
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_d = scales_d;
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_upsample_nearest_exact3d_backward_symint(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, scales_d, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _upsample_nearest_exact3d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _upsample_nearest_exact3d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_upsample_nearest_exact3d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result_new_fw_grad_opt = at::_upsample_nearest_exact3d_backward_symint(grad_output_t, output_size, input_size, scales_d, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _values(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::_values(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _values");
  #endif
  return result;
}
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool2d(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef output_size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(self));
  std::shared_ptr<AdaptiveMaxPool2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AdaptiveMaxPool2DBackward0>(new AdaptiveMaxPool2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::adaptive_max_pool2d(ks & c10::after_autograd_keyset, self_, output_size);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: adaptive_max_pool2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: adaptive_max_pool2d");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: adaptive_max_pool2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: adaptive_max_pool2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "adaptive_max_pool2d");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result0_new_fw_grad_opt = gather(self_t.flatten(-2), -1, result1.flatten(-2)).view_as(result1);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor & adaptive_max_pool2d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& indices_ = unpack(indices, "indices", 2);
  auto& grad_input_ = unpack(grad_input, "grad_input", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self, indices )) {
    throw_error_out_requires_grad("adaptive_max_pool2d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("adaptive_max_pool2d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::adaptive_max_pool2d_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, indices_, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(indices) || isFwGradDefined(grad_input))), "Trying to use forward AD with adaptive_max_pool2d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool3d(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef output_size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(self));
  std::shared_ptr<AdaptiveMaxPool3DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AdaptiveMaxPool3DBackward0>(new AdaptiveMaxPool3DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::adaptive_max_pool3d(ks & c10::after_autograd_keyset, self_, output_size);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: adaptive_max_pool3d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: adaptive_max_pool3d");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: adaptive_max_pool3d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: adaptive_max_pool3d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "adaptive_max_pool3d");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result0_new_fw_grad_opt = gather(self_t.flatten(-3), -1, result1.flatten(-3)).view_as(result1);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor & add__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(other));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<AddBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AddBackward0>(new AddBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->alpha = alpha;
    grad_fn->other_scalar_type = other.scalar_type();
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::add_(ks & c10::after_autograd_keyset, self_, other_, alpha);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(self_t + maybe_multiply(other_t, alpha)) : self_t + maybe_multiply(other_t, alpha);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & add__Scalar(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<AddBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AddBackward1>(new AddBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::add_(ks & c10::after_autograd_keyset, self_, other, alpha);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(self_t.clone()) : self_t.clone();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor addbmm(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  auto& batch1_ = unpack(batch1, "batch1", 1);
  auto& batch2_ = unpack(batch2, "batch2", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, batch1, batch2 );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(batch1) || isFwGradDefined(batch2));
  std::shared_ptr<AddbmmBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AddbmmBackward0>(new AddbmmBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, batch1, batch2 ));
    grad_fn->alpha = alpha;
    if (grad_fn->should_compute_output(2)) {
      grad_fn->batch1_ = SavedVariable(batch1, false);
    }
    grad_fn->batch1_sym_argsize_0 = batch1.sym_size(0);
    grad_fn->batch1_sym_argsize_1 = batch1.sym_size(1);
    if (grad_fn->should_compute_output(1)) {
      grad_fn->batch2_ = SavedVariable(batch2, false);
    }
    grad_fn->batch2_sym_argsize_2 = batch2.sym_size(2);
    grad_fn->beta = beta;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> batch1__storage_saved =
    batch1_.has_storage() ? c10::optional<Storage>(batch1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> batch1__impl_saved;
  if (batch1_.defined()) batch1__impl_saved = batch1_.getIntrusivePtr();
  c10::optional<Storage> batch2__storage_saved =
    batch2_.has_storage() ? c10::optional<Storage>(batch2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> batch2__impl_saved;
  if (batch2_.defined()) batch2__impl_saved = batch2_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::addbmm(ks & c10::after_autograd_keyset, self_, batch1_, batch2_, beta, alpha);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (batch1__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(batch1_))
    TORCH_INTERNAL_ASSERT(batch1__storage_saved.value().is_alias_of(batch1_.storage()));
  if (batch1__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(batch1_))
    TORCH_INTERNAL_ASSERT(batch1__impl_saved == batch1_.getIntrusivePtr());
  if (batch2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(batch2_))
    TORCH_INTERNAL_ASSERT(batch2__storage_saved.value().is_alias_of(batch2_.storage()));
  if (batch2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(batch2_))
    TORCH_INTERNAL_ASSERT(batch2__impl_saved == batch2_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: addbmm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: addbmm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto batch1_t_raw = toNonOptFwGrad(batch1);
      auto batch1_tensor = toNonOptTensor(batch1);
      auto batch1_t = (batch1_t_raw.defined() || !batch1_tensor.defined())
        ? batch1_t_raw : at::_efficientzerotensor(batch1_tensor.sizes(), batch1_tensor.options());
      auto batch1_p = toNonOptPrimal(batch1);
      auto batch2_t_raw = toNonOptFwGrad(batch2);
      auto batch2_tensor = toNonOptTensor(batch2);
      auto batch2_t = (batch2_t_raw.defined() || !batch2_tensor.defined())
        ? batch2_t_raw : at::_efficientzerotensor(batch2_tensor.sizes(), batch2_tensor.options());
      auto batch2_p = toNonOptPrimal(batch2);
      result_new_fw_grad_opt = maybe_multiply(self_t, beta) + maybe_multiply(batch1_t.bmm(batch2_p).sum(0), alpha) + maybe_multiply(batch1_p.bmm(batch2_t).sum(0), alpha);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & addcmul_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& tensor1_ = unpack(tensor1, "tensor1", 1);
  auto& tensor2_ = unpack(tensor2, "tensor2", 2);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, tensor1, tensor2 );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(tensor1) || isFwGradDefined(tensor2));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, tensor1, tensor2 )) {
    throw_error_out_requires_grad("addcmul");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("addcmul");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> tensor1__storage_saved =
    tensor1_.has_storage() ? c10::optional<Storage>(tensor1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> tensor1__impl_saved;
  if (tensor1_.defined()) tensor1__impl_saved = tensor1_.getIntrusivePtr();
  c10::optional<Storage> tensor2__storage_saved =
    tensor2_.has_storage() ? c10::optional<Storage>(tensor2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> tensor2__impl_saved;
  if (tensor2_.defined()) tensor2__impl_saved = tensor2_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::addcmul_outf(ks & c10::after_autograd_keyset, self_, tensor1_, tensor2_, value, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (tensor1__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(tensor1_))
    TORCH_INTERNAL_ASSERT(tensor1__storage_saved.value().is_alias_of(tensor1_.storage()));
  if (tensor1__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(tensor1_))
    TORCH_INTERNAL_ASSERT(tensor1__impl_saved == tensor1_.getIntrusivePtr());
  if (tensor2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(tensor2_))
    TORCH_INTERNAL_ASSERT(tensor2__storage_saved.value().is_alias_of(tensor2_.storage()));
  if (tensor2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(tensor2_))
    TORCH_INTERNAL_ASSERT(tensor2__impl_saved == tensor2_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(tensor1) || isFwGradDefined(tensor2) || isFwGradDefined(out))), "Trying to use forward AD with addcmul_out that does not support it because it is an out= function");
  return out;
}
at::Tensor affine_grid_generator(c10::DispatchKeySet ks, const at::Tensor & theta, c10::SymIntArrayRef size, bool align_corners) {
  auto& theta_ = unpack(theta, "theta", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( theta );
  
  std::shared_ptr<AffineGridGeneratorBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AffineGridGeneratorBackward0>(new AffineGridGeneratorBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( theta ));
    grad_fn->align_corners = align_corners;
    grad_fn->size = size.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> theta__storage_saved =
    theta_.has_storage() ? c10::optional<Storage>(theta_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> theta__impl_saved;
  if (theta_.defined()) theta__impl_saved = theta_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(theta))) {
      static c10::OperatorName full_name("aten::affine_grid_generator", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("affine_grid_generator", *opt_op, ks, theta, size, align_corners);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::affine_grid_generator_symint(ks & c10::after_autograd_keyset, theta_, size, align_corners);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (theta__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(theta_))
    TORCH_INTERNAL_ASSERT(theta__storage_saved.value().is_alias_of(theta_.storage()));
  if (theta__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(theta_))
    TORCH_INTERNAL_ASSERT(theta__impl_saved == theta_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: affine_grid_generator");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: affine_grid_generator");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "affine_grid_generator");
  return result;
}
at::Tensor & any_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::any_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with any_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & any_out_dims_out(c10::DispatchKeySet ks, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::any_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with any_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & any_out_all_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::any_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with any_out that does not support it because it is an out= function");
  return out;
}
at::Tensor as_strided_copy(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<AsStridedBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AsStridedBackward0_copy>(new AsStridedBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_geometry = TensorGeometry(self);
    grad_fn->size = size.vec();
    grad_fn->storage_offset = storage_offset;
    grad_fn->stride = stride.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::as_strided_copy_symint(ks & c10::after_autograd_keyset, self_, size, stride, storage_offset);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: as_strided_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: as_strided_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "as_strided_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::as_strided_symint(self_t, size, stride, storage_offset);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & atan_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("atan");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("atan");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::atan_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with atan_out that does not support it because it is an out= function");
  return out;
}
at::Tensor atanh(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<AtanhBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AtanhBackward0>(new AtanhBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::atanh(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: atanh");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: atanh");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() * 1 / (1 - self_p.pow(2)).conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor avg_pool2d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<AvgPool2DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AvgPool2DBackwardBackward0>(new AvgPool2DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->ceil_mode = ceil_mode;
    grad_fn->count_include_pad = count_include_pad;
    grad_fn->divisor_override = divisor_override;
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_info = self;
    grad_fn->stride = stride.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::avg_pool2d_backward(ks & c10::after_autograd_keyset, grad_output_, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: avg_pool2d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: avg_pool2d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "avg_pool2d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = avg_pool2d_backward(grad_output_t, self_p, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & avg_pool3d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& grad_input_ = unpack(grad_input, "grad_input", 8);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self )) {
    throw_error_out_requires_grad("avg_pool3d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("avg_pool3d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::avg_pool3d_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with avg_pool3d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & avg_pool3d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 7);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("avg_pool3d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("avg_pool3d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::avg_pool3d_outf(ks & c10::after_autograd_keyset, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with avg_pool3d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & bernoulli__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & p, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  auto& p_ = unpack(p, "p", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, p );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<BernoulliBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<BernoulliBackward1>(new BernoulliBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, p ));
    grad_fn->p_info = p;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> p__storage_saved =
    p_.has_storage() ? c10::optional<Storage>(p_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> p__impl_saved;
  if (p_.defined()) p__impl_saved = p_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::bernoulli_(ks & c10::after_autograd_keyset, self_, p_, generator);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (p__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(p_))
    TORCH_INTERNAL_ASSERT(p__storage_saved.value().is_alias_of(p_.storage()));
  if (p__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(p_))
    TORCH_INTERNAL_ASSERT(p__impl_saved == p_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & bernoulli__float(c10::DispatchKeySet ks, at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<BernoulliBackward2> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<BernoulliBackward2>(new BernoulliBackward2(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::bernoulli_(ks & c10::after_autograd_keyset, self_, p, generator);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor binary_cross_entropy(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
  auto& self_ = unpack(self, "self", 0);
  auto& target_ = unpack(target, "target", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, target );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(target));
  check_no_requires_grad(weight, "weight", "binary_cross_entropy");
  std::shared_ptr<BinaryCrossEntropyBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<BinaryCrossEntropyBackward0>(new BinaryCrossEntropyBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, target ));
    grad_fn->reduction = reduction;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->target_ = SavedVariable(target, false);
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::binary_cross_entropy(ks & c10::after_autograd_keyset, self_, target_, weight, reduction);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: binary_cross_entropy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: binary_cross_entropy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "binary_cross_entropy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto target_t_raw = toNonOptFwGrad(target);
      auto target_tensor = toNonOptTensor(target);
      auto target_t = (target_t_raw.defined() || !target_tensor.defined())
        ? target_t_raw : at::_efficientzerotensor(target_tensor.sizes(), target_tensor.options());
      auto target_p = toNonOptPrimal(target);
      result_new_fw_grad_opt = apply_loss_reduction( binary_cross_entropy_backward(self_t, self_p, target_p, weight, at::Reduction::None) + binary_cross_entropy_target_backward(target_t, self_p, target_p, weight, at::Reduction::None), reduction);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor bmm(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mat2) {
  auto& self_ = unpack(self, "self", 0);
  auto& mat2_ = unpack(mat2, "mat2", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, mat2 );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(mat2));
  std::shared_ptr<BmmBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<BmmBackward0>(new BmmBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, mat2 ));
    if (grad_fn->should_compute_output(0)) {
      grad_fn->mat2_ = SavedVariable(mat2, false);
    }
    if (grad_fn->should_compute_output(1)) {
      grad_fn->self_ = SavedVariable(self, false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mat2__storage_saved =
    mat2_.has_storage() ? c10::optional<Storage>(mat2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mat2__impl_saved;
  if (mat2_.defined()) mat2__impl_saved = mat2_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::bmm(ks & c10::after_autograd_keyset, self_, mat2_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mat2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mat2_))
    TORCH_INTERNAL_ASSERT(mat2__storage_saved.value().is_alias_of(mat2_.storage()));
  if (mat2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mat2_))
    TORCH_INTERNAL_ASSERT(mat2__impl_saved == mat2_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: bmm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: bmm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto mat2_t_raw = toNonOptFwGrad(mat2);
      auto mat2_tensor = toNonOptTensor(mat2);
      auto mat2_t = (mat2_t_raw.defined() || !mat2_tensor.defined())
        ? mat2_t_raw : at::_efficientzerotensor(mat2_tensor.sizes(), mat2_tensor.options());
      auto mat2_p = toNonOptPrimal(mat2);
      result_new_fw_grad_opt = self_t.bmm(mat2_p) + self_p.bmm(mat2_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & bmm_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& mat2_ = unpack(mat2, "mat2", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, mat2 );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(mat2));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, mat2 )) {
    throw_error_out_requires_grad("bmm");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("bmm");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mat2__storage_saved =
    mat2_.has_storage() ? c10::optional<Storage>(mat2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mat2__impl_saved;
  if (mat2_.defined()) mat2__impl_saved = mat2_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::bmm_outf(ks & c10::after_autograd_keyset, self_, mat2_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mat2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mat2_))
    TORCH_INTERNAL_ASSERT(mat2__storage_saved.value().is_alias_of(mat2_.storage()));
  if (mat2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mat2_))
    TORCH_INTERNAL_ASSERT(mat2__impl_saved == mat2_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(mat2) || isFwGradDefined(out))), "Trying to use forward AD with bmm_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & cauchy_(c10::DispatchKeySet ks, at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<CauchyBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CauchyBackward0>(new CauchyBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::cauchy_(ks & c10::after_autograd_keyset, self_, median, sigma, generator);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor ccol_indices_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::ccol_indices_copy(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: ccol_indices_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: ccol_indices_copy");
  #endif
  return result;
}
at::Tensor celu(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<CeluBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CeluBackward0>(new CeluBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->alpha = alpha;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::celu(ks & c10::after_autograd_keyset, self_, alpha);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: celu");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: celu");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "celu");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (elu_backward(self_t.conj(), alpha, 1, 1.0/alpha.toFloat(), /* is_result */ false, self_p)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & celu_(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<CeluBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CeluBackward1>(new CeluBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->alpha = alpha;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::celu_(ks & c10::after_autograd_keyset, self_, alpha);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t.copy_(elu_backward(original_self_t, alpha, 1, 1.0/alpha.toFloat(), /* is_result */ true, self_p));
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor clamp_min(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & min) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ClampMinBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ClampMinBackward0>(new ClampMinBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->min = min;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::clamp_min(ks & c10::after_autograd_keyset, self_, min);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: clamp_min");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: clamp_min");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "clamp_min");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (where(self_p >= min, self_t.conj(), at::scalar_tensor(0., self_t.conj().options()))).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor clamp_min_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & min) {
  auto& self_ = unpack(self, "self", 0);
  auto& min_ = unpack(min, "min", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, min );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(min));
  std::shared_ptr<ClampMinBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ClampMinBackward1>(new ClampMinBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, min ));
    grad_fn->min_ = SavedVariable(min, false);
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> min__storage_saved =
    min_.has_storage() ? c10::optional<Storage>(min_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> min__impl_saved;
  if (min_.defined()) min__impl_saved = min_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::clamp_min(ks & c10::after_autograd_keyset, self_, min_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (min__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(min_))
    TORCH_INTERNAL_ASSERT(min__storage_saved.value().is_alias_of(min_.storage()));
  if (min__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(min_))
    TORCH_INTERNAL_ASSERT(min__impl_saved == min_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: clamp_min_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: clamp_min_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "clamp_min");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto min_t_raw = toNonOptFwGrad(min);
      auto min_tensor = toNonOptTensor(min);
      auto min_t = (min_t_raw.defined() || !min_tensor.defined())
        ? min_t_raw : at::_efficientzerotensor(min_tensor.sizes(), min_tensor.options());
      auto min_p = toNonOptPrimal(min);
      result_new_fw_grad_opt = where(self_p >= min_p, self_t, min_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & clamp_min_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & min, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("clamp_min");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("clamp_min");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::clamp_min_outf(ks & c10::after_autograd_keyset, self_, min, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with clamp_min_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & clamp_min_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & min, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& min_ = unpack(min, "min", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, min );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(min));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, min )) {
    throw_error_out_requires_grad("clamp_min");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("clamp_min");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> min__storage_saved =
    min_.has_storage() ? c10::optional<Storage>(min_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> min__impl_saved;
  if (min_.defined()) min__impl_saved = min_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::clamp_min_outf(ks & c10::after_autograd_keyset, self_, min_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (min__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(min_))
    TORCH_INTERNAL_ASSERT(min__storage_saved.value().is_alias_of(min_.storage()));
  if (min__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(min_))
    TORCH_INTERNAL_ASSERT(min__impl_saved == min_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(min) || isFwGradDefined(out))), "Trying to use forward AD with clamp_min_out that does not support it because it is an out= function");
  return out;
}
at::Tensor col2im(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Col2ImBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Col2ImBackward0>(new Col2ImBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dilation = dilation.vec();
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->padding = padding.vec();
    grad_fn->stride = stride.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::col2im_symint(ks & c10::after_autograd_keyset, self_, output_size, kernel_size, dilation, padding, stride);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: col2im");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: col2im");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::col2im_symint(self_t, output_size, kernel_size, dilation, padding, stride);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & conj_physical_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<ConjPhysicalBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ConjPhysicalBackward1>(new ConjPhysicalBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::conj_physical_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.conj_physical_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor conv_depthwise3d(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<ConvDepthwise3DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ConvDepthwise3DBackward0>(new ConvDepthwise3DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight, bias ));
    grad_fn->bias_sym_sizes_opt = bias.has_value() ? c10::optional<c10::SymIntArrayRef>(bias->sym_sizes()) : c10::nullopt;
    grad_fn->dilation = dilation.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::conv_depthwise3d", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("conv_depthwise3d", *opt_op, ks, self, weight, kernel_size, bias, stride, padding, dilation);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::conv_depthwise3d_symint(ks & c10::after_autograd_keyset, self_, weight_, kernel_size, bias, stride, padding, dilation);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: conv_depthwise3d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: conv_depthwise3d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "conv_depthwise3d");
  return result;
}
at::Tensor & cosh_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<CoshBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CoshBackward0>(new CoshBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::cosh_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * original_self_p.sinh().conj()).conj()) : (original_self_t.conj() * original_self_p.sinh().conj()).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & cosh_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("cosh");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("cosh");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::cosh_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with cosh_out that does not support it because it is an out= function");
  return out;
}
at::Tensor crow_indices(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::crow_indices(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: crow_indices");
  #endif
  return result;
}
at::Tensor cudnn_convolution(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight );
  
  std::shared_ptr<CudnnConvolutionBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CudnnConvolutionBackward0>(new CudnnConvolutionBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight ));
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight))) {
      static c10::OperatorName full_name("aten::cudnn_convolution", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("cudnn_convolution", *opt_op, ks, self, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::cudnn_convolution_symint(ks & c10::after_autograd_keyset, self_, weight_, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: cudnn_convolution");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: cudnn_convolution");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "cudnn_convolution");
  return result;
}
::std::tuple<at::Tensor &,at::Tensor &> cummax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
  auto& self_ = unpack(self, "self", 0);
  auto& values_ = unpack(values, "values", 2);
  auto& indices_ = unpack(indices, "indices", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("cummax");
  }
  if (compute_requires_grad( values )) {
    throw_error_out_requires_grad("cummax");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> values__storage_saved =
    values_.has_storage() ? c10::optional<Storage>(values_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> values__impl_saved;
  if (values_.defined()) values__impl_saved = values_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::cummax_outf(ks & c10::after_autograd_keyset, self_, dim, values_, indices_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__storage_saved.value().is_alias_of(values_.storage()));
  if (values__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__impl_saved == values_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( values ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(values) || isFwGradDefined(indices))), "Trying to use forward AD with cummax_out that does not support it because it is an out= function");
  return std::forward_as_tuple(values, indices);
}
at::Tensor cumprod(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<CumprodBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CumprodBackward0>(new CumprodBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::cumprod(ks & c10::after_autograd_keyset, self_, dim, dtype);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: cumprod");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: cumprod");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = cumprod_jvp(self_t, self_p, result, dim).to(dtype.has_value() ? *dtype : self_p.scalar_type());
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor diag_embed(c10::DispatchKeySet ks, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<DiagEmbedBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DiagEmbedBackward0>(new DiagEmbedBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim1 = dim1;
    grad_fn->dim2 = dim2;
    grad_fn->offset = offset;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::diag_embed(ks & c10::after_autograd_keyset, self_, offset, dim1, dim2);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: diag_embed");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: diag_embed");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::diag_embed(self_t, offset, dim1, dim2);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor diagonal(c10::DispatchKeySet ks, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<DiagonalBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DiagonalBackward0>(new DiagonalBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim1 = dim1;
    grad_fn->dim2 = dim2;
    grad_fn->offset = offset;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::diagonal(ks & c10::after_autograd_keyset, self_, offset, dim1, dim2);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: diagonal");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::diagonal(self_t, offset, dim1, dim2);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & div__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(other));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<DivBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DivBackward0>(new DivBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    if (grad_fn->should_compute_output(1)) {
      if (!original_self.has_value()) original_self = self.clone();
      grad_fn->self_ = SavedVariable(original_self.value(), false);
    }
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::div_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((self_t - other_t * self_p) / other_p) : (self_t - other_t * self_p) / other_p;
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & div__Tensor_mode(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(other));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<DivBackward2> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DivBackward2>(new DivBackward2(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->rounding_mode = rounding_mode.has_value() ? c10::optional<std::string>(std::string(rounding_mode.value())) : c10::nullopt;
    if (grad_fn->should_compute_output(1)) {
      if (!original_self.has_value()) original_self = self.clone();
      grad_fn->self_ = SavedVariable(original_self.value(), false);
    }
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::div_(ks & c10::after_autograd_keyset, self_, other_, rounding_mode);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(rounding_mode.has_value() ? self_p.new_zeros_symint(self_p.sym_sizes()) : original_self_t / other_p - other_t * (original_self_p / other_p) / other_p) : rounding_mode.has_value() ? self_p.new_zeros_symint(self_p.sym_sizes()) : original_self_t / other_p - other_t * (original_self_p / other_p) / other_p;
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & div__Scalar(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<DivBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DivBackward1>(new DivBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->other = other;
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::div_(ks & c10::after_autograd_keyset, self_, other);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(self_t / other) : self_t / other;
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & div__Scalar_mode(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<DivBackward3> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DivBackward3>(new DivBackward3(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->other = other;
    grad_fn->rounding_mode = rounding_mode.has_value() ? c10::optional<std::string>(std::string(rounding_mode.value())) : c10::nullopt;
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::div_(ks & c10::after_autograd_keyset, self_, other, rounding_mode);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(rounding_mode.has_value() ? self_p.new_zeros_symint(self_p.sym_sizes()) : self_t / other) : rounding_mode.has_value() ? self_p.new_zeros_symint(self_p.sym_sizes()) : self_t / other;
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & embedding_renorm_(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & indices, double max_norm, double norm_type) {
  auto& self_ = unpack(self, "self", 0);
  auto& indices_ = unpack(indices, "indices", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<EmbeddingRenormBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<EmbeddingRenormBackward0>(new EmbeddingRenormBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::embedding_renorm_(ks & c10::after_autograd_keyset, self_, indices_, max_norm, norm_type);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self))), "Trying to use forward AD with embedding_renorm_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
  return self;
}
at::Tensor empty_like(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::empty_like(ks & c10::after_autograd_keyset, self_, dtype, layout, device, pin_memory, memory_format);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: empty_like");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: empty_like");
  #endif
  return result;
}
at::Tensor & eq__Scalar(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<EqBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<EqBackward0>(new EqBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::eq_(ks & c10::after_autograd_keyset, self_, other);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & eq__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<EqBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<EqBackward1>(new EqBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_info = other;
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::eq_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
bool equal(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::equal(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor & erf_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<ErfBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ErfBackward0>(new ErfBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::erf_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((2.0 / sqrt(M_PI) * exp(-(original_self_p.pow(2))) * original_self_t.conj()).conj()) : (2.0 / sqrt(M_PI) * exp(-(original_self_p.pow(2))) * original_self_t.conj()).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & exp2_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<Exp2Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Exp2Backward0>(new Exp2Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::exp2_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((self_t.conj() * self_p.conj() * M_LN2).conj()) : (self_t.conj() * self_p.conj() * M_LN2).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor & exp2_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("exp2");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("exp2");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::exp2_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with exp2_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & exp_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("exp");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("exp");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::exp_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with exp_out that does not support it because it is an out= function");
  return out;
}
at::Tensor expand(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, bool implicit) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ExpandBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ExpandBackward0>(new ExpandBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::expand_symint(ks & c10::after_autograd_keyset, self_, size, implicit);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: expand");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t.expand_symint(size, implicit);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor flip(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dims) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<FlipBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FlipBackward0>(new FlipBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dims = dims.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::flip(ks & c10::after_autograd_keyset, self_, dims);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: flip");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: flip");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::flip(self_t, dims);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor floor(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<FloorBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FloorBackward0>(new FloorBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::floor(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: floor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: floor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "floor");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (zeros_like(self_t.conj())).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & floor_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("floor");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("floor");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::floor_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with floor_out that does not support it because it is an out= function");
  return out;
}
at::Tensor fmin(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<FminBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FminBackward0>(new FminBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::fmin(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: fmin");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: fmin");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "fmin");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      result_new_fw_grad_opt = other_t + (self_p <= other_p).logical_or_(other_p.isnan()) * (self_t - other_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & frac_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("frac");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("frac");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::frac_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with frac_out that does not support it because it is an out= function");
  return out;
}
::std::tuple<at::Tensor,at::Tensor> fractional_max_pool2d(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
  auto& self_ = unpack(self, "self", 0);
  auto& random_samples_ = unpack(random_samples, "random_samples", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(self));
  check_no_requires_grad(random_samples, "random_samples", "fractional_max_pool2d");
  std::shared_ptr<FractionalMaxPool2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FractionalMaxPool2DBackward0>(new FractionalMaxPool2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->output_size = output_size.vec();
    grad_fn->self_ = SavedVariable(self, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> random_samples__storage_saved =
    random_samples_.has_storage() ? c10::optional<Storage>(random_samples_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> random_samples__impl_saved;
  if (random_samples_.defined()) random_samples__impl_saved = random_samples_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::fractional_max_pool2d(ks & c10::after_autograd_keyset, self_, kernel_size, output_size, random_samples_);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (random_samples__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(random_samples_))
    TORCH_INTERNAL_ASSERT(random_samples__storage_saved.value().is_alias_of(random_samples_.storage()));
  if (random_samples__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(random_samples_))
    TORCH_INTERNAL_ASSERT(random_samples__impl_saved == random_samples_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: fractional_max_pool2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: fractional_max_pool2d");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: fractional_max_pool2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: fractional_max_pool2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "fractional_max_pool2d");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result0_new_fw_grad_opt = gather(self_t.flatten(-2), -1, result1.flatten(-2)).view_as(result1);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
::std::tuple<at::Tensor,at::Tensor> frexp_Tensor(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_mantissa = (isFwGradDefined(self));
  std::shared_ptr<FrexpBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FrexpBackward0>(new FrexpBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  at::Tensor mantissa;
  at::Tensor exponent;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::frexp(ks & c10::after_autograd_keyset, self_);
  })();
  std::tie(mantissa, exponent) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mantissa.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mantissa)) {
    TORCH_INTERNAL_ASSERT(mantissa.storage().use_count() == 1, "function: frexp_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mantissa))
    TORCH_INTERNAL_ASSERT(mantissa.use_count() <= 1, "function: frexp_Tensor");
  if (exponent.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(exponent)) {
    TORCH_INTERNAL_ASSERT(exponent.storage().use_count() == 1, "function: frexp_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(exponent))
    TORCH_INTERNAL_ASSERT(exponent.use_count() <= 1, "function: frexp_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( mantissa ), grad_fn);
  }
  throw_error_for_complex_autograd(mantissa, "frexp");
  c10::optional<at::Tensor> mantissa_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_mantissa && (mantissa.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      mantissa_new_fw_grad_opt = self_t / exponent.exp2();
  }
  if (mantissa_new_fw_grad_opt.has_value() && mantissa_new_fw_grad_opt.value().defined() && mantissa.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    mantissa._set_fw_grad(mantissa_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->exponent_ = SavedVariable(exponent, true);
  }
  return std::make_tuple(std::move(mantissa), std::move(exponent));
}
at::Tensor gather(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<GatherBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GatherBackward0>(new GatherBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->index_ = SavedVariable(index, false);
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->sparse_grad = sparse_grad;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::gather(ks & c10::after_autograd_keyset, self_, dim, index_, sparse_grad);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: gather");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: gather");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::gather(self_t, dim, index, sparse_grad);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & gather_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("gather");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("gather");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::gather_outf(ks & c10::after_autograd_keyset, self_, dim, index_, sparse_grad, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with gather_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & ge__Scalar(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<GeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GeBackward0>(new GeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::ge_(ks & c10::after_autograd_keyset, self_, other);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & ge__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<GeBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GeBackward1>(new GeBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_info = other;
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::ge_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & gelu_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::string_view approximate, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("gelu");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("gelu");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::gelu_outf(ks & c10::after_autograd_keyset, self_, approximate, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with gelu_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & glu_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, int64_t dim, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& grad_input_ = unpack(grad_input, "grad_input", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self )) {
    throw_error_out_requires_grad("glu_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("glu_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::glu_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, dim, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with glu_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & gt_out_Scalar_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::gt_outf(ks & c10::after_autograd_keyset, self_, other, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with gt_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & gt_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::gt_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with gt_out that does not support it because it is an out= function");
  return out;
}
at::Tensor hardsigmoid(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<HardsigmoidBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<HardsigmoidBackward0>(new HardsigmoidBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::hardsigmoid(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: hardsigmoid");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: hardsigmoid");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "hardsigmoid");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (hardsigmoid_backward(self_t.conj(), self_p)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor huber_loss(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
  auto& self_ = unpack(self, "self", 0);
  auto& target_ = unpack(target, "target", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, target );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(target));
  std::shared_ptr<HuberLossBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<HuberLossBackward0>(new HuberLossBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, target ));
    grad_fn->delta = delta;
    grad_fn->reduction = reduction;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->target_ = SavedVariable(target, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::huber_loss(ks & c10::after_autograd_keyset, self_, target_, reduction, delta);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: huber_loss");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: huber_loss");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "huber_loss");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto target_t_raw = toNonOptFwGrad(target);
      auto target_tensor = toNonOptTensor(target);
      auto target_t = (target_t_raw.defined() || !target_tensor.defined())
        ? target_t_raw : at::_efficientzerotensor(target_tensor.sizes(), target_tensor.options());
      auto target_p = toNonOptPrimal(target);
      result_new_fw_grad_opt = apply_loss_reduction(huber_loss_backward(self_t.conj(), self_p, target_p, at::Reduction::None, delta).conj() + huber_loss_backward(target_t.conj(), target_p, self_p, at::Reduction::None, delta).conj(), reduction);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor huber_loss_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& target_ = unpack(target, "target", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self, target );
  
  std::shared_ptr<HuberLossBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<HuberLossBackwardBackward0>(new HuberLossBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self, target ));
    grad_fn->delta = delta;
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->reduction = reduction;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->target_ = SavedVariable(target, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target))) {
      static c10::OperatorName full_name("aten::huber_loss_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("huber_loss_backward", *opt_op, ks, grad_output, self, target, reduction, delta);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::huber_loss_backward(ks & c10::after_autograd_keyset, grad_output_, self_, target_, reduction, delta);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: huber_loss_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: huber_loss_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "huber_loss_backward");
  return result;
}
at::Tensor & hypot_(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(other));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<HypotBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<HypotBackward0>(new HypotBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    if (grad_fn->should_compute_output(1)) {
      grad_fn->other_ = SavedVariable(other, false);
    }
    if (grad_fn->should_compute_output(0)) {
      if (!original_self.has_value()) original_self = self.clone();
      grad_fn->self_ = SavedVariable(original_self.value(), false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::hypot_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(original_self_t * original_self_p / self_p + other_t * other_p / self_p) : original_self_t * original_self_p / self_p + other_t * other_p / self_p;
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor & hypot_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("hypot");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("hypot");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::hypot_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with hypot_out that does not support it because it is an out= function");
  return out;
}
at::Tensor i0(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<I0Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<I0Backward0>(new I0Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::i0(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: i0");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: i0");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "i0");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() * at::special_i1(self_p)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & index_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, indices )) {
    throw_error_out_requires_grad("index");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("index");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> indices_storage_saved(indices.size());
  for (const c10::optional<Tensor>& tensor : indices)
    indices_storage_saved.push_back(
      tensor.has_value() && tensor->has_storage() ? c10::optional<Storage>(tensor->storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> indices_impl_saved(indices.size());
  for (size_t i=0; i<indices.size(); i++) {
    c10::optional<Tensor> t = indices[i];
    if (t.has_value() && t->defined()) indices_impl_saved[i] = t->getIntrusivePtr();
  }
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::index_outf(ks & c10::after_autograd_keyset, self_, indices, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<indices.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (indices_storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(indices))
      TORCH_INTERNAL_ASSERT(indices_storage_saved[i].value().is_alias_of(
          static_cast<c10::optional<Tensor>>(indices[i])->storage()));
  }
  for (size_t i=0; i<indices.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (indices_impl_saved[i])
      TORCH_INTERNAL_ASSERT(
        indices_impl_saved[i] == static_cast<c10::optional<Tensor>>(indices[i])->getIntrusivePtr());
  }
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefinedTensorList(indices) || isFwGradDefined(out))), "Trying to use forward AD with index_out that does not support it because it is an out= function");
  return out;
}
at::Tensor indices_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::indices_copy(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: indices_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: indices_copy");
  #endif
  return result;
}
bool is_set_to(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & tensor) {
  auto& self_ = unpack(self, "self", 0);
  auto& tensor_ = unpack(tensor, "tensor", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> tensor__storage_saved =
    tensor_.has_storage() ? c10::optional<Storage>(tensor_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> tensor__impl_saved;
  if (tensor_.defined()) tensor__impl_saved = tensor_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::is_set_to(ks & c10::after_autograd_keyset, self_, tensor_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (tensor__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(tensor_))
    TORCH_INTERNAL_ASSERT(tensor__storage_saved.value().is_alias_of(tensor_.storage()));
  if (tensor__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(tensor_))
    TORCH_INTERNAL_ASSERT(tensor__impl_saved == tensor_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor isposinf(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::isposinf(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: isposinf");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: isposinf");
  #endif
  return result;
}
at::Tensor & isposinf_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::isposinf_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with isposinf_out that does not support it because it is an out= function");
  return out;
}
at::Tensor lerp_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
  auto& self_ = unpack(self, "self", 0);
  auto& end_ = unpack(end, "end", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, end );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(end));
  std::shared_ptr<LerpBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LerpBackward0>(new LerpBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, end ));
    grad_fn->weight = weight;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> end__storage_saved =
    end_.has_storage() ? c10::optional<Storage>(end_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> end__impl_saved;
  if (end_.defined()) end__impl_saved = end_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::lerp(ks & c10::after_autograd_keyset, self_, end_, weight);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (end__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(end_))
    TORCH_INTERNAL_ASSERT(end__storage_saved.value().is_alias_of(end_.storage()));
  if (end__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(end_))
    TORCH_INTERNAL_ASSERT(end__impl_saved == end_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: lerp_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: lerp_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto end_t_raw = toNonOptFwGrad(end);
      auto end_tensor = toNonOptTensor(end);
      auto end_t = (end_t_raw.defined() || !end_tensor.defined())
        ? end_t_raw : at::_efficientzerotensor(end_tensor.sizes(), end_tensor.options());
      result_new_fw_grad_opt = at::lerp(self_t, end_t, weight);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor lerp_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
  auto& self_ = unpack(self, "self", 0);
  auto& end_ = unpack(end, "end", 1);
  auto& weight_ = unpack(weight, "weight", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, end, weight );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(end) || isFwGradDefined(weight));
  std::shared_ptr<LerpBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LerpBackward1>(new LerpBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, end, weight ));
    if (grad_fn->should_compute_output(2)) {
      grad_fn->end_ = SavedVariable(end, false);
    }
    if (grad_fn->should_compute_output(2)) {
      grad_fn->self_ = SavedVariable(self, false);
    }
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> end__storage_saved =
    end_.has_storage() ? c10::optional<Storage>(end_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> end__impl_saved;
  if (end_.defined()) end__impl_saved = end_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::lerp(ks & c10::after_autograd_keyset, self_, end_, weight_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (end__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(end_))
    TORCH_INTERNAL_ASSERT(end__storage_saved.value().is_alias_of(end_.storage()));
  if (end__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(end_))
    TORCH_INTERNAL_ASSERT(end__impl_saved == end_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: lerp_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: lerp_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto end_t_raw = toNonOptFwGrad(end);
      auto end_tensor = toNonOptTensor(end);
      auto end_t = (end_t_raw.defined() || !end_tensor.defined())
        ? end_t_raw : at::_efficientzerotensor(end_tensor.sizes(), end_tensor.options());
      auto end_p = toNonOptPrimal(end);
      auto weight_t_raw = toNonOptFwGrad(weight);
      auto weight_tensor = toNonOptTensor(weight);
      auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
        ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
      auto weight_p = toNonOptPrimal(weight);
      result_new_fw_grad_opt = at::lerp(self_t, end_t, weight_p) + weight_t * (end_p - self_p);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & linalg_cross_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, int64_t dim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("linalg_cross");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("linalg_cross");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::linalg_cross_outf(ks & c10::after_autograd_keyset, self_, other_, dim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with linalg_cross_out that does not support it because it is an out= function");
  return out;
}
::std::tuple<at::Tensor,at::Tensor> linalg_inv_ex(c10::DispatchKeySet ks, const at::Tensor & A, bool check_errors) {
  auto& A_ = unpack(A, "A", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_inverse = (isFwGradDefined(A));
  std::shared_ptr<LinalgInvExBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgInvExBackward0>(new LinalgInvExBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( A ));
  }
  at::Tensor inverse;
  at::Tensor info;
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::linalg_inv_ex(ks & c10::after_autograd_keyset, A_, check_errors);
  })();
  std::tie(inverse, info) = std::move(_tmp);
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (inverse.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(inverse)) {
    TORCH_INTERNAL_ASSERT(inverse.storage().use_count() == 1, "function: linalg_inv_ex");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(inverse))
    TORCH_INTERNAL_ASSERT(inverse.use_count() <= 1, "function: linalg_inv_ex");
  if (info.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(info)) {
    TORCH_INTERNAL_ASSERT(info.storage().use_count() == 1, "function: linalg_inv_ex");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(info))
    TORCH_INTERNAL_ASSERT(info.use_count() <= 1, "function: linalg_inv_ex");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( inverse ), grad_fn);
  }
  c10::optional<at::Tensor> inverse_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_inverse && (inverse.defined())) {
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      inverse_new_fw_grad_opt = -at::matmul(at::matmul(inverse, A_t), inverse);
  }
  if (inverse_new_fw_grad_opt.has_value() && inverse_new_fw_grad_opt.value().defined() && inverse.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    inverse._set_fw_grad(inverse_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->inverse_ = SavedVariable(inverse, true);
  }
  return std::make_tuple(std::move(inverse), std::move(info));
}
at::Tensor linalg_solve_triangular(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular) {
  auto& self_ = unpack(self, "self", 0);
  auto& B_ = unpack(B, "B", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, B );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(B));
  std::shared_ptr<LinalgSolveTriangularBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgSolveTriangularBackward0>(new LinalgSolveTriangularBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, B ));
    grad_fn->left = left;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->unitriangular = unitriangular;
    grad_fn->upper = upper;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> B__storage_saved =
    B_.has_storage() ? c10::optional<Storage>(B_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> B__impl_saved;
  if (B_.defined()) B__impl_saved = B_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::linalg_solve_triangular(ks & c10::after_autograd_keyset, self_, B_, upper, left, unitriangular);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (B__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__storage_saved.value().is_alias_of(B_.storage()));
  if (B__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__impl_saved == B_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: linalg_solve_triangular");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: linalg_solve_triangular");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto B_t_raw = toNonOptFwGrad(B);
      auto B_tensor = toNonOptTensor(B);
      auto B_t = (B_t_raw.defined() || !B_tensor.defined())
        ? B_t_raw : at::_efficientzerotensor(B_tensor.sizes(), B_tensor.options());
      result_new_fw_grad_opt = linalg_solve_triangular_forward_AD(self_t, B_t, self_p, result, upper, left, unitriangular);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & linalg_solve_triangular_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& B_ = unpack(B, "B", 1);
  auto& out_ = unpack(out, "out", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, B );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(B));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, B )) {
    throw_error_out_requires_grad("linalg_solve_triangular");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("linalg_solve_triangular");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> B__storage_saved =
    B_.has_storage() ? c10::optional<Storage>(B_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> B__impl_saved;
  if (B_.defined()) B__impl_saved = B_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::linalg_solve_triangular_outf(ks & c10::after_autograd_keyset, self_, B_, upper, left, unitriangular, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (B__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__storage_saved.value().is_alias_of(B_.storage()));
  if (B__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__impl_saved == B_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(B) || isFwGradDefined(out))), "Trying to use forward AD with linalg_solve_triangular_out that does not support it because it is an out= function");
  return out;
}
at::Tensor linalg_vector_norm(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<LinalgVectorNormBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgVectorNormBackward0>(new LinalgVectorNormBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
    grad_fn->ord = ord;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::linalg_vector_norm(ks & c10::after_autograd_keyset, self_, ord, dim, keepdim, dtype);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: linalg_vector_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: linalg_vector_norm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = linalg_vector_norm_jvp(self_p, self_t, ord, result, dim, keepdim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> linear_backward(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
  auto& self_ = unpack(self, "self", 0);
  auto& grad_output_ = unpack(grad_output, "grad_output", 1);
  auto& weight_ = unpack(weight, "weight", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, grad_output, weight );
  
  std::shared_ptr<LinearBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinearBackwardBackward0>(new LinearBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, grad_output, weight ));
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(grad_output) || isFwGradDefined(weight))) {
      static c10::OperatorName full_name("aten::linear_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor>>("linear_backward", *opt_op, ks, self, grad_output, weight, output_mask);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::linear_backward(ks & c10::after_autograd_keyset, self_, grad_output_, weight_, output_mask);
    }
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: linear_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: linear_backward");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: linear_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: linear_backward");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: linear_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: linear_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "linear_backward");
  throw_error_for_complex_autograd(result1, "linear_backward");
  throw_error_for_complex_autograd(result2, "linear_backward");
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
at::Tensor & linear_out_out(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::Tensor & out) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( input, weight, bias )) {
    throw_error_out_requires_grad("linear");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("linear");
  }
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::linear_outf(ks & c10::after_autograd_keyset, input_, weight_, bias, out_);
  }
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias) || isFwGradDefined(out))), "Trying to use forward AD with linear_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & log_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("log");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("log");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::log_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with log_out that does not support it because it is an out= function");
  return out;
}
::std::tuple<at::Tensor &,at::Tensor &> log_sigmoid_forward_out_output(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & output, at::Tensor & buffer) {
  auto& self_ = unpack(self, "self", 0);
  auto& output_ = unpack(output, "output", 1);
  auto& buffer_ = unpack(buffer, "buffer", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_output = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("log_sigmoid_forward");
  }
  if (compute_requires_grad( output )) {
    throw_error_out_requires_grad("log_sigmoid_forward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> buffer__storage_saved =
    buffer_.has_storage() ? c10::optional<Storage>(buffer_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> buffer__impl_saved;
  if (buffer_.defined()) buffer__impl_saved = buffer_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::log_sigmoid_forward_outf(ks & c10::after_autograd_keyset, self_, output_, buffer_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (buffer__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(buffer_))
    TORCH_INTERNAL_ASSERT(buffer__storage_saved.value().is_alias_of(buffer_.storage()));
  if (buffer__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(buffer_))
    TORCH_INTERNAL_ASSERT(buffer__impl_saved == buffer_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( output ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(output) || isFwGradDefined(buffer))), "Trying to use forward AD with log_sigmoid_forward_out that does not support it because it is an out= function");
  return std::forward_as_tuple(output, buffer);
}
at::Tensor & logaddexp_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("logaddexp");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("logaddexp");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::logaddexp_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with logaddexp_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & logcumsumexp_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("logcumsumexp");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("logcumsumexp");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::logcumsumexp_outf(ks & c10::after_autograd_keyset, self_, dim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with logcumsumexp_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & logical_xor_(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::logical_xor_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  return self;
}
at::Tensor & logical_xor_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::logical_xor_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with logical_xor_out that does not support it because it is an out= function");
  return out;
}
at::Tensor logit(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<double> eps) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<LogitBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LogitBackward0>(new LogitBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->eps = eps;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::logit(ks & c10::after_autograd_keyset, self_, eps);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: logit");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: logit");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "logit");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (GradMode::is_enabled() ? infinitely_differentiable_logit_backward(self_t.conj(), self_p, eps) : logit_backward(self_t.conj(), self_p, eps)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & logsumexp_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("logsumexp");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("logsumexp");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::logsumexp_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with logsumexp_out that does not support it because it is an out= function");
  return out;
}
::std::tuple<at::Tensor,::std::vector<at::Tensor>,::std::vector<at::Tensor>> lstm_mps_backward(c10::DispatchKeySet ks, const c10::optional<at::Tensor> & grad_y, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & z_state, const at::Tensor & cell_state_fwd, const at::Tensor & input, const at::Tensor & layersOutputs, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
  auto& z_state_ = unpack(z_state, "z_state", 3);
  auto& cell_state_fwd_ = unpack(cell_state_fwd, "cell_state_fwd", 4);
  auto& input_ = unpack(input, "input", 5);
  auto& layersOutputs_ = unpack(layersOutputs, "layersOutputs", 6);
  auto hx_ = unpack(hx, "hx", 7);
  auto params_ = unpack(params, "params", 8);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_y, grad_hy, grad_cy, z_state, cell_state_fwd, input, layersOutputs, hx, params );
  
  std::shared_ptr<NotImplemented> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("lstm_mps_backward"), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_y, grad_hy, grad_cy, z_state, cell_state_fwd, input, layersOutputs, hx, params ));
  }
  at::Tensor result0;
  ::std::vector<at::Tensor> result1;
  ::std::vector<at::Tensor> result2;
  #ifndef NDEBUG
  c10::optional<Storage> z_state__storage_saved =
    z_state_.has_storage() ? c10::optional<Storage>(z_state_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> z_state__impl_saved;
  if (z_state_.defined()) z_state__impl_saved = z_state_.getIntrusivePtr();
  c10::optional<Storage> cell_state_fwd__storage_saved =
    cell_state_fwd_.has_storage() ? c10::optional<Storage>(cell_state_fwd_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> cell_state_fwd__impl_saved;
  if (cell_state_fwd_.defined()) cell_state_fwd__impl_saved = cell_state_fwd_.getIntrusivePtr();
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> layersOutputs__storage_saved =
    layersOutputs_.has_storage() ? c10::optional<Storage>(layersOutputs_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> layersOutputs__impl_saved;
  if (layersOutputs_.defined()) layersOutputs__impl_saved = layersOutputs_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> hx__storage_saved(hx_.size());
  for (const Tensor& tensor : hx_)
    hx__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> hx__impl_saved(hx_.size());
  for (size_t i=0; i<hx_.size(); i++)
    if (hx_[i].defined()) hx__impl_saved[i] = hx_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> params__storage_saved(params_.size());
  for (const Tensor& tensor : params_)
    params__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> params__impl_saved(params_.size());
  for (size_t i=0; i<params_.size(); i++)
    if (params_[i].defined()) params__impl_saved[i] = params_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(grad_y) || isFwGradDefined(grad_hy) || isFwGradDefined(grad_cy) || isFwGradDefined(z_state) || isFwGradDefined(cell_state_fwd) || isFwGradDefined(input) || isFwGradDefined(layersOutputs) || isFwGradDefinedTensorList(hx) || isFwGradDefinedTensorList(params))) {
      static c10::OperatorName full_name("aten::lstm_mps_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, ::std::vector<at::Tensor>, ::std::vector<at::Tensor>>>("lstm_mps_backward", *opt_op, ks, grad_y, grad_hy, grad_cy, z_state, cell_state_fwd, input, layersOutputs, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::lstm_mps_backward(ks & c10::after_autograd_keyset, grad_y, grad_hy, grad_cy, z_state_, cell_state_fwd_, input_, layersOutputs_, hx_, params_, has_biases, num_layers, dropout, train, bidirectional, batch_first);
    }
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (z_state__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(z_state_))
    TORCH_INTERNAL_ASSERT(z_state__storage_saved.value().is_alias_of(z_state_.storage()));
  if (z_state__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(z_state_))
    TORCH_INTERNAL_ASSERT(z_state__impl_saved == z_state_.getIntrusivePtr());
  if (cell_state_fwd__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(cell_state_fwd_))
    TORCH_INTERNAL_ASSERT(cell_state_fwd__storage_saved.value().is_alias_of(cell_state_fwd_.storage()));
  if (cell_state_fwd__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(cell_state_fwd_))
    TORCH_INTERNAL_ASSERT(cell_state_fwd__impl_saved == cell_state_fwd_.getIntrusivePtr());
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (layersOutputs__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(layersOutputs_))
    TORCH_INTERNAL_ASSERT(layersOutputs__storage_saved.value().is_alias_of(layersOutputs_.storage()));
  if (layersOutputs__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(layersOutputs_))
    TORCH_INTERNAL_ASSERT(layersOutputs__impl_saved == layersOutputs_.getIntrusivePtr());
  for (size_t i=0; i<hx_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (hx__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(hx_))
      TORCH_INTERNAL_ASSERT(hx__storage_saved[i].value().is_alias_of(hx_[i].storage()));
  }
  for (size_t i=0; i<hx_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (hx__impl_saved[i] && !at::impl::tensorlist_has_dispatch(hx_))
      TORCH_INTERNAL_ASSERT(hx__impl_saved[i] == hx_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<params_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (params__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(params_))
      TORCH_INTERNAL_ASSERT(params__storage_saved[i].value().is_alias_of(params_[i].storage()));
  }
  for (size_t i=0; i<params_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (params__impl_saved[i] && !at::impl::tensorlist_has_dispatch(params_))
      TORCH_INTERNAL_ASSERT(params__impl_saved[i] == params_[i].getIntrusivePtr());
  }
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: lstm_mps_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: lstm_mps_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "lstm_mps_backward");
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
at::Tensor & lt__Scalar(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<LtBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LtBackward0>(new LtBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::lt_(ks & c10::after_autograd_keyset, self_, other);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & lt__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<LtBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LtBackward1>(new LtBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_info = other;
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::lt_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & masked_fill__Scalar(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
  auto& self_ = unpack(self, "self", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<MaskedFillBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaskedFillBackward0>(new MaskedFillBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->mask_ = SavedVariable(mask, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::masked_fill_(ks & c10::after_autograd_keyset, self_, mask_, value);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.masked_fill_(mask, 0) : self_t.masked_fill(mask, 0);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & masked_fill__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
  auto& self_ = unpack(self, "self", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  auto& value_ = unpack(value, "value", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, value );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(value));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<MaskedFillBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaskedFillBackward1>(new MaskedFillBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, value ));
    grad_fn->mask_ = SavedVariable(mask, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  c10::optional<Storage> value__storage_saved =
    value_.has_storage() ? c10::optional<Storage>(value_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> value__impl_saved;
  if (value_.defined()) value__impl_saved = value_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::masked_fill_(ks & c10::after_autograd_keyset, self_, mask_, value_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (value__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__storage_saved.value().is_alias_of(value_.storage()));
  if (value__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__impl_saved == value_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto value_t_raw = toNonOptFwGrad(value);
      auto value_tensor = toNonOptTensor(value);
      auto value_t = (value_t_raw.defined() || !value_tensor.defined())
        ? value_t_raw : at::_efficientzerotensor(value_tensor.sizes(), value_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.masked_fill_(mask, value_t) : self_t.masked_fill(mask, value_t);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor max_pool2d(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<MaxPool2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaxPool2DBackward0>(new MaxPool2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->ceil_mode = ceil_mode;
    grad_fn->dilation = dilation.vec();
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::max_pool2d", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("max_pool2d", *opt_op, ks, self, kernel_size, stride, padding, dilation, ceil_mode);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::max_pool2d(ks & c10::after_autograd_keyset, self_, kernel_size, stride, padding, dilation, ceil_mode);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: max_pool2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: max_pool2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "max_pool2d");
  return result;
}
at::Tensor max_unpool2d(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size) {
  auto& self_ = unpack(self, "self", 0);
  auto& indices_ = unpack(indices, "indices", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<MaxUnpool2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaxUnpool2DBackward0>(new MaxUnpool2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->indices_ = SavedVariable(indices, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::max_unpool2d_symint(ks & c10::after_autograd_keyset, self_, indices_, output_size);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: max_unpool2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: max_unpool2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "max_unpool2d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::max_unpool2d_symint(self_t, indices, output_size);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor max_unpool3d(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
  auto& self_ = unpack(self, "self", 0);
  auto& indices_ = unpack(indices, "indices", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<MaxUnpool3DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaxUnpool3DBackward0>(new MaxUnpool3DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->indices_ = SavedVariable(indices, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::max_unpool3d_symint(ks & c10::after_autograd_keyset, self_, indices_, output_size, stride, padding);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: max_unpool3d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: max_unpool3d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "max_unpool3d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::max_unpool3d_symint(self_t, indices, output_size, stride, padding);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor mean(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<MeanBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MeanBackward0>(new MeanBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_numel = self.sym_numel();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::mean(ks & c10::after_autograd_keyset, self_, dtype);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mean");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mean");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::mean(self_t, dtype);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor mean_dim(c10::DispatchKeySet ks, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<MeanBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MeanBackward1>(new MeanBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
    grad_fn->self_sym_numel = self.sym_numel();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::mean(ks & c10::after_autograd_keyset, self_, dim, keepdim, dtype);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mean_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mean_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::mean(self_t, dim, keepdim, dtype);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor> min_dim(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<MinBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MinBackward0>(new MinBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  at::Tensor values;
  at::Tensor indices;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::min(ks & c10::after_autograd_keyset, self_, dim, keepdim);
  })();
  std::tie(values, indices) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values)) {
    TORCH_INTERNAL_ASSERT(values.storage().use_count() == 1, "function: min_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values))
    TORCH_INTERNAL_ASSERT(values.use_count() <= 1, "function: min_dim");
  if (indices.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices)) {
    TORCH_INTERNAL_ASSERT(indices.storage().use_count() == 1, "function: min_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices))
    TORCH_INTERNAL_ASSERT(indices.use_count() <= 1, "function: min_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( values ), grad_fn);
  }
  throw_error_for_complex_autograd(values, "min");
  c10::optional<at::Tensor> values_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_values && (values.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      values_new_fw_grad_opt = gather_with_keepdimed_indices(self_t, dim, indices, keepdim);
  }
  if (values_new_fw_grad_opt.has_value() && values_new_fw_grad_opt.value().defined() && values.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    values._set_fw_grad(values_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->indices_ = SavedVariable(indices, true);
  }
  return std::make_tuple(std::move(values), std::move(indices));
}
at::Tensor min(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<MinBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MinBackward1>(new MinBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::min(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: min");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: min");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "min");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = evenly_read_jvp(self_t, self_p, result);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
::std::tuple<at::Tensor &,at::Tensor &> min_out_dim_min(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
  auto& self_ = unpack(self, "self", 0);
  auto& min_ = unpack(min, "min", 3);
  auto& min_indices_ = unpack(min_indices, "min_indices", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("min");
  }
  if (compute_requires_grad( min )) {
    throw_error_out_requires_grad("min");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> min__storage_saved =
    min_.has_storage() ? c10::optional<Storage>(min_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> min__impl_saved;
  if (min_.defined()) min__impl_saved = min_.getIntrusivePtr();
  c10::optional<Storage> min_indices__storage_saved =
    min_indices_.has_storage() ? c10::optional<Storage>(min_indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> min_indices__impl_saved;
  if (min_indices_.defined()) min_indices__impl_saved = min_indices_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::min_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, min_, min_indices_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (min__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(min_))
    TORCH_INTERNAL_ASSERT(min__storage_saved.value().is_alias_of(min_.storage()));
  if (min__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(min_))
    TORCH_INTERNAL_ASSERT(min__impl_saved == min_.getIntrusivePtr());
  if (min_indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(min_indices_))
    TORCH_INTERNAL_ASSERT(min_indices__storage_saved.value().is_alias_of(min_indices_.storage()));
  if (min_indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(min_indices_))
    TORCH_INTERNAL_ASSERT(min_indices__impl_saved == min_indices_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( min ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(min) || isFwGradDefined(min_indices))), "Trying to use forward AD with min_out that does not support it because it is an out= function");
  return std::forward_as_tuple(min, min_indices);
}
at::Tensor & min_out_unary_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("min");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("min");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::min_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with min_out that does not support it because it is an out= function");
  return out;
}
void miopen_rnn_backward_out_out(c10::DispatchKeySet ks, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::TensorList out3) {
  auto& input_ = unpack(input, "input", 0);
  auto weight_ = unpack(weight, "weight", 1);
  auto& weight_buf_ = unpack(weight_buf, "weight_buf", 3);
  auto& hx_ = unpack(hx, "hx", 4);
  auto& output_ = unpack(output, "output", 6);
  auto& reserve_ = unpack(reserve, "reserve", 19);
  auto& out0_ = unpack(out0, "out0", 21);
  auto& out1_ = unpack(out1, "out1", 22);
  auto& out2_ = unpack(out2, "out2", 23);
  auto out3_ = unpack(out3, "out3", 24);
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> weight__storage_saved(weight_.size());
  for (const Tensor& tensor : weight_)
    weight__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> weight__impl_saved(weight_.size());
  for (size_t i=0; i<weight_.size(); i++)
    if (weight_[i].defined()) weight__impl_saved[i] = weight_[i].getIntrusivePtr();
  c10::optional<Storage> weight_buf__storage_saved =
    weight_buf_.has_storage() ? c10::optional<Storage>(weight_buf_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight_buf__impl_saved;
  if (weight_buf_.defined()) weight_buf__impl_saved = weight_buf_.getIntrusivePtr();
  c10::optional<Storage> hx__storage_saved =
    hx_.has_storage() ? c10::optional<Storage>(hx_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> hx__impl_saved;
  if (hx_.defined()) hx__impl_saved = hx_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> reserve__storage_saved =
    reserve_.has_storage() ? c10::optional<Storage>(reserve_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> reserve__impl_saved;
  if (reserve_.defined()) reserve__impl_saved = reserve_.getIntrusivePtr();
  c10::optional<Storage> out0__storage_saved =
    out0_.has_storage() ? c10::optional<Storage>(out0_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out0__impl_saved;
  if (out0_.defined()) out0__impl_saved = out0_.getIntrusivePtr();
  c10::optional<Storage> out1__storage_saved =
    out1_.has_storage() ? c10::optional<Storage>(out1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out1__impl_saved;
  if (out1_.defined()) out1__impl_saved = out1_.getIntrusivePtr();
  c10::optional<Storage> out2__storage_saved =
    out2_.has_storage() ? c10::optional<Storage>(out2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out2__impl_saved;
  if (out2_.defined()) out2__impl_saved = out2_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out3__storage_saved(out3_.size());
  for (const Tensor& tensor : out3_)
    out3__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out3__impl_saved(out3_.size());
  for (size_t i=0; i<out3_.size(); i++)
    if (out3_[i].defined()) out3__impl_saved[i] = out3_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::miopen_rnn_backward_outf(ks & c10::after_autograd_keyset, input_, weight_, weight_stride0, weight_buf_, hx_, cx, output_, grad_output, grad_hy, grad_cy, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve_, output_mask, out0_, out1_, out2_, out3_);
  }
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  for (size_t i=0; i<weight_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weight__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(weight_))
      TORCH_INTERNAL_ASSERT(weight__storage_saved[i].value().is_alias_of(weight_[i].storage()));
  }
  for (size_t i=0; i<weight_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weight__impl_saved[i] && !at::impl::tensorlist_has_dispatch(weight_))
      TORCH_INTERNAL_ASSERT(weight__impl_saved[i] == weight_[i].getIntrusivePtr());
  }
  if (weight_buf__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_buf_))
    TORCH_INTERNAL_ASSERT(weight_buf__storage_saved.value().is_alias_of(weight_buf_.storage()));
  if (weight_buf__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_buf_))
    TORCH_INTERNAL_ASSERT(weight_buf__impl_saved == weight_buf_.getIntrusivePtr());
  if (hx__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__storage_saved.value().is_alias_of(hx_.storage()));
  if (hx__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__impl_saved == hx_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (reserve__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(reserve_))
    TORCH_INTERNAL_ASSERT(reserve__storage_saved.value().is_alias_of(reserve_.storage()));
  if (reserve__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(reserve_))
    TORCH_INTERNAL_ASSERT(reserve__impl_saved == reserve_.getIntrusivePtr());
  if (out0__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out0_))
    TORCH_INTERNAL_ASSERT(out0__storage_saved.value().is_alias_of(out0_.storage()));
  if (out0__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out0_))
    TORCH_INTERNAL_ASSERT(out0__impl_saved == out0_.getIntrusivePtr());
  if (out1__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out1_))
    TORCH_INTERNAL_ASSERT(out1__storage_saved.value().is_alias_of(out1_.storage()));
  if (out1__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out1_))
    TORCH_INTERNAL_ASSERT(out1__impl_saved == out1_.getIntrusivePtr());
  if (out2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out2_))
    TORCH_INTERNAL_ASSERT(out2__storage_saved.value().is_alias_of(out2_.storage()));
  if (out2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out2_))
    TORCH_INTERNAL_ASSERT(out2__impl_saved == out2_.getIntrusivePtr());
  for (size_t i=0; i<out3_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out3__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out3_))
      TORCH_INTERNAL_ASSERT(out3__storage_saved[i].value().is_alias_of(out3_[i].storage()));
  }
  for (size_t i=0; i<out3_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out3__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out3_))
      TORCH_INTERNAL_ASSERT(out3__impl_saved[i] == out3_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(input) || isFwGradDefinedTensorList(weight) || isFwGradDefined(weight_buf) || isFwGradDefined(hx) || isFwGradDefined(cx) || isFwGradDefined(output) || isFwGradDefined(grad_output) || isFwGradDefined(grad_hy) || isFwGradDefined(grad_cy) || isFwGradDefined(dropout_state) || isFwGradDefined(reserve) || isFwGradDefined(out0) || isFwGradDefined(out1) || isFwGradDefined(out2) || isFwGradDefinedTensorList(out3))), "Trying to use forward AD with miopen_rnn_backward_out that does not support it because it is an out= function");
}
at::Tensor & mish_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<MishBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MishBackward0>(new MishBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::mish_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((GradMode::is_enabled() ? infinitely_differentiable_mish_backward(original_self_t.conj(), original_self_p) : mish_backward(original_self_t.conj(), original_self_p)).conj()) : (GradMode::is_enabled() ? infinitely_differentiable_mish_backward(original_self_t.conj(), original_self_p) : mish_backward(original_self_t.conj(), original_self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor mm(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mat2) {
  auto& self_ = unpack(self, "self", 0);
  auto& mat2_ = unpack(mat2, "mat2", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, mat2 );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(mat2));
  std::shared_ptr<MmBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MmBackward0>(new MmBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, mat2 ));
    if (grad_fn->should_compute_output(0)) {
      grad_fn->mat2_ = SavedVariable(mat2, false);
    }
    grad_fn->mat2_layout = mat2.layout();
    grad_fn->mat2_sym_sizes = mat2.sym_sizes().vec();
    grad_fn->mat2_sym_strides = strides_or_error(mat2, "mat2").vec();
    if (grad_fn->should_compute_output(1)) {
      grad_fn->self_ = SavedVariable(self, false);
    }
    grad_fn->self_layout = self.layout();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
    grad_fn->self_sym_strides = strides_or_error(self, "self").vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mat2__storage_saved =
    mat2_.has_storage() ? c10::optional<Storage>(mat2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mat2__impl_saved;
  if (mat2_.defined()) mat2__impl_saved = mat2_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::mm(ks & c10::after_autograd_keyset, self_, mat2_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mat2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mat2_))
    TORCH_INTERNAL_ASSERT(mat2__storage_saved.value().is_alias_of(mat2_.storage()));
  if (mat2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mat2_))
    TORCH_INTERNAL_ASSERT(mat2__impl_saved == mat2_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto mat2_t_raw = toNonOptFwGrad(mat2);
      auto mat2_tensor = toNonOptTensor(mat2);
      auto mat2_t = (mat2_t_raw.defined() || !mat2_tensor.defined())
        ? mat2_t_raw : at::_efficientzerotensor(mat2_tensor.sizes(), mat2_tensor.options());
      auto mat2_p = toNonOptPrimal(mat2);
      result_new_fw_grad_opt = at::mm(self_t, mat2_p) + at::mm(self_p, mat2_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor mv(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & vec) {
  auto& self_ = unpack(self, "self", 0);
  auto& vec_ = unpack(vec, "vec", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, vec );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(vec));
  std::shared_ptr<MvBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MvBackward0>(new MvBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, vec ));
    if (grad_fn->should_compute_output(1)) {
      grad_fn->self_ = SavedVariable(self, false);
    }
    if (grad_fn->should_compute_output(0)) {
      grad_fn->vec_ = SavedVariable(vec, false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> vec__storage_saved =
    vec_.has_storage() ? c10::optional<Storage>(vec_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> vec__impl_saved;
  if (vec_.defined()) vec__impl_saved = vec_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::mv(ks & c10::after_autograd_keyset, self_, vec_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (vec__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(vec_))
    TORCH_INTERNAL_ASSERT(vec__storage_saved.value().is_alias_of(vec_.storage()));
  if (vec__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(vec_))
    TORCH_INTERNAL_ASSERT(vec__impl_saved == vec_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mv");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mv");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto vec_t_raw = toNonOptFwGrad(vec);
      auto vec_tensor = toNonOptTensor(vec);
      auto vec_t = (vec_t_raw.defined() || !vec_tensor.defined())
        ? vec_t_raw : at::_efficientzerotensor(vec_tensor.sizes(), vec_tensor.options());
      auto vec_p = toNonOptPrimal(vec);
      result_new_fw_grad_opt = mv(self_t, vec_p) + mv(self_p, vec_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & mvlgamma_(c10::DispatchKeySet ks, at::Tensor & self, int64_t p) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<MvlgammaBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MvlgammaBackward0>(new MvlgammaBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->p = p;
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::mvlgamma_(ks & c10::after_autograd_keyset, self_, p);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((mvlgamma_backward(original_self_t.conj(), original_self_p, p)).conj()) : (mvlgamma_backward(original_self_t.conj(), original_self_p, p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor native_dropout_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & mask, double scale) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, mask );
  
  std::shared_ptr<NativeDropoutBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NativeDropoutBackwardBackward0>(new NativeDropoutBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, mask ));
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->mask_ = SavedVariable(mask, false);
    grad_fn->scale = scale;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(grad_output) || isFwGradDefined(mask))) {
      static c10::OperatorName full_name("aten::native_dropout_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("native_dropout_backward", *opt_op, ks, grad_output, mask, scale);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::native_dropout_backward(ks & c10::after_autograd_keyset, grad_output_, mask_, scale);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: native_dropout_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: native_dropout_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "native_dropout_backward");
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_group_norm(c10::DispatchKeySet ks, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, double eps) {
  auto& input_ = unpack(input, "input", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias));
  [[maybe_unused]] auto _any_has_forward_grad_result1 = (isFwGradDefined(input));
  [[maybe_unused]] auto _any_has_forward_grad_result2 = (isFwGradDefined(input));
  std::shared_ptr<NativeGroupNormBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NativeGroupNormBackward0>(new NativeGroupNormBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->C = C;
    grad_fn->HxW = HxW;
    grad_fn->N = N;
    grad_fn->eps = eps;
    grad_fn->group = group;
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::native_group_norm_symint(ks & c10::after_autograd_keyset, input_, weight, bias, N, C, HxW, group, eps);
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: native_group_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: native_group_norm");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: native_group_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: native_group_norm");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: native_group_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: native_group_norm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "native_group_norm");
  throw_error_for_complex_autograd(result1, "native_group_norm");
  throw_error_for_complex_autograd(result2, "native_group_norm");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto input_t_raw = toNonOptFwGrad(input);
      auto input_tensor = toNonOptTensor(input);
      auto input_t = (input_t_raw.defined() || !input_tensor.defined())
        ? input_t_raw : at::_efficientzerotensor(input_tensor.sizes(), input_tensor.options());
      auto input_p = toNonOptPrimal(input);
      auto weight_t_raw = toNonOptFwGrad(weight);
      auto weight_tensor = toNonOptTensor(weight);
      auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
        ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
      auto weight_p = toNonOptPrimal(weight);
      auto bias_t_raw = toNonOptFwGrad(bias);
      auto bias_tensor = toNonOptTensor(bias);
      auto bias_t = (bias_t_raw.defined() || !bias_tensor.defined())
        ? bias_t_raw : at::_efficientzerotensor(bias_tensor.sizes(), bias_tensor.options());
      auto bias_p = toNonOptPrimal(bias);
      result0_new_fw_grad_opt = group_norm_jvp(input_p, input_t, weight_p, weight_t, bias_p, bias_t, result1, result2, group);
  }
  c10::optional<at::Tensor> result1_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result1 && (result1.defined())) {
      auto input_t_raw = toNonOptFwGrad(input);
      auto input_tensor = toNonOptTensor(input);
      auto input_t = (input_t_raw.defined() || !input_tensor.defined())
        ? input_t_raw : at::_efficientzerotensor(input_tensor.sizes(), input_tensor.options());
      result1_new_fw_grad_opt = group_norm_mean_jvp(input_t, result1, group);
  }
  c10::optional<at::Tensor> result2_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result2 && (result2.defined())) {
      auto input_t_raw = toNonOptFwGrad(input);
      auto input_tensor = toNonOptTensor(input);
      auto input_t = (input_t_raw.defined() || !input_tensor.defined())
        ? input_t_raw : at::_efficientzerotensor(input_tensor.sizes(), input_tensor.options());
      auto input_p = toNonOptPrimal(input);
      result2_new_fw_grad_opt = group_norm_invstd_jvp(input_p, input_t, result1, result2, group);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (result1_new_fw_grad_opt.has_value() && result1_new_fw_grad_opt.value().defined() && result1.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result1._set_fw_grad(result1_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (result2_new_fw_grad_opt.has_value() && result2_new_fw_grad_opt.value().defined() && result2.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result2._set_fw_grad(result2_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
at::Tensor & new_empty_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::new_empty_symint_outf(ks & c10::after_autograd_keyset, self_, size, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with new_empty_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & new_full_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, const at::Scalar & fill_value, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::new_full_symint_outf(ks & c10::after_autograd_keyset, self_, size, fill_value, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with new_full_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & new_ones_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::new_ones_symint_outf(ks & c10::after_autograd_keyset, self_, size, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with new_ones_out that does not support it because it is an out= function");
  return out;
}
at::Tensor nll_loss_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& target_ = unpack(target, "target", 2);
  auto& total_weight_ = unpack(total_weight, "total_weight", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  check_no_requires_grad(weight, "weight", "nll_loss_backward");
  check_no_requires_grad(total_weight, "total_weight", "nll_loss_backward");
  std::shared_ptr<NllLossBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NllLossBackwardBackward0>(new NllLossBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->ignore_index = ignore_index;
    grad_fn->reduction = reduction;
    grad_fn->target_ = SavedVariable(target, false);
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> total_weight__storage_saved =
    total_weight_.has_storage() ? c10::optional<Storage>(total_weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> total_weight__impl_saved;
  if (total_weight_.defined()) total_weight__impl_saved = total_weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(total_weight))) {
      static c10::OperatorName full_name("aten::nll_loss_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("nll_loss_backward", *opt_op, ks, grad_output, self, target, weight, reduction, ignore_index, total_weight);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::nll_loss_backward_symint(ks & c10::after_autograd_keyset, grad_output_, self_, target_, weight, reduction, ignore_index, total_weight_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (total_weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(total_weight_))
    TORCH_INTERNAL_ASSERT(total_weight__storage_saved.value().is_alias_of(total_weight_.storage()));
  if (total_weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(total_weight_))
    TORCH_INTERNAL_ASSERT(total_weight__impl_saved == total_weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: nll_loss_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: nll_loss_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "nll_loss_backward");
  return result;
}
at::Tensor & nonzero_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::nonzero_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with nonzero_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & norm_out_dtype_out(c10::DispatchKeySet ks, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("norm");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("norm");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::norm_outf(ks & c10::after_autograd_keyset, self_, p, dim, keepdim, dtype, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with norm_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & norm_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("norm");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("norm");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::norm_outf(ks & c10::after_autograd_keyset, self_, p, dim, keepdim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with norm_out that does not support it because it is an out= function");
  return out;
}
at::Tensor ormqr(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose) {
  auto& self_ = unpack(self, "self", 0);
  auto& input2_ = unpack(input2, "input2", 1);
  auto& input3_ = unpack(input3, "input3", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, input2, input3 );
  
  std::shared_ptr<OrmqrBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<OrmqrBackward0>(new OrmqrBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, input2, input3 ));
    grad_fn->input2_ = SavedVariable(input2, false);
    grad_fn->input3_ = SavedVariable(input3, false);
    grad_fn->left = left;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->transpose = transpose;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> input2__storage_saved =
    input2_.has_storage() ? c10::optional<Storage>(input2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input2__impl_saved;
  if (input2_.defined()) input2__impl_saved = input2_.getIntrusivePtr();
  c10::optional<Storage> input3__storage_saved =
    input3_.has_storage() ? c10::optional<Storage>(input3_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input3__impl_saved;
  if (input3_.defined()) input3__impl_saved = input3_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(input2) || isFwGradDefined(input3))) {
      static c10::OperatorName full_name("aten::ormqr", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("ormqr", *opt_op, ks, self, input2, input3, left, transpose);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::ormqr(ks & c10::after_autograd_keyset, self_, input2_, input3_, left, transpose);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (input2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input2_))
    TORCH_INTERNAL_ASSERT(input2__storage_saved.value().is_alias_of(input2_.storage()));
  if (input2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input2_))
    TORCH_INTERNAL_ASSERT(input2__impl_saved == input2_.getIntrusivePtr());
  if (input3__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input3_))
    TORCH_INTERNAL_ASSERT(input3__storage_saved.value().is_alias_of(input3_.storage()));
  if (input3__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input3_))
    TORCH_INTERNAL_ASSERT(input3__impl_saved == input3_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: ormqr");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: ormqr");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & ormqr_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& input2_ = unpack(input2, "input2", 1);
  auto& input3_ = unpack(input3, "input3", 2);
  auto& out_ = unpack(out, "out", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, input2, input3 );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, input2, input3 )) {
    throw_error_out_requires_grad("ormqr");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("ormqr");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> input2__storage_saved =
    input2_.has_storage() ? c10::optional<Storage>(input2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input2__impl_saved;
  if (input2_.defined()) input2__impl_saved = input2_.getIntrusivePtr();
  c10::optional<Storage> input3__storage_saved =
    input3_.has_storage() ? c10::optional<Storage>(input3_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input3__impl_saved;
  if (input3_.defined()) input3__impl_saved = input3_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::ormqr_outf(ks & c10::after_autograd_keyset, self_, input2_, input3_, left, transpose, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (input2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input2_))
    TORCH_INTERNAL_ASSERT(input2__storage_saved.value().is_alias_of(input2_.storage()));
  if (input2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input2_))
    TORCH_INTERNAL_ASSERT(input2__impl_saved == input2_.getIntrusivePtr());
  if (input3__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input3_))
    TORCH_INTERNAL_ASSERT(input3__storage_saved.value().is_alias_of(input3_.storage()));
  if (input3__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input3_))
    TORCH_INTERNAL_ASSERT(input3__impl_saved == input3_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(input2) || isFwGradDefined(input3) || isFwGradDefined(out))), "Trying to use forward AD with ormqr_out that does not support it because it is an out= function");
  return out;
}
at::Tensor permute(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dims) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<PermuteBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PermuteBackward0>(new PermuteBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dims = dims.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::permute(ks & c10::after_autograd_keyset, self_, dims);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: permute");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::permute(self_t, dims);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor pixel_shuffle(c10::DispatchKeySet ks, const at::Tensor & self, int64_t upscale_factor) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<PixelShuffleBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PixelShuffleBackward0>(new PixelShuffleBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->upscale_factor = upscale_factor;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::pixel_shuffle(ks & c10::after_autograd_keyset, self_, upscale_factor);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: pixel_shuffle");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: pixel_shuffle");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::pixel_shuffle(self_t, upscale_factor);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor q_per_channel_scales(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<NotImplemented> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("q_per_channel_scales"), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::q_per_channel_scales", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("q_per_channel_scales", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::q_per_channel_scales(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "q_per_channel_scales");
  return result;
}
int64_t q_zero_point(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::q_zero_point(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor & randn_like_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::randn_like_outf(ks & c10::after_autograd_keyset, self_, memory_format, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with randn_like_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & reciprocal_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<ReciprocalBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReciprocalBackward0>(new ReciprocalBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::reciprocal_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((-self_t.conj() * (self_p * self_p).conj()).conj()) : (-self_t.conj() * (self_p * self_p).conj()).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor & reciprocal_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("reciprocal");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("reciprocal");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::reciprocal_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with reciprocal_out that does not support it because it is an out= function");
  return out;
}
void record_stream(c10::DispatchKeySet ks, at::Tensor & self, at::Stream s) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowADInplaceOrView guard;
    at::redispatch::record_stream(ks & c10::after_autograd_keyset, self_, s);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
}
at::Tensor & reflection_pad1d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("reflection_pad1d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("reflection_pad1d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::reflection_pad1d_symint_outf(ks & c10::after_autograd_keyset, self_, padding, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with reflection_pad1d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor reflection_pad3d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<ReflectionPad3DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReflectionPad3DBackwardBackward0>(new ReflectionPad3DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->padding = padding.vec();
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::reflection_pad3d_backward_symint(ks & c10::after_autograd_keyset, grad_output_, self_, padding);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: reflection_pad3d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: reflection_pad3d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = reflection_pad3d_backward_symint(grad_output_t, self_p, padding);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & remainder__Scalar(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<RemainderBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RemainderBackward0>(new RemainderBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::remainder_(ks & c10::after_autograd_keyset, self_, other);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((self_t.conj()).conj()) : (self_t.conj()).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & remainder__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(other));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<RemainderBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RemainderBackward1>(new RemainderBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    if (grad_fn->should_compute_output(1)) {
      grad_fn->other_ = SavedVariable(other, false);
    }
    if (grad_fn->should_compute_output(1)) {
      if (!original_self.has_value()) original_self = self.clone();
      grad_fn->self_ = SavedVariable(original_self.value(), false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::remainder_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(original_self_t - other_t * original_self_p.div(other_p, /*rounding_mode=*/"floor")) : original_self_t - other_t * original_self_p.div(other_p, /*rounding_mode=*/"floor");
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor replication_pad2d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<ReplicationPad2DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReplicationPad2DBackwardBackward0>(new ReplicationPad2DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->padding = padding.vec();
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::replication_pad2d_backward_symint(ks & c10::after_autograd_keyset, grad_output_, self_, padding);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: replication_pad2d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: replication_pad2d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = replication_pad2d_backward_symint(grad_output_t, self_p, padding);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor replication_pad3d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef padding) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ReplicationPad3DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReplicationPad3DBackward0>(new ReplicationPad3DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::replication_pad3d_symint(ks & c10::after_autograd_keyset, self_, padding);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: replication_pad3d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: replication_pad3d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::replication_pad3d_symint(self_t, padding);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor roll(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef shifts, at::IntArrayRef dims) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<RollBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RollBackward0>(new RollBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dims = dims.vec();
    grad_fn->shifts = shifts.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::roll_symint(ks & c10::after_autograd_keyset, self_, shifts, dims);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: roll");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: roll");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::roll_symint(self_t, shifts, dims);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor rsub_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<RsubBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RsubBackward0>(new RsubBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->alpha = alpha;
    grad_fn->other_scalar_type = other.scalar_type();
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::rsub(ks & c10::after_autograd_keyset, self_, other_, alpha);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: rsub_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: rsub_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      result_new_fw_grad_opt = -maybe_multiply(self_t, alpha) + other_t;
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor rsub_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<RsubBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RsubBackward1>(new RsubBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->alpha = alpha;
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::rsub(ks & c10::after_autograd_keyset, self_, other, alpha);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: rsub_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: rsub_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (handle_r_to_c(self_p.scalar_type(), maybe_multiply(-self_t.conj(), alpha.conj()))).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & scatter_add_(c10::DispatchKeySet ks, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  auto& src_ = unpack(src, "src", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, src );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(src));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<ScatterAddBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ScatterAddBackward0>(new ScatterAddBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, src ));
    grad_fn->dim = dim;
    if (grad_fn->should_compute_output(1)) {
      grad_fn->index_ = SavedVariable(index, false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> src__storage_saved =
    src_.has_storage() ? c10::optional<Storage>(src_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> src__impl_saved;
  if (src_.defined()) src__impl_saved = src_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::scatter_add_(ks & c10::after_autograd_keyset, self_, dim, index_, src_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (src__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__storage_saved.value().is_alias_of(src_.storage()));
  if (src__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__impl_saved == src_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto src_t_raw = toNonOptFwGrad(src);
      auto src_tensor = toNonOptTensor(src);
      auto src_t = (src_t_raw.defined() || !src_tensor.defined())
        ? src_t_raw : at::_efficientzerotensor(src_tensor.sizes(), src_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(scatter_add(self_t, dim, index, src_t)) : scatter_add(self_t, dim, index, src_t);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & scatter_out_src_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  auto& src_ = unpack(src, "src", 3);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, src );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(src));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, src )) {
    throw_error_out_requires_grad("scatter");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("scatter");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> src__storage_saved =
    src_.has_storage() ? c10::optional<Storage>(src_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> src__impl_saved;
  if (src_.defined()) src__impl_saved = src_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::scatter_outf(ks & c10::after_autograd_keyset, self_, dim, index_, src_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (src__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__storage_saved.value().is_alias_of(src_.storage()));
  if (src__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__impl_saved == src_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(src) || isFwGradDefined(out))), "Trying to use forward AD with scatter_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & scatter_out_value_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("scatter");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("scatter");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::scatter_outf(ks & c10::after_autograd_keyset, self_, dim, index_, value, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with scatter_out that does not support it because it is an out= function");
  return out;
}
at::Tensor select_int(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, c10::SymInt index) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SelectBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SelectBackward0>(new SelectBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->index = index;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::select_symint(ks & c10::after_autograd_keyset, self_, dim, index);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: select_int");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::select_symint(self_t, dim, index);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor sign(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SignBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SignBackward0>(new SignBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::sign(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sign");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sign");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "sign");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (zeros_like(self_t.conj())).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor silu(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SiluBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SiluBackward0>(new SiluBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::silu(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: silu");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: silu");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "silu");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (GradMode::is_enabled() ? infinitely_differentiable_silu_backward(self_t.conj(), self_p) : silu_backward(self_t.conj(), self_p)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & silu_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("silu");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("silu");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::silu_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with silu_out that does not support it because it is an out= function");
  return out;
}
at::Tensor sinh(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SinhBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SinhBackward0>(new SinhBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::sinh(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sinh");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sinh");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() * self_p.cosh().conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & sinh_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<SinhBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SinhBackward0>(new SinhBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::sinh_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * original_self_p.cosh().conj()).conj()) : (original_self_t.conj() * original_self_p.cosh().conj()).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor slice_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt start, c10::SymInt end, c10::SymInt step) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<SliceBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SliceBackwardBackward0>(new SliceBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output ));
    grad_fn->dim = dim;
    grad_fn->end = end;
    grad_fn->start = start;
    grad_fn->step = step;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::slice_backward_symint(ks & c10::after_autograd_keyset, grad_output_, input_sizes, dim, start, end, step);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: slice_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: slice_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result_new_fw_grad_opt = at::slice_backward_symint(grad_output_t, input_sizes, dim, start, end, step);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & slow_conv3d_forward_out_output(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & output) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  auto& output_ = unpack(output, "output", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, weight, bias )) {
    throw_error_out_requires_grad("slow_conv3d_forward");
  }
  if (compute_requires_grad( output )) {
    throw_error_out_requires_grad("slow_conv3d_forward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::slow_conv3d_forward_symint_outf(ks & c10::after_autograd_keyset, self_, weight_, kernel_size, bias, stride, padding, output_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( output ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias) || isFwGradDefined(output))), "Trying to use forward AD with slow_conv3d_forward_out that does not support it because it is an out= function");
  return output;
}
at::Tensor slow_conv_dilated2d(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<SlowConvDilated2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SlowConvDilated2DBackward0>(new SlowConvDilated2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight, bias ));
    grad_fn->bias_sym_sizes_opt = bias.has_value() ? c10::optional<c10::SymIntArrayRef>(bias->sym_sizes()) : c10::nullopt;
    grad_fn->dilation = dilation.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::slow_conv_dilated2d", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("slow_conv_dilated2d", *opt_op, ks, self, weight, kernel_size, bias, stride, padding, dilation);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::slow_conv_dilated2d_symint(ks & c10::after_autograd_keyset, self_, weight_, kernel_size, bias, stride, padding, dilation);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: slow_conv_dilated2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: slow_conv_dilated2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "slow_conv_dilated2d");
  return result;
}
at::Tensor slow_conv_transpose2d(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<SlowConvTranspose2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SlowConvTranspose2DBackward0>(new SlowConvTranspose2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight, bias ));
    grad_fn->bias_sym_sizes_opt = bias.has_value() ? c10::optional<c10::SymIntArrayRef>(bias->sym_sizes()) : c10::nullopt;
    grad_fn->dilation = dilation.vec();
    grad_fn->output_padding = output_padding.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::slow_conv_transpose2d", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("slow_conv_transpose2d", *opt_op, ks, self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::slow_conv_transpose2d_symint(ks & c10::after_autograd_keyset, self_, weight_, kernel_size, bias, stride, padding, output_padding, dilation);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: slow_conv_transpose2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: slow_conv_transpose2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "slow_conv_transpose2d");
  return result;
}
at::Tensor smooth_l1_loss_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& target_ = unpack(target, "target", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self, target );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target));
  std::shared_ptr<SmoothL1LossBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SmoothL1LossBackwardBackward0>(new SmoothL1LossBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self, target ));
    grad_fn->beta = beta;
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->reduction = reduction;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->target_ = SavedVariable(target, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::smooth_l1_loss_backward(ks & c10::after_autograd_keyset, grad_output_, self_, target_, reduction, beta);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: smooth_l1_loss_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: smooth_l1_loss_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "smooth_l1_loss_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto target_t_raw = toNonOptFwGrad(target);
      auto target_tensor = toNonOptTensor(target);
      auto target_t = (target_t_raw.defined() || !target_tensor.defined())
        ? target_t_raw : at::_efficientzerotensor(target_tensor.sizes(), target_tensor.options());
      auto target_p = toNonOptPrimal(target);
      result_new_fw_grad_opt =   smooth_l1_loss_double_backward(self_t * grad_output_p, self_p, target_p, reduction, beta) - smooth_l1_loss_double_backward(target_t * grad_output_p, self_p, target_p, reduction, beta) + smooth_l1_loss_backward(grad_output_t, self_p, target_p, reduction, beta) ;
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & smooth_l1_loss_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& target_ = unpack(target, "target", 2);
  auto& grad_input_ = unpack(grad_input, "grad_input", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self, target );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self, target )) {
    throw_error_out_requires_grad("smooth_l1_loss_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("smooth_l1_loss_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::smooth_l1_loss_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, target_, reduction, beta, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target) || isFwGradDefined(grad_input))), "Trying to use forward AD with smooth_l1_loss_backward_out that does not support it because it is an out= function");
  return grad_input;
}
::std::tuple<at::Tensor &,at::Tensor &> sort_out_values(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  auto& self_ = unpack(self, "self", 0);
  auto& values_ = unpack(values, "values", 3);
  auto& indices_ = unpack(indices, "indices", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("sort");
  }
  if (compute_requires_grad( values )) {
    throw_error_out_requires_grad("sort");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> values__storage_saved =
    values_.has_storage() ? c10::optional<Storage>(values_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> values__impl_saved;
  if (values_.defined()) values__impl_saved = values_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::sort_outf(ks & c10::after_autograd_keyset, self_, dim, descending, values_, indices_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__storage_saved.value().is_alias_of(values_.storage()));
  if (values__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__impl_saved == values_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( values ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(values) || isFwGradDefined(indices))), "Trying to use forward AD with sort_out that does not support it because it is an out= function");
  return std::forward_as_tuple(values, indices);
}
::std::tuple<at::Tensor &,at::Tensor &> sort_out_values_stable(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  auto& self_ = unpack(self, "self", 0);
  auto& values_ = unpack(values, "values", 4);
  auto& indices_ = unpack(indices, "indices", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("sort");
  }
  if (compute_requires_grad( values )) {
    throw_error_out_requires_grad("sort");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> values__storage_saved =
    values_.has_storage() ? c10::optional<Storage>(values_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> values__impl_saved;
  if (values_.defined()) values__impl_saved = values_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::sort_outf(ks & c10::after_autograd_keyset, self_, stable, dim, descending, values_, indices_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__storage_saved.value().is_alias_of(values_.storage()));
  if (values__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__impl_saved == values_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( values ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(values) || isFwGradDefined(indices))), "Trying to use forward AD with sort_out that does not support it because it is an out= function");
  return std::forward_as_tuple(values, indices);
}
at::Tensor special_bessel_j1(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_bessel_j1(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_bessel_j1");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_bessel_j1");
  #endif
  return result;
}
at::Tensor & special_bessel_j1_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_bessel_j1_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_bessel_j1_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_bessel_y0_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_bessel_y0_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_bessel_y0_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_bessel_y1_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_bessel_y1_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_bessel_y1_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_chebyshev_polynomial_t_out_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_chebyshev_polynomial_t_outf(ks & c10::after_autograd_keyset, x_, n_, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_chebyshev_polynomial_t_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_chebyshev_polynomial_t_out_x_scalar_out(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_chebyshev_polynomial_t_outf(ks & c10::after_autograd_keyset, x, n_, out_);
  }
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_chebyshev_polynomial_t_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_chebyshev_polynomial_t_out_n_scalar_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_chebyshev_polynomial_t_outf(ks & c10::after_autograd_keyset, x_, n, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_chebyshev_polynomial_t_out that does not support it because it is an out= function");
  return out;
}
at::Tensor special_chebyshev_polynomial_v(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_chebyshev_polynomial_v(ks & c10::after_autograd_keyset, x_, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_chebyshev_polynomial_v");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_chebyshev_polynomial_v");
  #endif
  return result;
}
at::Tensor special_chebyshev_polynomial_v_x_scalar(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n) {
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_chebyshev_polynomial_v(ks & c10::after_autograd_keyset, x, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_chebyshev_polynomial_v_x_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_chebyshev_polynomial_v_x_scalar");
  #endif
  return result;
}
at::Tensor special_chebyshev_polynomial_v_n_scalar(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n) {
  auto& x_ = unpack(x, "x", 0);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_chebyshev_polynomial_v(ks & c10::after_autograd_keyset, x_, n);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_chebyshev_polynomial_v_n_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_chebyshev_polynomial_v_n_scalar");
  #endif
  return result;
}
at::Tensor & special_i1e_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("special_i1e");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("special_i1e");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_i1e_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with special_i1e_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_ndtri_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("special_ndtri");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("special_ndtri");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_ndtri_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with special_ndtri_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_w_out_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_w_outf(ks & c10::after_autograd_keyset, x_, n_, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_w_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_w_out_x_scalar_out(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_w_outf(ks & c10::after_autograd_keyset, x, n_, out_);
  }
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_w_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_w_out_n_scalar_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_w_outf(ks & c10::after_autograd_keyset, x_, n, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_w_out that does not support it because it is an out= function");
  return out;
}
at::Tensor special_xlog1py(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<SpecialXlog1PyBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SpecialXlog1PyBackward0>(new SpecialXlog1PyBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_xlog1py(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_xlog1py");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_xlog1py");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "special_xlog1py");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      result_new_fw_grad_opt = at::special_xlog1py(self_t,  other_p).masked_fill((self_p == 0.) & (other_p <= -1.), 0.) + other_t * self_p / (other_p + 1);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor special_xlog1py_self_scalar(c10::DispatchKeySet ks, const at::Scalar & self, const at::Tensor & other) {
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(other));
  std::shared_ptr<SpecialXlog1PyBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SpecialXlog1PyBackward1>(new SpecialXlog1PyBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( other ));
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->self = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_xlog1py(ks & c10::after_autograd_keyset, self, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_xlog1py_self_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_xlog1py_self_scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "special_xlog1py");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      result_new_fw_grad_opt = (other_t.conj() * self / (other_p + 1)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor special_xlog1py_other_scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SpecialXlog1PyBackward2> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SpecialXlog1PyBackward2>(new SpecialXlog1PyBackward2(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->other = other;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_xlog1py(ks & c10::after_autograd_keyset, self_, other);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_xlog1py_other_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_xlog1py_other_scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "special_xlog1py");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (other.toDouble() > -1. ? at::special_xlog1py(self_t.conj(),  other) : at::special_xlog1py(self_t.conj(),  other).masked_fill(self_p == 0., 0.)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & special_xlog1py_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("special_xlog1py");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("special_xlog1py");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_xlog1py_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with special_xlog1py_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_xlog1py_out_self_scalar_out(c10::DispatchKeySet ks, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( other )) {
    throw_error_out_requires_grad("special_xlog1py");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("special_xlog1py");
  }
  #ifndef NDEBUG
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_xlog1py_outf(ks & c10::after_autograd_keyset, self, other_, out_);
  }
  #ifndef NDEBUG
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with special_xlog1py_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_xlog1py_out_other_scalar_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("special_xlog1py");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("special_xlog1py");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_xlog1py_outf(ks & c10::after_autograd_keyset, self_, other, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with special_xlog1py_out that does not support it because it is an out= function");
  return out;
}
::std::vector<at::Tensor> split_copy_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymInt split_size, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SplitBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SplitBackward0_copy>(new SplitBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_options = self.options();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
    grad_fn->split_size = split_size;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::split_copy_symint(ks & c10::after_autograd_keyset, self_, split_size, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<::std::vector<at::Tensor>> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::split_symint(self_t, split_size, dim);
  }
  if (result_new_fw_grad_opt.has_value()) {
    auto result_new_fw_grad = result_new_fw_grad_opt.value();
    TORCH_INTERNAL_ASSERT(result.size() == result_new_fw_grad.size());
    for (const auto i : c10::irange(result.size())) {
      if (result_new_fw_grad[i].defined() && result[i].defined()) {
        // The hardcoded 0 here will need to be updated once we support multiple levels.
        result[i]._set_fw_grad(result_new_fw_grad[i], /* level */ 0, /* is_inplace_op */ false);
      }
    }
  }
  return result;
}
void split_copy_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymInt split_size, int64_t dim, at::TensorList out) {
  auto& self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::split_copy_symint_outf(ks & c10::after_autograd_keyset, self_, split_size, dim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with split_copy_out that does not support it because it is an out= function");
}
void split_with_sizes_copy_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim, at::TensorList out) {
  auto& self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::split_with_sizes_copy_symint_outf(ks & c10::after_autograd_keyset, self_, split_sizes, dim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with split_with_sizes_copy_out that does not support it because it is an out= function");
}
at::Tensor sqrt(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SqrtBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqrtBackward0>(new SqrtBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::sqrt(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sqrt");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sqrt");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (self_t.conj() / (2 * result.conj())).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor squeeze_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SqueezeBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqueezeBackward0_copy>(new SqueezeBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::squeeze_copy(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: squeeze_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: squeeze_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "squeeze_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::squeeze(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor squeeze_copy_dim(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SqueezeBackward1_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqueezeBackward1_copy>(new SqueezeBackward1_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::squeeze_copy(ks & c10::after_autograd_keyset, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: squeeze_copy_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: squeeze_copy_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "squeeze_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::squeeze(self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor squeeze_copy_dims(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SqueezeBackward2_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqueezeBackward2_copy>(new SqueezeBackward2_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim.vec();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::squeeze_copy(ks & c10::after_autograd_keyset, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: squeeze_copy_dims");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: squeeze_copy_dims");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "squeeze_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::squeeze(self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & stack_out_out(c10::DispatchKeySet ks, at::TensorList tensors, int64_t dim, at::Tensor & out) {
  auto tensors_ = unpack(tensors, "tensors", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( tensors );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = true;
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( tensors )) {
    throw_error_out_requires_grad("stack");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("stack");
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> tensors__storage_saved(tensors_.size());
  for (const Tensor& tensor : tensors_)
    tensors__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensors__impl_saved(tensors_.size());
  for (size_t i=0; i<tensors_.size(); i++)
    if (tensors_[i].defined()) tensors__impl_saved[i] = tensors_[i].getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::stack_outf(ks & c10::after_autograd_keyset, tensors_, dim, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<tensors_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensors_))
      TORCH_INTERNAL_ASSERT(tensors__storage_saved[i].value().is_alias_of(tensors_[i].storage()));
  }
  for (size_t i=0; i<tensors_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensors_))
      TORCH_INTERNAL_ASSERT(tensors__impl_saved[i] == tensors_[i].getIntrusivePtr());
  }
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(tensors) || isFwGradDefined(out))), "Trying to use forward AD with stack_out that does not support it because it is an out= function");
  return out;
}
at::Tensor std_correction(c10::DispatchKeySet ks, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<StdBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<StdBackward0>(new StdBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->correction = correction;
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::std(ks & c10::after_autograd_keyset, self_, dim, correction, keepdim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: std_correction");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: std_correction");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "std");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (at::real(var_backward(self_t.conj(), self_p, dim, correction, true).sum(dim.value_or(IntArrayRef({})), keepdim)) / (2. * result)).masked_fill_(result == 0, 0);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor> std_mean_correction(c10::DispatchKeySet ks, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(self));
  [[maybe_unused]] auto _any_has_forward_grad_result1 = (isFwGradDefined(self));
  std::shared_ptr<StdMeanBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<StdMeanBackward0>(new StdMeanBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->correction = correction;
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
    grad_fn->self_ = SavedVariable(self, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::std_mean(ks & c10::after_autograd_keyset, self_, dim, correction, keepdim);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: std_mean_correction");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: std_mean_correction");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: std_mean_correction");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: std_mean_correction");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1 ), grad_fn);
  }
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result0_new_fw_grad_opt = (at::real(var_backward(self_t.conj(), self_p, dim, correction, true).sum(dim.value_or(IntArrayRef({})), keepdim)) / (2. * result0)).masked_fill_(result0 == 0, 0);
  }
  c10::optional<at::Tensor> result1_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result1 && (result1.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result1_new_fw_grad_opt = mean(self_t, dim.value_or(IntArrayRef({})), keepdim);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (result1_new_fw_grad_opt.has_value() && result1_new_fw_grad_opt.value().defined() && result1.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result1._set_fw_grad(result1_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor & sub_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("sub");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("sub");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::sub_outf(ks & c10::after_autograd_keyset, self_, other_, alpha, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with sub_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & sum_out_IntList_out(c10::DispatchKeySet ks, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("sum");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("sum");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::sum_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, dtype, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with sum_out that does not support it because it is an out= function");
  return out;
}
void sym_constrain_range(c10::DispatchKeySet ks, const at::Scalar & size, c10::optional<int64_t> min, c10::optional<int64_t> max) {
  {
    at::AutoDispatchBelowADInplaceOrView guard;
    at::redispatch::sym_constrain_range(ks & c10::after_autograd_keyset, size, min, max);
  }
}
at::Tensor t(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<TBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TBackward0>(new TBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::t(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: t");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::t(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & tanh_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<TanhBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TanhBackward0>(new TanhBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::tanh_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((tanh_backward(self_t.conj(), self_p)).conj()) : (tanh_backward(self_t.conj(), self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor threshold(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ThresholdBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ThresholdBackward0>(new ThresholdBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->threshold = threshold;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::threshold(ks & c10::after_autograd_keyset, self_, threshold, value);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: threshold");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: threshold");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "threshold");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (threshold_backward(self_t.conj(), self_p, threshold)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & threshold_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("threshold");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("threshold");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::threshold_outf(ks & c10::after_autograd_keyset, self_, threshold, value, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with threshold_out that does not support it because it is an out= function");
  return out;
}
at::Tensor transpose_int(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim0, int64_t dim1) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<TransposeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TransposeBackward0>(new TransposeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim0 = dim0;
    grad_fn->dim1 = dim1;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::transpose(ks & c10::after_autograd_keyset, self_, dim0, dim1);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: transpose_int");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::transpose(self_t, dim0, dim1);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & transpose_(c10::DispatchKeySet ks, at::Tensor & self, int64_t dim0, int64_t dim1) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<TransposeBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TransposeBackward1>(new TransposeBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim0 = dim0;
    grad_fn->dim1 = dim1;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::transpose_(ks & c10::after_autograd_keyset, self_, dim0, dim1);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.transpose_(dim0, dim1);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
::std::tuple<at::Tensor,at::Tensor> triangular_solve(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular) {
  auto& self_ = unpack(self, "self", 0);
  auto& A_ = unpack(A, "A", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, A );
  
  [[maybe_unused]] auto _any_has_forward_grad_cloned_coefficient = (isFwGradDefined(A));
  [[maybe_unused]] auto _any_has_forward_grad_solution = (isFwGradDefined(self) || isFwGradDefined(A));
  std::shared_ptr<TriangularSolveBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TriangularSolveBackward0>(new TriangularSolveBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, A ));
    grad_fn->A_ = SavedVariable(A, false);
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->transpose = transpose;
    grad_fn->unitriangular = unitriangular;
    grad_fn->upper = upper;
  }
  at::Tensor solution;
  at::Tensor cloned_coefficient;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::triangular_solve(ks & c10::after_autograd_keyset, self_, A_, upper, transpose, unitriangular);
  })();
  std::tie(solution, cloned_coefficient) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (solution.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(solution)) {
    TORCH_INTERNAL_ASSERT(solution.storage().use_count() == 1, "function: triangular_solve");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(solution))
    TORCH_INTERNAL_ASSERT(solution.use_count() <= 1, "function: triangular_solve");
  if (cloned_coefficient.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(cloned_coefficient)) {
    TORCH_INTERNAL_ASSERT(cloned_coefficient.storage().use_count() == 1, "function: triangular_solve");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(cloned_coefficient))
    TORCH_INTERNAL_ASSERT(cloned_coefficient.use_count() <= 1, "function: triangular_solve");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( solution, cloned_coefficient ), grad_fn);
  }
  c10::optional<at::Tensor> cloned_coefficient_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_cloned_coefficient && (cloned_coefficient.defined())) {
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      cloned_coefficient_new_fw_grad_opt = A_t;
  }
  c10::optional<at::Tensor> solution_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_solution && (solution.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      auto A_p = toNonOptPrimal(A);
      solution_new_fw_grad_opt = triangular_solve_jvp(solution, A_p, A_t, self_t, upper, transpose, unitriangular);
  }
  if (cloned_coefficient_new_fw_grad_opt.has_value() && cloned_coefficient_new_fw_grad_opt.value().defined() && cloned_coefficient.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    cloned_coefficient._set_fw_grad(cloned_coefficient_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (solution_new_fw_grad_opt.has_value() && solution_new_fw_grad_opt.value().defined() && solution.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    solution._set_fw_grad(solution_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->solution_ = SavedVariable(solution, true);
  }
  return std::make_tuple(std::move(solution), std::move(cloned_coefficient));
}
at::Tensor triu(c10::DispatchKeySet ks, const at::Tensor & self, int64_t diagonal) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<TriuBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TriuBackward0>(new TriuBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->diagonal = diagonal;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::triu(ks & c10::after_autograd_keyset, self_, diagonal);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: triu");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: triu");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::triu(self_t, diagonal);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & triu_(c10::DispatchKeySet ks, at::Tensor & self, int64_t diagonal) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<TriuBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TriuBackward0>(new TriuBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->diagonal = diagonal;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::triu_(ks & c10::after_autograd_keyset, self_, diagonal);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(at::triu(self_t, diagonal)) : at::triu(self_t, diagonal);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & triu_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("triu");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("triu");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::triu_outf(ks & c10::after_autograd_keyset, self_, diagonal, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with triu_out that does not support it because it is an out= function");
  return out;
}
::std::vector<at::Tensor> unsafe_split_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymInt split_size, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UnsafeSplitBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnsafeSplitBackward0>(new UnsafeSplitBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_options = self.options();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
    grad_fn->split_size = split_size;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::unsafe_split_symint(ks & c10::after_autograd_keyset, self_, split_size, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<::std::vector<at::Tensor>> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::unsafe_split_symint(self_t, split_size, dim);
  }
  if (result_new_fw_grad_opt.has_value()) {
    auto result_new_fw_grad = result_new_fw_grad_opt.value();
    TORCH_INTERNAL_ASSERT(result.size() == result_new_fw_grad.size());
    for (const auto i : c10::irange(result.size())) {
      if (result_new_fw_grad[i].defined() && result[i].defined()) {
        // The hardcoded 0 here will need to be updated once we support multiple levels.
        result[i]._set_fw_grad(result_new_fw_grad[i], /* level */ 0, /* is_inplace_op */ false);
      }
    }
  }
  return result;
}
void unsafe_split_with_sizes_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim, at::TensorList out) {
  auto& self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::unsafe_split_with_sizes_symint_outf(ks & c10::after_autograd_keyset, self_, split_sizes, dim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with unsafe_split_with_sizes_out that does not support it because it is an out= function");
}
at::Tensor upsample_bicubic2d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UpsampleBicubic2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleBicubic2DBackward0>(new UpsampleBicubic2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->align_corners = align_corners;
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_bicubic2d_symint(ks & c10::after_autograd_keyset, self_, output_size, align_corners, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_bicubic2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_bicubic2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_bicubic2d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::upsample_bicubic2d_symint(self_t, output_size, align_corners, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor upsample_bicubic2d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<UpsampleBicubic2DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleBicubic2DBackwardBackward0>(new UpsampleBicubic2DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output ));
    grad_fn->align_corners = align_corners;
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_bicubic2d_backward_symint(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, align_corners, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_bicubic2d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_bicubic2d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_bicubic2d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result_new_fw_grad_opt = at::upsample_bicubic2d_backward_symint(grad_output_t, output_size, input_size, align_corners, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor upsample_bilinear2d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UpsampleBilinear2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleBilinear2DBackward0>(new UpsampleBilinear2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->align_corners = align_corners;
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_bilinear2d_symint(ks & c10::after_autograd_keyset, self_, output_size, align_corners, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_bilinear2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_bilinear2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_bilinear2d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::upsample_bilinear2d_symint(self_t, output_size, align_corners, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor upsample_linear1d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UpsampleLinear1DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleLinear1DBackward0>(new UpsampleLinear1DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->align_corners = align_corners;
    grad_fn->output_size = output_size.vec();
    grad_fn->scales = scales;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_linear1d_symint(ks & c10::after_autograd_keyset, self_, output_size, align_corners, scales);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_linear1d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_linear1d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_linear1d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::upsample_linear1d_symint(self_t, output_size, align_corners, scales);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor upsample_nearest2d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UpsampleNearest2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleNearest2DBackward0>(new UpsampleNearest2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_nearest2d_symint(ks & c10::after_autograd_keyset, self_, output_size, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_nearest2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_nearest2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_nearest2d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::upsample_nearest2d_symint(self_t, output_size, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & upsample_nearest2d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& grad_input_ = unpack(grad_input, "grad_input", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output )) {
    throw_error_out_requires_grad("upsample_nearest2d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("upsample_nearest2d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::upsample_nearest2d_backward_symint_outf(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, scales_h, scales_w, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(grad_input))), "Trying to use forward AD with upsample_nearest2d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & upsample_nearest2d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("upsample_nearest2d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("upsample_nearest2d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::upsample_nearest2d_symint_outf(ks & c10::after_autograd_keyset, self_, output_size, scales_h, scales_w, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with upsample_nearest2d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor upsample_nearest3d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<UpsampleNearest3DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleNearest3DBackwardBackward0>(new UpsampleNearest3DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output ));
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_d = scales_d;
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_nearest3d_backward_symint(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, scales_d, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_nearest3d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_nearest3d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_nearest3d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result_new_fw_grad_opt = at::upsample_nearest3d_backward_symint(grad_output_t, output_size, input_size, scales_d, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & upsample_nearest3d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& grad_input_ = unpack(grad_input, "grad_input", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output )) {
    throw_error_out_requires_grad("upsample_nearest3d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("upsample_nearest3d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::upsample_nearest3d_backward_symint_outf(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, scales_d, scales_h, scales_w, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(grad_input))), "Trying to use forward AD with upsample_nearest3d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & upsample_nearest3d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("upsample_nearest3d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("upsample_nearest3d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::upsample_nearest3d_symint_outf(ks & c10::after_autograd_keyset, self_, output_size, scales_d, scales_h, scales_w, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with upsample_nearest3d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor upsample_trilinear3d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<UpsampleTrilinear3DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleTrilinear3DBackwardBackward0>(new UpsampleTrilinear3DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output ));
    grad_fn->align_corners = align_corners;
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_d = scales_d;
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_trilinear3d_backward_symint(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_trilinear3d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_trilinear3d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_trilinear3d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result_new_fw_grad_opt = at::upsample_trilinear3d_backward_symint(grad_output_t, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & upsample_trilinear3d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& grad_input_ = unpack(grad_input, "grad_input", 7);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output )) {
    throw_error_out_requires_grad("upsample_trilinear3d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("upsample_trilinear3d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::upsample_trilinear3d_backward_symint_outf(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, align_corners, scales_d, scales_h, scales_w, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(grad_input))), "Trying to use forward AD with upsample_trilinear3d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor view(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ViewBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ViewBackward0>(new ViewBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::view_symint(ks & c10::after_autograd_keyset, self_, size);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: view");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t.view_symint(size);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor view_dtype(c10::DispatchKeySet ks, const at::Tensor & self, at::ScalarType dtype) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::view(ks & c10::after_autograd_keyset, self_, dtype);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: view_dtype");
  #endif
  return result;
}
at::Tensor view_as_real_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ViewAsRealBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ViewAsRealBackward0_copy>(new ViewAsRealBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::view_as_real_copy(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: view_as_real_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: view_as_real_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "view_as_real_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::view_as_real(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & xlogy__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(other));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<XlogyBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<XlogyBackward0>(new XlogyBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::xlogy_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(at::xlogy(original_self_t, other_p).masked_fill((original_self_p == 0.) & (other_p <= 0.), 0.) + other_t * original_self_p / other_p) : at::xlogy(original_self_t, other_p).masked_fill((original_self_p == 0.) & (other_p <= 0.), 0.) + other_t * original_self_p / other_p;
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & xlogy__Scalar_Other(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<XlogyBackward2> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<XlogyBackward2>(new XlogyBackward2(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->other = other;
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::xlogy_(ks & c10::after_autograd_keyset, self_, other);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((other.toDouble() > 0. ? at::xlogy(original_self_t.conj(),  other) : at::xlogy(original_self_t.conj(),  other).masked_fill(original_self_p == 0., 0.)).conj()) : (other.toDouble() > 0. ? at::xlogy(original_self_t.conj(),  other) : at::xlogy(original_self_t.conj(),  other).masked_fill(original_self_p == 0., 0.)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor zero(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ZeroBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ZeroBackward0>(new ZeroBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::zero(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: zero");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: zero");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "zero");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::zero_(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor zeros_like(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::zeros_like(ks & c10::after_autograd_keyset, self_, dtype, layout, device, pin_memory, memory_format);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: zeros_like");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: zeros_like");
  #endif
  return result;
}
}
}

namespace {

TORCH_LIBRARY_IMPL(aten, AutogradCUDA, m) {
m.impl("_test_autograd_multiple_dispatch_view_copy",
       TORCH_FN(VariableType::_test_autograd_multiple_dispatch_view_copy_AutogradCUDA)
);
}

TORCH_LIBRARY_IMPL(aten, AutogradNestedTensor, m) {
m.impl("select.int",
       TORCH_FN(VariableType::select_int_AutogradNestedTensor)
);
m.impl("split_with_sizes_copy.out",
       TORCH_FN(VariableType::split_with_sizes_copy_out_out_AutogradNestedTensor)
);
m.impl("squeeze_copy.dim",
       TORCH_FN(VariableType::squeeze_copy_dim_AutogradNestedTensor)
);
m.impl("squeeze_copy.dims",
       TORCH_FN(VariableType::squeeze_copy_dims_AutogradNestedTensor)
);
m.impl("sum.IntList_out",
       TORCH_FN(VariableType::sum_out_IntList_out_AutogradNestedTensor)
);
m.impl("view",
       TORCH_FN(VariableType::view_AutogradNestedTensor)
);
}

TORCH_LIBRARY_IMPL(aten, Autograd, m) {
m.impl("__ilshift__.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("__ilshift__.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("__irshift__.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("__irshift__.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("__lshift__.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("__lshift__.Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("__rshift__.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("__rshift__.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("_adaptive_avg_pool3d",
       TORCH_FN(VariableType::_adaptive_avg_pool3d)
);
m.impl("_adaptive_avg_pool3d.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_add_relu.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("_add_relu.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("_amp_update_scale_", torch::autograd::autogradNotImplementedFallback());
m.impl("_cholesky_solve_helper.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_coalesce",
       TORCH_FN(VariableType::_coalesce)
);
m.impl("_conj_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_convert_indices_from_coo_to_csr.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_convert_indices_from_csr_to_coo", torch::autograd::autogradNotImplementedFallback());
m.impl("_convert_indices_from_csr_to_coo.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_copy_from_and_resize", torch::autograd::autogradNotImplementedFallback());
m.impl("_cudnn_ctc_loss.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_cudnn_rnn_backward",
       TORCH_FN(VariableType::_cudnn_rnn_backward)
);
m.impl("_cudnn_rnn.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_dirichlet_grad.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_efficient_attention_forward",
       TORCH_FN(VariableType::_efficient_attention_forward)
);
m.impl("_efficientzerotensor.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_embedding_bag.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_empty_affine_quantized", torch::autograd::autogradNotImplementedFallback());
m.impl("_empty_per_channel_affine_quantized.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_euclidean_dist.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_fake_quantize_learnable_per_channel_affine_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("_fake_quantize_learnable_per_channel_affine.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_fake_quantize_learnable_per_tensor_affine_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("_fft_r2c",
       TORCH_FN(VariableType::_fft_r2c)
);
m.impl("_fill_mem_eff_dropout_mask_", torch::autograd::autogradNotImplementedFallback());
m.impl("_foobar.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_foreach_addcmul.Scalar",
       TORCH_FN(VariableType::_foreach_addcmul_Scalar)
);
m.impl("_foreach_addcmul.ScalarList",
       TORCH_FN(VariableType::_foreach_addcmul_ScalarList)
);
m.impl("_foreach_addcmul.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("_foreach_asin",
       TORCH_FN(VariableType::_foreach_asin)
);
m.impl("_foreach_asin.out",
       TORCH_FN(VariableType::_foreach_asin_out_out)
);
m.impl("_foreach_ceil_",
       TORCH_FN(VariableType::_foreach_ceil_)
);
m.impl("_foreach_ceil.out",
       TORCH_FN(VariableType::_foreach_ceil_out_out)
);
m.impl("_foreach_cos",
       TORCH_FN(VariableType::_foreach_cos)
);
m.impl("_foreach_cos_",
       TORCH_FN(VariableType::_foreach_cos_)
);
m.impl("_foreach_div.Scalar",
       TORCH_FN(VariableType::_foreach_div_Scalar)
);
m.impl("_foreach_div.List",
       TORCH_FN(VariableType::_foreach_div_List)
);
m.impl("_foreach_div.ScalarList",
       TORCH_FN(VariableType::_foreach_div_ScalarList)
);
m.impl("_foreach_div.Tensor",
       TORCH_FN(VariableType::_foreach_div_Tensor)
);
m.impl("_foreach_div_.Scalar",
       TORCH_FN(VariableType::_foreach_div__Scalar)
);
m.impl("_foreach_div_.List",
       TORCH_FN(VariableType::_foreach_div__List)
);
m.impl("_foreach_div_.ScalarList",
       TORCH_FN(VariableType::_foreach_div__ScalarList)
);
m.impl("_foreach_div_.Tensor",
       TORCH_FN(VariableType::_foreach_div__Tensor)
);
m.impl("_foreach_div.Scalar_out",
       TORCH_FN(VariableType::_foreach_div_out_Scalar_out)
);
m.impl("_foreach_div.List_out",
       TORCH_FN(VariableType::_foreach_div_out_List_out)
);
m.impl("_foreach_div.ScalarList_out",
       TORCH_FN(VariableType::_foreach_div_out_ScalarList_out)
);
m.impl("_foreach_div.Tensor_out",
       TORCH_FN(VariableType::_foreach_div_out_Tensor_out)
);
m.impl("_foreach_erfc.out",
       TORCH_FN(VariableType::_foreach_erfc_out_out)
);
m.impl("_foreach_expm1.out",
       TORCH_FN(VariableType::_foreach_expm1_out_out)
);
m.impl("_foreach_floor",
       TORCH_FN(VariableType::_foreach_floor)
);
m.impl("_foreach_floor_",
       TORCH_FN(VariableType::_foreach_floor_)
);
m.impl("_foreach_lgamma_",
       TORCH_FN(VariableType::_foreach_lgamma_)
);
m.impl("_foreach_lgamma.out",
       TORCH_FN(VariableType::_foreach_lgamma_out_out)
);
m.impl("_foreach_log2_",
       TORCH_FN(VariableType::_foreach_log2_)
);
m.impl("_foreach_mul.Scalar",
       TORCH_FN(VariableType::_foreach_mul_Scalar)
);
m.impl("_foreach_mul.List",
       TORCH_FN(VariableType::_foreach_mul_List)
);
m.impl("_foreach_mul.ScalarList",
       TORCH_FN(VariableType::_foreach_mul_ScalarList)
);
m.impl("_foreach_mul.Tensor",
       TORCH_FN(VariableType::_foreach_mul_Tensor)
);
m.impl("_foreach_mul_.Scalar",
       TORCH_FN(VariableType::_foreach_mul__Scalar)
);
m.impl("_foreach_mul_.List",
       TORCH_FN(VariableType::_foreach_mul__List)
);
m.impl("_foreach_mul_.ScalarList",
       TORCH_FN(VariableType::_foreach_mul__ScalarList)
);
m.impl("_foreach_mul_.Tensor",
       TORCH_FN(VariableType::_foreach_mul__Tensor)
);
m.impl("_foreach_mul.Scalar_out",
       TORCH_FN(VariableType::_foreach_mul_out_Scalar_out)
);
m.impl("_foreach_mul.List_out",
       TORCH_FN(VariableType::_foreach_mul_out_List_out)
);
m.impl("_foreach_mul.ScalarList_out",
       TORCH_FN(VariableType::_foreach_mul_out_ScalarList_out)
);
m.impl("_foreach_mul.Tensor_out",
       TORCH_FN(VariableType::_foreach_mul_out_Tensor_out)
);
m.impl("_foreach_pow_.List",
       TORCH_FN(VariableType::_foreach_pow__List)
);
m.impl("_foreach_pow_.Scalar",
       TORCH_FN(VariableType::_foreach_pow__Scalar)
);
m.impl("_foreach_pow_.ScalarList",
       TORCH_FN(VariableType::_foreach_pow__ScalarList)
);
m.impl("_foreach_round.out",
       TORCH_FN(VariableType::_foreach_round_out_out)
);
m.impl("_foreach_sin_",
       TORCH_FN(VariableType::_foreach_sin_)
);
m.impl("_foreach_tanh",
       TORCH_FN(VariableType::_foreach_tanh)
);
m.impl("_foreach_zero", torch::autograd::autogradNotImplementedFallback());
m.impl("_foreach_zero_",
       TORCH_FN(VariableType::_foreach_zero_)
);
m.impl("_functional_sym_constrain_range", torch::autograd::autogradNotImplementedFallback());
m.impl("_fw_primal_copy", torch::autograd::autogradNotImplementedFallback());
m.impl("_histogramdd_bin_edges.out",
       TORCH_FN(VariableType::_histogramdd_bin_edges_out_out)
);
m.impl("_histogramdd_from_bin_tensors.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_index_put_impl.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_is_any_true",
       TORCH_FN(VariableType::_is_any_true)
);
m.impl("_linalg_check_errors",
       TORCH_FN(VariableType::_linalg_check_errors)
);
m.impl("_linalg_det",
       TORCH_FN(VariableType::_linalg_det)
);
m.impl("_linalg_slogdet.sign",
       TORCH_FN(VariableType::_linalg_slogdet_out_sign)
);
m.impl("_log_softmax_backward_data.out",
       TORCH_FN(VariableType::_log_softmax_backward_data_out_out)
);
m.impl("_logcumsumexp", torch::autograd::autogradNotImplementedFallback());
m.impl("_masked_scale.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_masked_softmax",
       TORCH_FN(VariableType::_masked_softmax)
);
m.impl("_native_batch_norm_legit_functional",
       TORCH_FN(VariableType::_native_batch_norm_legit_functional)
);
m.impl("_native_batch_norm_legit_no_training",
       TORCH_FN(VariableType::_native_batch_norm_legit_no_training)
);
m.impl("_native_batch_norm_legit.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_native_batch_norm_legit.no_stats_out",
       TORCH_FN(VariableType::_native_batch_norm_legit_out_no_stats_out)
);
m.impl("_neg_view_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_nested_from_padded",
       TORCH_FN(VariableType::_nested_from_padded)
);
m.impl("_nested_sum_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("_nested_tensor_size.out",
       TORCH_FN(VariableType::_nested_tensor_size_out_out)
);
m.impl("_nested_tensor_strides.out",
       TORCH_FN(VariableType::_nested_tensor_strides_out_out)
);
m.impl("_nested_view_from_buffer",
       TORCH_FN(VariableType::_nested_view_from_buffer)
);
m.impl("_pack_padded_sequence",
       TORCH_FN(VariableType::_pack_padded_sequence)
);
m.impl("_pack_padded_sequence.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_pdist_backward",
       TORCH_FN(VariableType::_pdist_backward)
);
m.impl("_pdist_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_reshape_alias",
       TORCH_FN(VariableType::_reshape_alias)
);
m.impl("_reshape_alias_copy",
       TORCH_FN(VariableType::_reshape_alias_copy)
);
m.impl("_resize_output", torch::autograd::autogradNotImplementedFallback());
m.impl("_scaled_dot_product_efficient_attention",
       TORCH_FN(VariableType::_scaled_dot_product_efficient_attention)
);
m.impl("_scaled_mm", torch::autograd::autogradNotImplementedFallback());
m.impl("_segment_reduce_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_slow_conv2d_backward.output_mask",
       TORCH_FN(VariableType::_slow_conv2d_backward_output_mask)
);
m.impl("_softmax",
       TORCH_FN(VariableType::_softmax)
);
m.impl("_sparse_broadcast_to", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_coo_tensor_with_dims", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_csr_prod.dim_dtype", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_csr_prod.dim_dtype_out", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_log_softmax",
       TORCH_FN(VariableType::_sparse_log_softmax)
);
m.impl("_sparse_log_softmax_backward_data", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_log_softmax_backward_data.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_mm_reduce_impl",
       TORCH_FN(VariableType::_sparse_mm_reduce_impl)
);
m.impl("_sparse_semi_structured_linear", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_softmax_backward_data", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_sparse_matmul.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_spdiags", torch::autograd::autogradNotImplementedFallback());
m.impl("_test_autograd_multiple_dispatch_view_copy",
       TORCH_FN(VariableType::_test_autograd_multiple_dispatch_view_copy)
);
m.impl("_test_functorch_fallback", torch::autograd::autogradNotImplementedFallback());
m.impl("_test_optional_filled_intlist", torch::autograd::autogradNotImplementedFallback());
m.impl("_thnn_fused_gru_cell",
       TORCH_FN(VariableType::_thnn_fused_gru_cell)
);
m.impl("_thnn_fused_lstm_cell_backward_impl", torch::autograd::autogradNotImplementedFallback());
m.impl("_to_sparse.sparse_dim",
       TORCH_FN(VariableType::_to_sparse_sparse_dim)
);
m.impl("_to_sparse",
       TORCH_FN(VariableType::_to_sparse)
);
m.impl("_to_sparse_bsr.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_to_sparse_csc.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_transformer_encoder_layer_fwd.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_trilinear",
       TORCH_FN(VariableType::_trilinear)
);
m.impl("_unique2",
       TORCH_FN(VariableType::_unique2)
);
m.impl("_unsafe_index.Tensor",
       TORCH_FN(VariableType::_unsafe_index_Tensor)
);
m.impl("_upsample_bilinear2d_aa_backward.grad_input",
       TORCH_FN(VariableType::_upsample_bilinear2d_aa_backward_out_grad_input)
);
m.impl("_upsample_nearest_exact2d_backward.grad_input",
       TORCH_FN(VariableType::_upsample_nearest_exact2d_backward_out_grad_input)
);
m.impl("_upsample_nearest_exact3d_backward",
       TORCH_FN(VariableType::_upsample_nearest_exact3d_backward)
);
m.impl("_values",
       TORCH_FN(VariableType::_values)
);
m.impl("_weight_int4pack_mm", torch::autograd::autogradNotImplementedFallback());
m.impl("_weight_norm_interface_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("adaptive_avg_pool2d.out", torch::autograd::autogradNotImplementedFallback());
m.impl("adaptive_max_pool2d",
       TORCH_FN(VariableType::adaptive_max_pool2d)
);
m.impl("adaptive_max_pool2d_backward.grad_input",
       TORCH_FN(VariableType::adaptive_max_pool2d_backward_out_grad_input)
);
m.impl("adaptive_max_pool3d",
       TORCH_FN(VariableType::adaptive_max_pool3d)
);
m.impl("add_.Tensor",
       TORCH_FN(VariableType::add__Tensor)
);
m.impl("add_.Scalar",
       TORCH_FN(VariableType::add__Scalar)
);
m.impl("addbmm",
       TORCH_FN(VariableType::addbmm)
);
m.impl("addcmul.out",
       TORCH_FN(VariableType::addcmul_out_out)
);
m.impl("affine_grid_generator",
       TORCH_FN(VariableType::affine_grid_generator)
);
m.impl("affine_grid_generator.out", torch::autograd::autogradNotImplementedFallback());
m.impl("alias_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("any.out",
       TORCH_FN(VariableType::any_out_out)
);
m.impl("any.dims_out",
       TORCH_FN(VariableType::any_out_dims_out)
);
m.impl("any.all_out",
       TORCH_FN(VariableType::any_out_all_out)
);
m.impl("arange", torch::autograd::autogradNotImplementedFallback());
m.impl("arange.start", torch::autograd::autogradNotImplementedFallback());
m.impl("arange.start_step", torch::autograd::autogradNotImplementedFallback());
m.impl("as_strided_copy",
       TORCH_FN(VariableType::as_strided_copy)
);
m.impl("atan.out",
       TORCH_FN(VariableType::atan_out_out)
);
m.impl("atanh",
       TORCH_FN(VariableType::atanh)
);
m.impl("avg_pool2d_backward",
       TORCH_FN(VariableType::avg_pool2d_backward)
);
m.impl("avg_pool3d_backward.grad_input",
       TORCH_FN(VariableType::avg_pool3d_backward_out_grad_input)
);
m.impl("avg_pool3d.out",
       TORCH_FN(VariableType::avg_pool3d_out_out)
);
m.impl("bartlett_window", torch::autograd::autogradNotImplementedFallback());
m.impl("bartlett_window.periodic", torch::autograd::autogradNotImplementedFallback());
m.impl("batch_norm_gather_stats.out", torch::autograd::autogradNotImplementedFallback());
m.impl("batch_norm_gather_stats_with_counts", torch::autograd::autogradNotImplementedFallback());
m.impl("batch_norm_stats.out", torch::autograd::autogradNotImplementedFallback());
m.impl("bernoulli_.Tensor",
       TORCH_FN(VariableType::bernoulli__Tensor)
);
m.impl("bernoulli_.float",
       TORCH_FN(VariableType::bernoulli__float)
);
m.impl("binary_cross_entropy",
       TORCH_FN(VariableType::binary_cross_entropy)
);
m.impl("bitwise_left_shift_.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_left_shift_.Tensor_Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_not.out", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_xor.Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_xor.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_xor.Scalar_Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("bmm",
       TORCH_FN(VariableType::bmm)
);
m.impl("bmm.out",
       TORCH_FN(VariableType::bmm_out_out)
);
m.impl("cauchy_",
       TORCH_FN(VariableType::cauchy_)
);
m.impl("ccol_indices_copy",
       TORCH_FN(VariableType::ccol_indices_copy)
);
m.impl("celu",
       TORCH_FN(VariableType::celu)
);
m.impl("celu_",
       TORCH_FN(VariableType::celu_)
);
m.impl("channel_shuffle.out", torch::autograd::autogradNotImplementedFallback());
m.impl("clamp_min",
       TORCH_FN(VariableType::clamp_min)
);
m.impl("clamp_min.Tensor",
       TORCH_FN(VariableType::clamp_min_Tensor)
);
m.impl("clamp_min.out",
       TORCH_FN(VariableType::clamp_min_out_out)
);
m.impl("clamp_min.Tensor_out",
       TORCH_FN(VariableType::clamp_min_out_Tensor_out)
);
m.impl("col2im",
       TORCH_FN(VariableType::col2im)
);
m.impl("col_indices_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("conj_physical_",
       TORCH_FN(VariableType::conj_physical_)
);
m.impl("conv_depthwise3d",
       TORCH_FN(VariableType::conv_depthwise3d)
);
m.impl("convolution.out", torch::autograd::autogradNotImplementedFallback());
m.impl("convolution_overrideable.out", torch::autograd::autogradNotImplementedFallback());
m.impl("copy", torch::autograd::autogradNotImplementedFallback());
m.impl("cosh_",
       TORCH_FN(VariableType::cosh_)
);
m.impl("cosh.out",
       TORCH_FN(VariableType::cosh_out_out)
);
m.impl("crow_indices",
       TORCH_FN(VariableType::crow_indices)
);
m.impl("cudnn_affine_grid_generator_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("cudnn_convolution",
       TORCH_FN(VariableType::cudnn_convolution)
);
m.impl("cudnn_convolution_relu", torch::autograd::autogradNotImplementedFallback());
m.impl("cudnn_convolution_transpose.out", torch::autograd::autogradNotImplementedFallback());
m.impl("cummax.out",
       TORCH_FN(VariableType::cummax_out_out)
);
m.impl("cumprod",
       TORCH_FN(VariableType::cumprod)
);
m.impl("diag_embed",
       TORCH_FN(VariableType::diag_embed)
);
m.impl("diagonal",
       TORCH_FN(VariableType::diagonal)
);
m.impl("div_.Tensor",
       TORCH_FN(VariableType::div__Tensor)
);
m.impl("div_.Tensor_mode",
       TORCH_FN(VariableType::div__Tensor_mode)
);
m.impl("div_.Scalar",
       TORCH_FN(VariableType::div__Scalar)
);
m.impl("div_.Scalar_mode",
       TORCH_FN(VariableType::div__Scalar_mode)
);
m.impl("embedding_renorm_",
       TORCH_FN(VariableType::embedding_renorm_)
);
m.impl("empty_like",
       TORCH_FN(VariableType::empty_like)
);
m.impl("empty_permuted", torch::autograd::autogradNotImplementedFallback());
m.impl("eq_.Scalar",
       TORCH_FN(VariableType::eq__Scalar)
);
m.impl("eq_.Tensor",
       TORCH_FN(VariableType::eq__Tensor)
);
m.impl("equal",
       TORCH_FN(VariableType::equal)
);
m.impl("erf_",
       TORCH_FN(VariableType::erf_)
);
m.impl("exp2_",
       TORCH_FN(VariableType::exp2_)
);
m.impl("exp2.out",
       TORCH_FN(VariableType::exp2_out_out)
);
m.impl("exp.out",
       TORCH_FN(VariableType::exp_out_out)
);
m.impl("expand",
       TORCH_FN(VariableType::expand)
);
m.impl("eye.out", torch::autograd::autogradNotImplementedFallback());
m.impl("eye.m_out", torch::autograd::autogradNotImplementedFallback());
m.impl("fill.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("fill.Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("flip",
       TORCH_FN(VariableType::flip)
);
m.impl("floor",
       TORCH_FN(VariableType::floor)
);
m.impl("floor.out",
       TORCH_FN(VariableType::floor_out_out)
);
m.impl("fmin",
       TORCH_FN(VariableType::fmin)
);
m.impl("frac.out",
       TORCH_FN(VariableType::frac_out_out)
);
m.impl("fractional_max_pool2d",
       TORCH_FN(VariableType::fractional_max_pool2d)
);
m.impl("frexp.Tensor",
       TORCH_FN(VariableType::frexp_Tensor)
);
m.impl("gather",
       TORCH_FN(VariableType::gather)
);
m.impl("gather.out",
       TORCH_FN(VariableType::gather_out_out)
);
m.impl("ge_.Scalar",
       TORCH_FN(VariableType::ge__Scalar)
);
m.impl("ge_.Tensor",
       TORCH_FN(VariableType::ge__Tensor)
);
m.impl("gelu.out",
       TORCH_FN(VariableType::gelu_out_out)
);
m.impl("glu_backward_jvp.out", torch::autograd::autogradNotImplementedFallback());
m.impl("glu_backward.grad_input",
       TORCH_FN(VariableType::glu_backward_out_grad_input)
);
m.impl("grid_sampler_3d_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("gt.Scalar_out",
       TORCH_FN(VariableType::gt_out_Scalar_out)
);
m.impl("gt.Tensor_out",
       TORCH_FN(VariableType::gt_out_Tensor_out)
);
m.impl("hardsigmoid",
       TORCH_FN(VariableType::hardsigmoid)
);
m.impl("histogram.bins_tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("histogram.bin_ct_out", torch::autograd::autogradNotImplementedFallback());
m.impl("huber_loss",
       TORCH_FN(VariableType::huber_loss)
);
m.impl("huber_loss_backward",
       TORCH_FN(VariableType::huber_loss_backward)
);
m.impl("hypot_",
       TORCH_FN(VariableType::hypot_)
);
m.impl("hypot.out",
       TORCH_FN(VariableType::hypot_out_out)
);
m.impl("i0",
       TORCH_FN(VariableType::i0)
);
m.impl("index.Tensor_out",
       TORCH_FN(VariableType::index_out_Tensor_out)
);
m.impl("indices_copy",
       TORCH_FN(VariableType::indices_copy)
);
m.impl("is_set_to",
       TORCH_FN(VariableType::is_set_to)
);
m.impl("isposinf",
       TORCH_FN(VariableType::isposinf)
);
m.impl("isposinf.out",
       TORCH_FN(VariableType::isposinf_out_out)
);
m.impl("lerp.Scalar",
       TORCH_FN(VariableType::lerp_Scalar)
);
m.impl("lerp.Tensor",
       TORCH_FN(VariableType::lerp_Tensor)
);
m.impl("linalg_cross.out",
       TORCH_FN(VariableType::linalg_cross_out_out)
);
m.impl("linalg_inv_ex",
       TORCH_FN(VariableType::linalg_inv_ex)
);
m.impl("linalg_solve_triangular",
       TORCH_FN(VariableType::linalg_solve_triangular)
);
m.impl("linalg_solve_triangular.out",
       TORCH_FN(VariableType::linalg_solve_triangular_out_out)
);
m.impl("linalg_vector_norm",
       TORCH_FN(VariableType::linalg_vector_norm)
);
m.impl("linear_backward",
       TORCH_FN(VariableType::linear_backward)
);
m.impl("linear.out",
       TORCH_FN(VariableType::linear_out_out)
);
m.impl("log.out",
       TORCH_FN(VariableType::log_out_out)
);
m.impl("log_sigmoid_forward.output",
       TORCH_FN(VariableType::log_sigmoid_forward_out_output)
);
m.impl("logaddexp.out",
       TORCH_FN(VariableType::logaddexp_out_out)
);
m.impl("logcumsumexp.out",
       TORCH_FN(VariableType::logcumsumexp_out_out)
);
m.impl("logical_xor_",
       TORCH_FN(VariableType::logical_xor_)
);
m.impl("logical_xor.out",
       TORCH_FN(VariableType::logical_xor_out_out)
);
m.impl("logit",
       TORCH_FN(VariableType::logit)
);
m.impl("logit_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("logsumexp.out",
       TORCH_FN(VariableType::logsumexp_out_out)
);
m.impl("lstm_mps_backward",
       TORCH_FN(VariableType::lstm_mps_backward)
);
m.impl("lt_.Scalar",
       TORCH_FN(VariableType::lt__Scalar)
);
m.impl("lt_.Tensor",
       TORCH_FN(VariableType::lt__Tensor)
);
m.impl("masked_fill_.Scalar",
       TORCH_FN(VariableType::masked_fill__Scalar)
);
m.impl("masked_fill_.Tensor",
       TORCH_FN(VariableType::masked_fill__Tensor)
);
m.impl("matmul_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("max_pool2d",
       TORCH_FN(VariableType::max_pool2d)
);
m.impl("max_unpool2d",
       TORCH_FN(VariableType::max_unpool2d)
);
m.impl("max_unpool3d",
       TORCH_FN(VariableType::max_unpool3d)
);
m.impl("mean",
       TORCH_FN(VariableType::mean)
);
m.impl("mean.dim",
       TORCH_FN(VariableType::mean_dim)
);
m.impl("min.dim",
       TORCH_FN(VariableType::min_dim)
);
m.impl("min",
       TORCH_FN(VariableType::min)
);
m.impl("min.dim_min",
       TORCH_FN(VariableType::min_out_dim_min)
);
m.impl("min.unary_out",
       TORCH_FN(VariableType::min_out_unary_out)
);
m.impl("miopen_rnn_backward.out",
       TORCH_FN(VariableType::miopen_rnn_backward_out_out)
);
m.impl("mish_",
       TORCH_FN(VariableType::mish_)
);
m.impl("mish_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_linear_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_linear_backward_input", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_linear_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("mm",
       TORCH_FN(VariableType::mm)
);
m.impl("mps_convolution_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("mv",
       TORCH_FN(VariableType::mv)
);
m.impl("mvlgamma_",
       TORCH_FN(VariableType::mvlgamma_)
);
m.impl("narrow_copy", torch::autograd::autogradNotImplementedFallback());
m.impl("native_batch_norm_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("native_dropout_backward",
       TORCH_FN(VariableType::native_dropout_backward)
);
m.impl("native_dropout_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("native_dropout.out", torch::autograd::autogradNotImplementedFallback());
m.impl("native_group_norm",
       TORCH_FN(VariableType::native_group_norm)
);
m.impl("new_empty.out",
       TORCH_FN(VariableType::new_empty_out_out)
);
m.impl("new_full.out",
       TORCH_FN(VariableType::new_full_out_out)
);
m.impl("new_ones.out",
       TORCH_FN(VariableType::new_ones_out_out)
);
m.impl("nll_loss_backward",
       TORCH_FN(VariableType::nll_loss_backward)
);
m.impl("nonzero.out",
       TORCH_FN(VariableType::nonzero_out_out)
);
m.impl("norm.dtype_out",
       TORCH_FN(VariableType::norm_out_dtype_out)
);
m.impl("norm.out",
       TORCH_FN(VariableType::norm_out_out)
);
m.impl("norm.ScalarOpt_dtype_out", torch::autograd::autogradNotImplementedFallback());
m.impl("norm.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("ones.out", torch::autograd::autogradNotImplementedFallback());
m.impl("ones.names_out", torch::autograd::autogradNotImplementedFallback());
m.impl("ormqr",
       TORCH_FN(VariableType::ormqr)
);
m.impl("ormqr.out",
       TORCH_FN(VariableType::ormqr_out_out)
);
m.impl("permute",
       TORCH_FN(VariableType::permute)
);
m.impl("pixel_shuffle",
       TORCH_FN(VariableType::pixel_shuffle)
);
m.impl("pixel_shuffle.out", torch::autograd::autogradNotImplementedFallback());
m.impl("pixel_unshuffle.out", torch::autograd::autogradNotImplementedFallback());
m.impl("put.out", torch::autograd::autogradNotImplementedFallback());
m.impl("q_per_channel_scales",
       TORCH_FN(VariableType::q_per_channel_scales)
);
m.impl("q_zero_point",
       TORCH_FN(VariableType::q_zero_point)
);
m.impl("quantized_max_pool2d.out", torch::autograd::autogradNotImplementedFallback());
m.impl("randn_like.out",
       TORCH_FN(VariableType::randn_like_out_out)
);
m.impl("reciprocal_",
       TORCH_FN(VariableType::reciprocal_)
);
m.impl("reciprocal.out",
       TORCH_FN(VariableType::reciprocal_out_out)
);
m.impl("record_stream",
       TORCH_FN(VariableType::record_stream)
);
m.impl("reflection_pad1d.out",
       TORCH_FN(VariableType::reflection_pad1d_out_out)
);
m.impl("reflection_pad3d_backward",
       TORCH_FN(VariableType::reflection_pad3d_backward)
);
m.impl("remainder_.Scalar",
       TORCH_FN(VariableType::remainder__Scalar)
);
m.impl("remainder_.Tensor",
       TORCH_FN(VariableType::remainder__Tensor)
);
m.impl("repeat.out", torch::autograd::autogradNotImplementedFallback());
m.impl("replication_pad2d_backward",
       TORCH_FN(VariableType::replication_pad2d_backward)
);
m.impl("replication_pad3d",
       TORCH_FN(VariableType::replication_pad3d)
);
m.impl("resize_as.out", torch::autograd::autogradNotImplementedFallback());
m.impl("resize_as_sparse_", torch::autograd::autogradNotImplementedFallback());
m.impl("resize_as_sparse.out", torch::autograd::autogradNotImplementedFallback());
m.impl("resize.out", torch::autograd::autogradNotImplementedFallback());
m.impl("roll",
       TORCH_FN(VariableType::roll)
);
m.impl("roll.out", torch::autograd::autogradNotImplementedFallback());
m.impl("rrelu_with_noise_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("rsub.Tensor",
       TORCH_FN(VariableType::rsub_Tensor)
);
m.impl("rsub.Scalar",
       TORCH_FN(VariableType::rsub_Scalar)
);
m.impl("scatter_add_",
       TORCH_FN(VariableType::scatter_add_)
);
m.impl("scatter.src_out",
       TORCH_FN(VariableType::scatter_out_src_out)
);
m.impl("scatter.value_out",
       TORCH_FN(VariableType::scatter_out_value_out)
);
m.impl("scatter.reduce_out", torch::autograd::autogradNotImplementedFallback());
m.impl("scatter.value_reduce_out", torch::autograd::autogradNotImplementedFallback());
m.impl("select.int",
       TORCH_FN(VariableType::select_int)
);
m.impl("sign",
       TORCH_FN(VariableType::sign)
);
m.impl("silu",
       TORCH_FN(VariableType::silu)
);
m.impl("silu_backward.grad_input", torch::autograd::autogradNotImplementedFallback());
m.impl("silu.out",
       TORCH_FN(VariableType::silu_out_out)
);
m.impl("sinh",
       TORCH_FN(VariableType::sinh)
);
m.impl("sinh_",
       TORCH_FN(VariableType::sinh_)
);
m.impl("slice_backward",
       TORCH_FN(VariableType::slice_backward)
);
m.impl("slice_copy.Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("slow_conv3d_forward.output",
       TORCH_FN(VariableType::slow_conv3d_forward_out_output)
);
m.impl("slow_conv_dilated2d",
       TORCH_FN(VariableType::slow_conv_dilated2d)
);
m.impl("slow_conv_dilated3d.out", torch::autograd::autogradNotImplementedFallback());
m.impl("slow_conv_transpose2d",
       TORCH_FN(VariableType::slow_conv_transpose2d)
);
m.impl("smooth_l1_loss_backward",
       TORCH_FN(VariableType::smooth_l1_loss_backward)
);
m.impl("smooth_l1_loss_backward.grad_input",
       TORCH_FN(VariableType::smooth_l1_loss_backward_out_grad_input)
);
m.impl("sort.values",
       TORCH_FN(VariableType::sort_out_values)
);
m.impl("sort.values_stable",
       TORCH_FN(VariableType::sort_out_values_stable)
);
m.impl("sparse_coo_tensor.size_out", torch::autograd::autogradNotImplementedFallback());
m.impl("sparse_resize.out", torch::autograd::autogradNotImplementedFallback());
m.impl("special_bessel_j1",
       TORCH_FN(VariableType::special_bessel_j1)
);
m.impl("special_bessel_j1.out",
       TORCH_FN(VariableType::special_bessel_j1_out_out)
);
m.impl("special_bessel_y0.out",
       TORCH_FN(VariableType::special_bessel_y0_out_out)
);
m.impl("special_bessel_y1.out",
       TORCH_FN(VariableType::special_bessel_y1_out_out)
);
m.impl("special_chebyshev_polynomial_t.out",
       TORCH_FN(VariableType::special_chebyshev_polynomial_t_out_out)
);
m.impl("special_chebyshev_polynomial_t.x_scalar_out",
       TORCH_FN(VariableType::special_chebyshev_polynomial_t_out_x_scalar_out)
);
m.impl("special_chebyshev_polynomial_t.n_scalar_out",
       TORCH_FN(VariableType::special_chebyshev_polynomial_t_out_n_scalar_out)
);
m.impl("special_chebyshev_polynomial_v",
       TORCH_FN(VariableType::special_chebyshev_polynomial_v)
);
m.impl("special_chebyshev_polynomial_v.x_scalar",
       TORCH_FN(VariableType::special_chebyshev_polynomial_v_x_scalar)
);
m.impl("special_chebyshev_polynomial_v.n_scalar",
       TORCH_FN(VariableType::special_chebyshev_polynomial_v_n_scalar)
);
m.impl("special_i1e.out",
       TORCH_FN(VariableType::special_i1e_out_out)
);
m.impl("special_ndtri.out",
       TORCH_FN(VariableType::special_ndtri_out_out)
);
m.impl("special_shifted_chebyshev_polynomial_w.out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_w_out_out)
);
m.impl("special_shifted_chebyshev_polynomial_w.x_scalar_out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_w_out_x_scalar_out)
);
m.impl("special_shifted_chebyshev_polynomial_w.n_scalar_out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_w_out_n_scalar_out)
);
m.impl("special_xlog1py",
       TORCH_FN(VariableType::special_xlog1py)
);
m.impl("special_xlog1py.self_scalar",
       TORCH_FN(VariableType::special_xlog1py_self_scalar)
);
m.impl("special_xlog1py.other_scalar",
       TORCH_FN(VariableType::special_xlog1py_other_scalar)
);
m.impl("special_xlog1py.out",
       TORCH_FN(VariableType::special_xlog1py_out_out)
);
m.impl("special_xlog1py.self_scalar_out",
       TORCH_FN(VariableType::special_xlog1py_out_self_scalar_out)
);
m.impl("special_xlog1py.other_scalar_out",
       TORCH_FN(VariableType::special_xlog1py_out_other_scalar_out)
);
m.impl("split_copy.Tensor",
       TORCH_FN(VariableType::split_copy_Tensor)
);
m.impl("split_copy.Tensor_out",
       TORCH_FN(VariableType::split_copy_out_Tensor_out)
);
m.impl("split_with_sizes_copy.out",
       TORCH_FN(VariableType::split_with_sizes_copy_out_out)
);
m.impl("sqrt",
       TORCH_FN(VariableType::sqrt)
);
m.impl("squeeze_copy",
       TORCH_FN(VariableType::squeeze_copy)
);
m.impl("squeeze_copy.dim",
       TORCH_FN(VariableType::squeeze_copy_dim)
);
m.impl("squeeze_copy.dims",
       TORCH_FN(VariableType::squeeze_copy_dims)
);
m.impl("stack.out",
       TORCH_FN(VariableType::stack_out_out)
);
m.impl("std.correction",
       TORCH_FN(VariableType::std_correction)
);
m.impl("std_mean.correction",
       TORCH_FN(VariableType::std_mean_correction)
);
m.impl("sub.out",
       TORCH_FN(VariableType::sub_out_out)
);
m.impl("sub.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("sum.IntList_out",
       TORCH_FN(VariableType::sum_out_IntList_out)
);
m.impl("sum.out", torch::autograd::autogradNotImplementedFallback());
m.impl("sym_constrain_range",
       TORCH_FN(VariableType::sym_constrain_range)
);
m.impl("t",
       TORCH_FN(VariableType::t)
);
m.impl("tanh_",
       TORCH_FN(VariableType::tanh_)
);
m.impl("threshold",
       TORCH_FN(VariableType::threshold)
);
m.impl("threshold.out",
       TORCH_FN(VariableType::threshold_out_out)
);
m.impl("transpose.int",
       TORCH_FN(VariableType::transpose_int)
);
m.impl("transpose_",
       TORCH_FN(VariableType::transpose_)
);
m.impl("triangular_solve",
       TORCH_FN(VariableType::triangular_solve)
);
m.impl("triu",
       TORCH_FN(VariableType::triu)
);
m.impl("triu_",
       TORCH_FN(VariableType::triu_)
);
m.impl("triu.out",
       TORCH_FN(VariableType::triu_out_out)
);
m.impl("unsafe_split.Tensor",
       TORCH_FN(VariableType::unsafe_split_Tensor)
);
m.impl("unsafe_split_with_sizes.out",
       TORCH_FN(VariableType::unsafe_split_with_sizes_out_out)
);
m.impl("upsample_bicubic2d",
       TORCH_FN(VariableType::upsample_bicubic2d)
);
m.impl("upsample_bicubic2d_backward",
       TORCH_FN(VariableType::upsample_bicubic2d_backward)
);
m.impl("upsample_bilinear2d",
       TORCH_FN(VariableType::upsample_bilinear2d)
);
m.impl("upsample_linear1d",
       TORCH_FN(VariableType::upsample_linear1d)
);
m.impl("upsample_nearest2d",
       TORCH_FN(VariableType::upsample_nearest2d)
);
m.impl("upsample_nearest2d_backward.grad_input",
       TORCH_FN(VariableType::upsample_nearest2d_backward_out_grad_input)
);
m.impl("upsample_nearest2d.out",
       TORCH_FN(VariableType::upsample_nearest2d_out_out)
);
m.impl("upsample_nearest3d_backward",
       TORCH_FN(VariableType::upsample_nearest3d_backward)
);
m.impl("upsample_nearest3d_backward.grad_input",
       TORCH_FN(VariableType::upsample_nearest3d_backward_out_grad_input)
);
m.impl("upsample_nearest3d.out",
       TORCH_FN(VariableType::upsample_nearest3d_out_out)
);
m.impl("upsample_trilinear3d_backward",
       TORCH_FN(VariableType::upsample_trilinear3d_backward)
);
m.impl("upsample_trilinear3d_backward.grad_input",
       TORCH_FN(VariableType::upsample_trilinear3d_backward_out_grad_input)
);
m.impl("var_mean.correction_out", torch::autograd::autogradNotImplementedFallback());
m.impl("view",
       TORCH_FN(VariableType::view)
);
m.impl("view.dtype",
       TORCH_FN(VariableType::view_dtype)
);
m.impl("view_as_real_copy",
       TORCH_FN(VariableType::view_as_real_copy)
);
m.impl("view_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("view_copy.dtype_out", torch::autograd::autogradNotImplementedFallback());
m.impl("xlogy_.Tensor",
       TORCH_FN(VariableType::xlogy__Tensor)
);
m.impl("xlogy_.Scalar_Other",
       TORCH_FN(VariableType::xlogy__Scalar_Other)
);
m.impl("zero",
       TORCH_FN(VariableType::zero)
);
m.impl("zeros_like",
       TORCH_FN(VariableType::zeros_like)
);
}

}

} // namespace torch::autograd
