#include "torch/csrc/autograd/VariableTypeUtils.h"
#include "torch/csrc/autograd/generated/VariableType.h"
#include "torch/csrc/autograd/FunctionsManual.h"

#include <ATen/RedispatchFunctions.h>
#include <c10/core/impl/TorchDispatchModeTLS.h>
#include <ATen/core/TorchDispatchUtils.h>
#include <torch/library.h>

#include <ATen/SparseCsrTensorUtils.h>


// @generated by torchgen/gen.py from VariableType.cpp

// NOTE [Sharded File]: on this file's split-into-shards state
//
// Back in the good old days, VariableType.cpp was generated as one
// file with every function in it, and everything was great and
// simple.
//
// However, this file was also very large (over 36,000 lines), and
// compiling it was very slow, and in fact was a significant
// bottleneck for incremental rebuilds. To address this, we now
// generate the file split across multiple shards, named
// VariableType_0.cpp and so on, which can be compiled in parallel.
//
// For ease of inspection and debugging, so that it's not necessary to
// go rooting around in multiple files, we also generate all the
// functions together in VariableTypeEverything.cpp. This generated
// file is only for convenience; it's not actually used in the
// build. If the file you're looking at now is one of the shards, you
// may want to switch over to the Everything variant to make you
// grepping smoother.

using namespace at;
using namespace torch::autograd::generated;
using namespace torch::autograd::generated::details;


namespace torch::autograd {

namespace VariableType {
namespace{
  C10_UNUSED void reset_grad_accumulator(Variable & self) {
    AutogradMeta* meta = torch::autograd::impl::get_autograd_meta(self);
    if (meta != nullptr) {
      meta->grad_accumulator_.reset();
    }
  }
}

namespace {


at::Tensor _test_autograd_multiple_dispatch_view_AutogradCUDA(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<TestAutogradMultipleDispatchViewBackwardAutogradCUDA0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TestAutogradMultipleDispatchViewBackwardAutogradCUDA0>(new TestAutogradMultipleDispatchViewBackwardAutogradCUDA0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_test_autograd_multiple_dispatch_view", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_test_autograd_multiple_dispatch_view", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowAutograd guard;
      return at::redispatch::_test_autograd_multiple_dispatch_view(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _test_autograd_multiple_dispatch_view");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_test_autograd_multiple_dispatch_view");
  return result;
}

at::Tensor select_copy_int_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, c10::SymInt index) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<SelectBackwardAutogradNestedTensor0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SelectBackwardAutogradNestedTensor0_copy>(new SelectBackwardAutogradNestedTensor0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->index = index;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::select_copy", "int");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("select_copy", *opt_op, ks, self, dim, index);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::select_copy_symint(ks & c10::after_autograd_keyset, self_, dim, index);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: select_copy_int");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: select_copy_int");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "select_copy");
  return result;
}
::std::vector<at::Tensor> split_with_sizes_copy_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<SplitWithSizesBackwardAutogradNestedTensor0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SplitWithSizesBackwardAutogradNestedTensor0_copy>(new SplitWithSizesBackwardAutogradNestedTensor0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->self_options = self.options();
    grad_fn->split_sizes = split_sizes.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::split_with_sizes_copy", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<::std::vector<at::Tensor>>("split_with_sizes_copy", *opt_op, ks, self, split_sizes, dim);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::split_with_sizes_copy_symint(ks & c10::after_autograd_keyset, self_, split_sizes, dim);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  return result;
}
::std::vector<at::Tensor> unbind_copy_int_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UnbindBackwardAutogradNestedTensor0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnbindBackwardAutogradNestedTensor0_copy>(new UnbindBackwardAutogradNestedTensor0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->self_options = self.options();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::unbind_copy(ks & c10::after_autograd_keyset, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<::std::vector<at::Tensor>> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::unbind(self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value()) {
    auto result_new_fw_grad = result_new_fw_grad_opt.value();
    TORCH_INTERNAL_ASSERT(result.size() == result_new_fw_grad.size());
    for (const auto i : c10::irange(result.size())) {
      if (result_new_fw_grad[i].defined() && result[i].defined()) {
        // The hardcoded 0 here will need to be updated once we support multiple levels.
        result[i]._set_fw_grad(result_new_fw_grad[i], /* level */ 0, /* is_inplace_op */ false);
      }
    }
  }
  return result;
}

at::Tensor _adaptive_avg_pool2d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<AdaptiveAvgPool2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AdaptiveAvgPool2DBackward0>(new AdaptiveAvgPool2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_adaptive_avg_pool2d_symint(ks & c10::after_autograd_keyset, self_, output_size);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _adaptive_avg_pool2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _adaptive_avg_pool2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_adaptive_avg_pool2d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_adaptive_avg_pool2d_symint(self_t, output_size);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
void _assert_async(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowADInplaceOrView guard;
    at::redispatch::_assert_async(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
}
void _assert_async_msg(c10::DispatchKeySet ks, const at::Tensor & self, c10::string_view assert_msg) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowADInplaceOrView guard;
    at::redispatch::_assert_async(ks & c10::after_autograd_keyset, self_, assert_msg);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
}
int64_t _cslt_sparse_mm_search(c10::DispatchKeySet ks, const at::Tensor & compressed_A, const at::Tensor & dense_B, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & alpha, c10::optional<at::ScalarType> out_dtype, bool transpose_result) {
  auto& compressed_A_ = unpack(compressed_A, "compressed_A", 0);
  auto& dense_B_ = unpack(dense_B, "dense_B", 1);
  #ifndef NDEBUG
  c10::optional<Storage> compressed_A__storage_saved =
    compressed_A_.has_storage() ? c10::optional<Storage>(compressed_A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> compressed_A__impl_saved;
  if (compressed_A_.defined()) compressed_A__impl_saved = compressed_A_.getIntrusivePtr();
  c10::optional<Storage> dense_B__storage_saved =
    dense_B_.has_storage() ? c10::optional<Storage>(dense_B_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> dense_B__impl_saved;
  if (dense_B_.defined()) dense_B__impl_saved = dense_B_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_cslt_sparse_mm_search(ks & c10::after_autograd_keyset, compressed_A_, dense_B_, bias, alpha, out_dtype, transpose_result);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (compressed_A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(compressed_A_))
    TORCH_INTERNAL_ASSERT(compressed_A__storage_saved.value().is_alias_of(compressed_A_.storage()));
  if (compressed_A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(compressed_A_))
    TORCH_INTERNAL_ASSERT(compressed_A__impl_saved == compressed_A_.getIntrusivePtr());
  if (dense_B__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(dense_B_))
    TORCH_INTERNAL_ASSERT(dense_B__storage_saved.value().is_alias_of(dense_B_.storage()));
  if (dense_B__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(dense_B_))
    TORCH_INTERNAL_ASSERT(dense_B__impl_saved == dense_B_.getIntrusivePtr());
  #endif
  return result;
}
int64_t _dimI(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_dimI(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor _efficientzerotensor(c10::DispatchKeySet ks, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_efficientzerotensor_symint(ks & c10::after_autograd_keyset, size, dtype, layout, device, pin_memory);
  })();
  auto result = std::move(_tmp);
  return result;
}
at::Tensor _embedding_bag_dense_backward(c10::DispatchKeySet ks, const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, c10::SymInt num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
  auto& grad_ = unpack(grad, "grad", 0);
  auto& indices_ = unpack(indices, "indices", 1);
  auto& offset2bag_ = unpack(offset2bag, "offset2bag", 2);
  auto& bag_size_ = unpack(bag_size, "bag_size", 3);
  auto& maximum_indices_ = unpack(maximum_indices, "maximum_indices", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad, per_sample_weights );
  
  std::shared_ptr<NotImplemented> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("_embedding_bag_dense_backward"), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad, per_sample_weights ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad__storage_saved =
    grad_.has_storage() ? c10::optional<Storage>(grad_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad__impl_saved;
  if (grad_.defined()) grad__impl_saved = grad_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  c10::optional<Storage> offset2bag__storage_saved =
    offset2bag_.has_storage() ? c10::optional<Storage>(offset2bag_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> offset2bag__impl_saved;
  if (offset2bag_.defined()) offset2bag__impl_saved = offset2bag_.getIntrusivePtr();
  c10::optional<Storage> bag_size__storage_saved =
    bag_size_.has_storage() ? c10::optional<Storage>(bag_size_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> bag_size__impl_saved;
  if (bag_size_.defined()) bag_size__impl_saved = bag_size_.getIntrusivePtr();
  c10::optional<Storage> maximum_indices__storage_saved =
    maximum_indices_.has_storage() ? c10::optional<Storage>(maximum_indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> maximum_indices__impl_saved;
  if (maximum_indices_.defined()) maximum_indices__impl_saved = maximum_indices_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(grad) || isFwGradDefined(per_sample_weights))) {
      static c10::OperatorName full_name("aten::_embedding_bag_dense_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_embedding_bag_dense_backward", *opt_op, ks, grad, indices, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_embedding_bag_dense_backward_symint(ks & c10::after_autograd_keyset, grad_, indices_, offset2bag_, bag_size_, maximum_indices_, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_))
    TORCH_INTERNAL_ASSERT(grad__storage_saved.value().is_alias_of(grad_.storage()));
  if (grad__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_))
    TORCH_INTERNAL_ASSERT(grad__impl_saved == grad_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (offset2bag__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(offset2bag_))
    TORCH_INTERNAL_ASSERT(offset2bag__storage_saved.value().is_alias_of(offset2bag_.storage()));
  if (offset2bag__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(offset2bag_))
    TORCH_INTERNAL_ASSERT(offset2bag__impl_saved == offset2bag_.getIntrusivePtr());
  if (bag_size__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(bag_size_))
    TORCH_INTERNAL_ASSERT(bag_size__storage_saved.value().is_alias_of(bag_size_.storage()));
  if (bag_size__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(bag_size_))
    TORCH_INTERNAL_ASSERT(bag_size__impl_saved == bag_size_.getIntrusivePtr());
  if (maximum_indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(maximum_indices_))
    TORCH_INTERNAL_ASSERT(maximum_indices__storage_saved.value().is_alias_of(maximum_indices_.storage()));
  if (maximum_indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(maximum_indices_))
    TORCH_INTERNAL_ASSERT(maximum_indices__impl_saved == maximum_indices_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _embedding_bag_dense_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _embedding_bag_dense_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_embedding_bag_dense_backward");
  return result;
}
at::Tensor _fft_c2r(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, c10::SymInt last_dim_size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<FftC2RBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FftC2RBackward0>(new FftC2RBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim.vec();
    grad_fn->normalization = normalization;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_fft_c2r_symint(ks & c10::after_autograd_keyset, self_, dim, normalization, last_dim_size);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _fft_c2r");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _fft_c2r");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_fft_c2r");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_fft_c2r_symint(self_t, dim, normalization, last_dim_size);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::vector<at::Tensor> _foreach_abs(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachAbsBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachAbsBackward0>(new ForeachAbsBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_abs(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = handle_r_to_c(result[i].scalar_type(), self_t.conj() * self_p.sgn());
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_add__Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<AddBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<AddBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<AddBackward1>(new AddBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_add_(ks & c10::after_autograd_keyset, self_, scalar);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(self_t.clone()) : self_t.clone();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_add__List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<AddBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<AddBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<AddBackward0>(new AddBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->alpha = alpha;
                grad_fn->other_scalar_type = other[i].scalar_type();
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_add_(ks & c10::after_autograd_keyset, self_, other_, alpha);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(self_t + maybe_multiply(other_t, alpha)) : self_t + maybe_multiply(other_t, alpha);
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_add__ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<AddBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<AddBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<AddBackward1>(new AddBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_add_(ks & c10::after_autograd_keyset, self_, scalars);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(self_t.clone()) : self_t.clone();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_add__Tensor(c10::DispatchKeySet ks, at::TensorList self, const at::Tensor & other, const at::Scalar & alpha) {
  auto self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(other);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<AddBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<AddBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<AddBackward0>(new AddBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->alpha = alpha;
                grad_fn->other_scalar_type = other.scalar_type();
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_add_(ks & c10::after_autograd_keyset, self_, other_, alpha);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto other_t_raw = toNonOptFwGrad(other);
        auto other_tensor = toNonOptTensor(other);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(self_t + maybe_multiply(other_t, alpha)) : self_t + maybe_multiply(other_t, alpha);
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_addcdiv_out_Scalar_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto tensor1_ = unpack(tensor1, "tensor1", 1);
  auto tensor2_ = unpack(tensor2, "tensor2", 2);
  auto out_ = unpack(out, "out", 4);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor1__storage_saved(tensor1_.size());
  for (const Tensor& tensor : tensor1_)
    tensor1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor1__impl_saved(tensor1_.size());
  for (size_t i=0; i<tensor1_.size(); i++)
    if (tensor1_[i].defined()) tensor1__impl_saved[i] = tensor1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor2__storage_saved(tensor2_.size());
  for (const Tensor& tensor : tensor2_)
    tensor2__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor2__impl_saved(tensor2_.size());
  for (size_t i=0; i<tensor2_.size(); i++)
    if (tensor2_[i].defined()) tensor2__impl_saved[i] = tensor2_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_addcdiv_outf(ks & c10::after_autograd_keyset, self_, tensor1_, tensor2_, value, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__storage_saved[i].value().is_alias_of(tensor1_[i].storage()));
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__impl_saved[i] == tensor1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__storage_saved[i].value().is_alias_of(tensor2_[i].storage()));
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__impl_saved[i] == tensor2_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(tensor1) || isFwGradDefinedTensorList(tensor2) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_addcdiv_out that does not support it because it is an out= function");
}
void _foreach_addcdiv_out_ScalarList_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto tensor1_ = unpack(tensor1, "tensor1", 1);
  auto tensor2_ = unpack(tensor2, "tensor2", 2);
  auto out_ = unpack(out, "out", 4);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor1__storage_saved(tensor1_.size());
  for (const Tensor& tensor : tensor1_)
    tensor1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor1__impl_saved(tensor1_.size());
  for (size_t i=0; i<tensor1_.size(); i++)
    if (tensor1_[i].defined()) tensor1__impl_saved[i] = tensor1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor2__storage_saved(tensor2_.size());
  for (const Tensor& tensor : tensor2_)
    tensor2__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor2__impl_saved(tensor2_.size());
  for (size_t i=0; i<tensor2_.size(); i++)
    if (tensor2_[i].defined()) tensor2__impl_saved[i] = tensor2_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_addcdiv_outf(ks & c10::after_autograd_keyset, self_, tensor1_, tensor2_, scalars, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__storage_saved[i].value().is_alias_of(tensor1_[i].storage()));
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__impl_saved[i] == tensor1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__storage_saved[i].value().is_alias_of(tensor2_[i].storage()));
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__impl_saved[i] == tensor2_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(tensor1) || isFwGradDefinedTensorList(tensor2) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_addcdiv_out that does not support it because it is an out= function");
}
void _foreach_addcdiv_out_Tensor_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto tensor1_ = unpack(tensor1, "tensor1", 1);
  auto tensor2_ = unpack(tensor2, "tensor2", 2);
  auto& scalars_ = unpack(scalars, "scalars", 3);
  auto out_ = unpack(out, "out", 4);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor1__storage_saved(tensor1_.size());
  for (const Tensor& tensor : tensor1_)
    tensor1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor1__impl_saved(tensor1_.size());
  for (size_t i=0; i<tensor1_.size(); i++)
    if (tensor1_[i].defined()) tensor1__impl_saved[i] = tensor1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor2__storage_saved(tensor2_.size());
  for (const Tensor& tensor : tensor2_)
    tensor2__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor2__impl_saved(tensor2_.size());
  for (size_t i=0; i<tensor2_.size(); i++)
    if (tensor2_[i].defined()) tensor2__impl_saved[i] = tensor2_[i].getIntrusivePtr();
  c10::optional<Storage> scalars__storage_saved =
    scalars_.has_storage() ? c10::optional<Storage>(scalars_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> scalars__impl_saved;
  if (scalars_.defined()) scalars__impl_saved = scalars_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_addcdiv_outf(ks & c10::after_autograd_keyset, self_, tensor1_, tensor2_, scalars_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__storage_saved[i].value().is_alias_of(tensor1_[i].storage()));
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__impl_saved[i] == tensor1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__storage_saved[i].value().is_alias_of(tensor2_[i].storage()));
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__impl_saved[i] == tensor2_[i].getIntrusivePtr());
  }
  if (scalars__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(scalars_))
    TORCH_INTERNAL_ASSERT(scalars__storage_saved.value().is_alias_of(scalars_.storage()));
  if (scalars__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(scalars_))
    TORCH_INTERNAL_ASSERT(scalars__impl_saved == scalars_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(tensor1) || isFwGradDefinedTensorList(tensor2) || isFwGradDefined(scalars) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_addcdiv_out that does not support it because it is an out= function");
}
void _foreach_atan_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<AtanBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<AtanBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<AtanBackward0>(new AtanBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_atan_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() / (original_self_p * original_self_p + 1).conj()).conj()) : (original_self_t.conj() / (original_self_p * original_self_p + 1).conj()).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
::std::vector<at::Tensor> _foreach_clamp_max_Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachClampMaxBackward0Scalar> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachClampMaxBackward0Scalar>(new ForeachClampMaxBackward0Scalar(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->scalar = scalar;
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_clamp_max(ks & c10::after_autograd_keyset, self_, scalar);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (where(self_p <= scalar, self_t.conj(), at::scalar_tensor(0., self_t.conj().options()))).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_clamp_max_List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::shared_ptr<ForeachClampMaxBackward1List> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachClampMaxBackward1List>(new ForeachClampMaxBackward1List(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = make_saved_variable_list(other, false);
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
    grad_fn->other_size_ = other.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_clamp_max(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other[i]);
        result_new_fw_grad_opts[i] = where(self_p <= other_p, self_t, other_t);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_clamp_max_ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachClampMaxBackward0ScalarList> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachClampMaxBackward0ScalarList>(new ForeachClampMaxBackward0ScalarList(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->scalars = scalars.vec();
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_clamp_max(ks & c10::after_autograd_keyset, self_, scalars);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (where(self_p <= scalars[i], self_t.conj(), at::scalar_tensor(0., self_t.conj().options()))).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_clamp_min_out_Scalar_out(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_clamp_min_outf(ks & c10::after_autograd_keyset, self_, scalar, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_clamp_min_out that does not support it because it is an out= function");
}
void _foreach_clamp_min_out_List_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_clamp_min_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(other) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_clamp_min_out that does not support it because it is an out= function");
}
void _foreach_clamp_min_out_ScalarList_out(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_clamp_min_outf(ks & c10::after_autograd_keyset, self_, scalars, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_clamp_min_out that does not support it because it is an out= function");
}
void _foreach_erf_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<ErfBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<ErfBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<ErfBackward0>(new ErfBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_erf_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((2.0 / sqrt(M_PI) * exp(-(original_self_p.pow(2))) * original_self_t.conj()).conj()) : (2.0 / sqrt(M_PI) * exp(-(original_self_p.pow(2))) * original_self_t.conj()).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_erf_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_erf_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_erf_out that does not support it because it is an out= function");
}
void _foreach_erfc_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<ErfcBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<ErfcBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<ErfcBackward0>(new ErfcBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_erfc_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((-2.0 / sqrt(M_PI) * exp(-(original_self_p.pow(2))) * original_self_t.conj()).conj()) : (-2.0 / sqrt(M_PI) * exp(-(original_self_p.pow(2))) * original_self_t.conj()).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_exp_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<ExpBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<ExpBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<ExpBackward0>(new ExpBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_exp_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((self_t.conj() * self_p.conj()).conj()) : (self_t.conj() * self_p.conj()).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
  if (!grad_fns.empty()) {
  
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              grad_fn->result_ = SavedVariable(self[i], true, self[i].is_view());
          }
      }
  }
}
void _foreach_exp_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_exp_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_exp_out that does not support it because it is an out= function");
}
::std::vector<at::Tensor> _foreach_expm1(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachExpm1Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachExpm1Backward0>(new ForeachExpm1Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_expm1(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = (self_t.conj() * (result[i].conj() + 1)).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  if (grad_fn) {
    grad_fn->result_ = make_saved_variable_list(result, true);
  }
  return result;
}
void _foreach_floor_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_floor_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_floor_out that does not support it because it is an out= function");
}
void _foreach_lerp__List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensors1, at::TensorList weights) {
  auto self_ = unpack(self, "self", 0);
  auto tensors1_ = unpack(tensors1, "tensors1", 1);
  auto weights_ = unpack(weights, "weights", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, tensors1, weights );
  
  TORCH_CHECK(
      self.size() == tensors1.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      tensors1.size());
  TORCH_CHECK(
      self.size() == weights.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      weights.size());
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(tensors1[i]) || isFwGradDefined(weights[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<LerpBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], tensors1[i], weights[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<LerpBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<LerpBackward1>(new LerpBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], tensors1[i], weights[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (grad_fn->should_compute_output(2)) {
                  grad_fn->end_ = SavedVariable(tensors1[i], false);
                }
                if (grad_fn->should_compute_output(2)) {
                  if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                  grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
                }
                grad_fn->weight_ = SavedVariable(weights[i], false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensors1__storage_saved(tensors1_.size());
  for (const Tensor& tensor : tensors1_)
    tensors1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensors1__impl_saved(tensors1_.size());
  for (size_t i=0; i<tensors1_.size(); i++)
    if (tensors1_[i].defined()) tensors1__impl_saved[i] = tensors1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> weights__storage_saved(weights_.size());
  for (const Tensor& tensor : weights_)
    weights__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> weights__impl_saved(weights_.size());
  for (size_t i=0; i<weights_.size(); i++)
    if (weights_[i].defined()) weights__impl_saved[i] = weights_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_lerp_(ks & c10::after_autograd_keyset, self_, tensors1_, weights_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__storage_saved[i].value().is_alias_of(tensors1_[i].storage()));
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__impl_saved[i] == tensors1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<weights_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weights__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(weights_))
      TORCH_INTERNAL_ASSERT(weights__storage_saved[i].value().is_alias_of(weights_[i].storage()));
  }
  for (size_t i=0; i<weights_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weights__impl_saved[i] && !at::impl::tensorlist_has_dispatch(weights_))
      TORCH_INTERNAL_ASSERT(weights__impl_saved[i] == weights_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto end_t_raw = toNonOptFwGrad(tensors1[i]);
        auto end_tensor = toNonOptTensor(tensors1[i]);
        auto end_t = (end_t_raw.defined() || !end_tensor.defined())
          ? end_t_raw : at::_efficientzerotensor(end_tensor.sizes(), end_tensor.options());
        auto end_p = toNonOptPrimal(tensors1[i]);
        auto weight_t_raw = toNonOptFwGrad(weights[i]);
        auto weight_tensor = toNonOptTensor(weights[i]);
        auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
          ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
        auto weight_p = toNonOptPrimal(weights[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(at::lerp(original_self_t, end_t, weight_p) + weight_t * (end_p - original_self_p)) : at::lerp(original_self_t, end_t, weight_p) + weight_t * (end_p - original_self_p);
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_lerp__Scalar(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensors1, const at::Scalar & weight) {
  auto self_ = unpack(self, "self", 0);
  auto tensors1_ = unpack(tensors1, "tensors1", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, tensors1 );
  
  TORCH_CHECK(
      self.size() == tensors1.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      tensors1.size());
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(tensors1[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<LerpBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], tensors1[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<LerpBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<LerpBackward0>(new LerpBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], tensors1[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->weight = weight;
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensors1__storage_saved(tensors1_.size());
  for (const Tensor& tensor : tensors1_)
    tensors1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensors1__impl_saved(tensors1_.size());
  for (size_t i=0; i<tensors1_.size(); i++)
    if (tensors1_[i].defined()) tensors1__impl_saved[i] = tensors1_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_lerp_(ks & c10::after_autograd_keyset, self_, tensors1_, weight);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__storage_saved[i].value().is_alias_of(tensors1_[i].storage()));
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__impl_saved[i] == tensors1_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto end_t_raw = toNonOptFwGrad(tensors1[i]);
        auto end_tensor = toNonOptTensor(tensors1[i]);
        auto end_t = (end_t_raw.defined() || !end_tensor.defined())
          ? end_t_raw : at::_efficientzerotensor(end_tensor.sizes(), end_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(at::lerp(self_t, end_t, weight)) : at::lerp(self_t, end_t, weight);
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_lerp_out_List_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensors1, at::TensorList weights, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto tensors1_ = unpack(tensors1, "tensors1", 1);
  auto weights_ = unpack(weights, "weights", 2);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensors1__storage_saved(tensors1_.size());
  for (const Tensor& tensor : tensors1_)
    tensors1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensors1__impl_saved(tensors1_.size());
  for (size_t i=0; i<tensors1_.size(); i++)
    if (tensors1_[i].defined()) tensors1__impl_saved[i] = tensors1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> weights__storage_saved(weights_.size());
  for (const Tensor& tensor : weights_)
    weights__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> weights__impl_saved(weights_.size());
  for (size_t i=0; i<weights_.size(); i++)
    if (weights_[i].defined()) weights__impl_saved[i] = weights_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_lerp_outf(ks & c10::after_autograd_keyset, self_, tensors1_, weights_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__storage_saved[i].value().is_alias_of(tensors1_[i].storage()));
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__impl_saved[i] == tensors1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<weights_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weights__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(weights_))
      TORCH_INTERNAL_ASSERT(weights__storage_saved[i].value().is_alias_of(weights_[i].storage()));
  }
  for (size_t i=0; i<weights_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weights__impl_saved[i] && !at::impl::tensorlist_has_dispatch(weights_))
      TORCH_INTERNAL_ASSERT(weights__impl_saved[i] == weights_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(tensors1) || isFwGradDefinedTensorList(weights) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_lerp_out that does not support it because it is an out= function");
}
void _foreach_lerp_out_Scalar_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensors1, const at::Scalar & weight, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto tensors1_ = unpack(tensors1, "tensors1", 1);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensors1__storage_saved(tensors1_.size());
  for (const Tensor& tensor : tensors1_)
    tensors1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensors1__impl_saved(tensors1_.size());
  for (size_t i=0; i<tensors1_.size(); i++)
    if (tensors1_[i].defined()) tensors1__impl_saved[i] = tensors1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_lerp_outf(ks & c10::after_autograd_keyset, self_, tensors1_, weight, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__storage_saved[i].value().is_alias_of(tensors1_[i].storage()));
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__impl_saved[i] == tensors1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(tensors1) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_lerp_out that does not support it because it is an out= function");
}
::std::vector<at::Tensor> _foreach_log10(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachLog10Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachLog10Backward0>(new ForeachLog10Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_log10(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (self_t.conj() / (self_p.conj() * 2.3025850929940456)).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_log10_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_log10_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_log10_out that does not support it because it is an out= function");
}
void _foreach_log2_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_log2_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_log2_out that does not support it because it is an out= function");
}
void _foreach_maximum__Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<NotImplemented>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<NotImplemented> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("_foreach_maximum_"), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_maximum_(ks & c10::after_autograd_keyset, self_, scalar);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self))), "Trying to use forward AD with _foreach_maximum_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
}
void _foreach_maximum__List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<MaximumBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<MaximumBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<MaximumBackward0>(new MaximumBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->other_ = SavedVariable(other[i], false);
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_maximum_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(other_t + at::where(original_self_p == other_p, at::scalar_tensor(0.5, self_p.options()), (original_self_p > other_p).to(self_p.scalar_type())) * (original_self_t - other_t)) : other_t + at::where(original_self_p == other_p, at::scalar_tensor(0.5, self_p.options()), (original_self_p > other_p).to(self_p.scalar_type())) * (original_self_t - other_t);
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_maximum__ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<NotImplemented>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<NotImplemented> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("_foreach_maximum_"), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_maximum_(ks & c10::after_autograd_keyset, self_, scalars);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self))), "Trying to use forward AD with _foreach_maximum_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
}
void _foreach_sigmoid_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<SigmoidBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<SigmoidBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<SigmoidBackward0>(new SigmoidBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sigmoid_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((sigmoid_backward(self_t.conj(), self_p)).conj()) : (sigmoid_backward(self_t.conj(), self_p)).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
  if (!grad_fns.empty()) {
  
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              grad_fn->result_ = SavedVariable(self[i], true, self[i].is_view());
          }
      }
  }
}
::std::vector<at::Tensor> _foreach_sign(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachSignBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachSignBackward0>(new ForeachSignBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_sign(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = (zeros_like(self_t.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_sign_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<SignBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<SignBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<SignBackward0>(new SignBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sign_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((zeros_like(self_t.conj())).conj()) : (zeros_like(self_t.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_sin_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sin_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_sin_out that does not support it because it is an out= function");
}
::std::vector<at::Tensor> _foreach_sinh(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachSinhBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachSinhBackward0>(new ForeachSinhBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_sinh(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (self_t.conj() * self_p.cosh().conj()).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_sqrt_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<SqrtBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<SqrtBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<SqrtBackward0>(new SqrtBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sqrt_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((self_t.conj() / (2 * self_p.conj())).conj()) : (self_t.conj() / (2 * self_p.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
  if (!grad_fns.empty()) {
  
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              grad_fn->result_ = SavedVariable(self[i], true, self[i].is_view());
          }
      }
  }
}
void _foreach_sub__Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<SubBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<SubBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<SubBackward1>(new SubBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sub_(ks & c10::after_autograd_keyset, self_, scalar);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((handle_r_to_c(original_self_p.scalar_type(), original_self_t.conj())).conj()) : (handle_r_to_c(original_self_p.scalar_type(), original_self_t.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_sub__List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<SubBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<SubBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<SubBackward0>(new SubBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->alpha = alpha;
                grad_fn->other_scalar_type = other[i].scalar_type();
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sub_(ks & c10::after_autograd_keyset, self_, other_, alpha);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(self_t - maybe_multiply(other_t, alpha)) : self_t - maybe_multiply(other_t, alpha);
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_sub__ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<SubBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<SubBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<SubBackward1>(new SubBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sub_(ks & c10::after_autograd_keyset, self_, scalars);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((handle_r_to_c(original_self_p.scalar_type(), original_self_t.conj())).conj()) : (handle_r_to_c(original_self_p.scalar_type(), original_self_t.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
::std::vector<at::Tensor> _foreach_tan(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachTanBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachTanBackward0>(new ForeachTanBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_tan(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = (self_t.conj() * (1 + result[i].pow(2)).conj()).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  if (grad_fn) {
    grad_fn->result_ = make_saved_variable_list(result, true);
  }
  return result;
}
void _foreach_tan_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_tan_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_tan_out that does not support it because it is an out= function");
}
void _foreach_zero_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_zero_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_zero_out that does not support it because it is an out= function");
}
void _fused_adamw_(c10::DispatchKeySet ks, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
  auto self_ = unpack(self, "self", 0);
  auto grads_ = unpack(grads, "grads", 1);
  auto exp_avgs_ = unpack(exp_avgs, "exp_avgs", 2);
  auto exp_avg_sqs_ = unpack(exp_avg_sqs, "exp_avg_sqs", 3);
  auto max_exp_avg_sqs_ = unpack(max_exp_avg_sqs, "max_exp_avg_sqs", 4);
  auto state_steps_ = unpack(state_steps, "state_steps", 5);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> grads__storage_saved(grads_.size());
  for (const Tensor& tensor : grads_)
    grads__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> grads__impl_saved(grads_.size());
  for (size_t i=0; i<grads_.size(); i++)
    if (grads_[i].defined()) grads__impl_saved[i] = grads_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> exp_avgs__storage_saved(exp_avgs_.size());
  for (const Tensor& tensor : exp_avgs_)
    exp_avgs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> exp_avgs__impl_saved(exp_avgs_.size());
  for (size_t i=0; i<exp_avgs_.size(); i++)
    if (exp_avgs_[i].defined()) exp_avgs__impl_saved[i] = exp_avgs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> exp_avg_sqs__storage_saved(exp_avg_sqs_.size());
  for (const Tensor& tensor : exp_avg_sqs_)
    exp_avg_sqs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> exp_avg_sqs__impl_saved(exp_avg_sqs_.size());
  for (size_t i=0; i<exp_avg_sqs_.size(); i++)
    if (exp_avg_sqs_[i].defined()) exp_avg_sqs__impl_saved[i] = exp_avg_sqs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> max_exp_avg_sqs__storage_saved(max_exp_avg_sqs_.size());
  for (const Tensor& tensor : max_exp_avg_sqs_)
    max_exp_avg_sqs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> max_exp_avg_sqs__impl_saved(max_exp_avg_sqs_.size());
  for (size_t i=0; i<max_exp_avg_sqs_.size(); i++)
    if (max_exp_avg_sqs_[i].defined()) max_exp_avg_sqs__impl_saved[i] = max_exp_avg_sqs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> state_steps__storage_saved(state_steps_.size());
  for (const Tensor& tensor : state_steps_)
    state_steps__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> state_steps__impl_saved(state_steps_.size());
  for (size_t i=0; i<state_steps_.size(); i++)
    if (state_steps_[i].defined()) state_steps__impl_saved[i] = state_steps_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_fused_adamw_(ks & c10::after_autograd_keyset, self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<grads_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (grads__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(grads_))
      TORCH_INTERNAL_ASSERT(grads__storage_saved[i].value().is_alias_of(grads_[i].storage()));
  }
  for (size_t i=0; i<grads_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (grads__impl_saved[i] && !at::impl::tensorlist_has_dispatch(grads_))
      TORCH_INTERNAL_ASSERT(grads__impl_saved[i] == grads_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<exp_avgs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avgs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(exp_avgs_))
      TORCH_INTERNAL_ASSERT(exp_avgs__storage_saved[i].value().is_alias_of(exp_avgs_[i].storage()));
  }
  for (size_t i=0; i<exp_avgs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avgs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(exp_avgs_))
      TORCH_INTERNAL_ASSERT(exp_avgs__impl_saved[i] == exp_avgs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avg_sqs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(exp_avg_sqs__storage_saved[i].value().is_alias_of(exp_avg_sqs_[i].storage()));
  }
  for (size_t i=0; i<exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avg_sqs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(exp_avg_sqs__impl_saved[i] == exp_avg_sqs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<max_exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (max_exp_avg_sqs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(max_exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(max_exp_avg_sqs__storage_saved[i].value().is_alias_of(max_exp_avg_sqs_[i].storage()));
  }
  for (size_t i=0; i<max_exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (max_exp_avg_sqs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(max_exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(max_exp_avg_sqs__impl_saved[i] == max_exp_avg_sqs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<state_steps_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (state_steps__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(state_steps_))
      TORCH_INTERNAL_ASSERT(state_steps__storage_saved[i].value().is_alias_of(state_steps_[i].storage()));
  }
  for (size_t i=0; i<state_steps_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (state_steps__impl_saved[i] && !at::impl::tensorlist_has_dispatch(state_steps_))
      TORCH_INTERNAL_ASSERT(state_steps__impl_saved[i] == state_steps_[i].getIntrusivePtr());
  }
  #endif
}
void _fused_adamw__tensor_lr(c10::DispatchKeySet ks, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
  auto self_ = unpack(self, "self", 0);
  auto grads_ = unpack(grads, "grads", 1);
  auto exp_avgs_ = unpack(exp_avgs, "exp_avgs", 2);
  auto exp_avg_sqs_ = unpack(exp_avg_sqs, "exp_avg_sqs", 3);
  auto max_exp_avg_sqs_ = unpack(max_exp_avg_sqs, "max_exp_avg_sqs", 4);
  auto state_steps_ = unpack(state_steps, "state_steps", 5);
  auto& lr_ = unpack(lr, "lr", 6);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> grads__storage_saved(grads_.size());
  for (const Tensor& tensor : grads_)
    grads__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> grads__impl_saved(grads_.size());
  for (size_t i=0; i<grads_.size(); i++)
    if (grads_[i].defined()) grads__impl_saved[i] = grads_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> exp_avgs__storage_saved(exp_avgs_.size());
  for (const Tensor& tensor : exp_avgs_)
    exp_avgs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> exp_avgs__impl_saved(exp_avgs_.size());
  for (size_t i=0; i<exp_avgs_.size(); i++)
    if (exp_avgs_[i].defined()) exp_avgs__impl_saved[i] = exp_avgs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> exp_avg_sqs__storage_saved(exp_avg_sqs_.size());
  for (const Tensor& tensor : exp_avg_sqs_)
    exp_avg_sqs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> exp_avg_sqs__impl_saved(exp_avg_sqs_.size());
  for (size_t i=0; i<exp_avg_sqs_.size(); i++)
    if (exp_avg_sqs_[i].defined()) exp_avg_sqs__impl_saved[i] = exp_avg_sqs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> max_exp_avg_sqs__storage_saved(max_exp_avg_sqs_.size());
  for (const Tensor& tensor : max_exp_avg_sqs_)
    max_exp_avg_sqs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> max_exp_avg_sqs__impl_saved(max_exp_avg_sqs_.size());
  for (size_t i=0; i<max_exp_avg_sqs_.size(); i++)
    if (max_exp_avg_sqs_[i].defined()) max_exp_avg_sqs__impl_saved[i] = max_exp_avg_sqs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> state_steps__storage_saved(state_steps_.size());
  for (const Tensor& tensor : state_steps_)
    state_steps__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> state_steps__impl_saved(state_steps_.size());
  for (size_t i=0; i<state_steps_.size(); i++)
    if (state_steps_[i].defined()) state_steps__impl_saved[i] = state_steps_[i].getIntrusivePtr();
  c10::optional<Storage> lr__storage_saved =
    lr_.has_storage() ? c10::optional<Storage>(lr_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> lr__impl_saved;
  if (lr_.defined()) lr__impl_saved = lr_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_fused_adamw_(ks & c10::after_autograd_keyset, self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr_, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<grads_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (grads__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(grads_))
      TORCH_INTERNAL_ASSERT(grads__storage_saved[i].value().is_alias_of(grads_[i].storage()));
  }
  for (size_t i=0; i<grads_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (grads__impl_saved[i] && !at::impl::tensorlist_has_dispatch(grads_))
      TORCH_INTERNAL_ASSERT(grads__impl_saved[i] == grads_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<exp_avgs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avgs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(exp_avgs_))
      TORCH_INTERNAL_ASSERT(exp_avgs__storage_saved[i].value().is_alias_of(exp_avgs_[i].storage()));
  }
  for (size_t i=0; i<exp_avgs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avgs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(exp_avgs_))
      TORCH_INTERNAL_ASSERT(exp_avgs__impl_saved[i] == exp_avgs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avg_sqs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(exp_avg_sqs__storage_saved[i].value().is_alias_of(exp_avg_sqs_[i].storage()));
  }
  for (size_t i=0; i<exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avg_sqs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(exp_avg_sqs__impl_saved[i] == exp_avg_sqs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<max_exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (max_exp_avg_sqs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(max_exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(max_exp_avg_sqs__storage_saved[i].value().is_alias_of(max_exp_avg_sqs_[i].storage()));
  }
  for (size_t i=0; i<max_exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (max_exp_avg_sqs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(max_exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(max_exp_avg_sqs__impl_saved[i] == max_exp_avg_sqs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<state_steps_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (state_steps__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(state_steps_))
      TORCH_INTERNAL_ASSERT(state_steps__storage_saved[i].value().is_alias_of(state_steps_[i].storage()));
  }
  for (size_t i=0; i<state_steps_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (state_steps__impl_saved[i] && !at::impl::tensorlist_has_dispatch(state_steps_))
      TORCH_INTERNAL_ASSERT(state_steps__impl_saved[i] == state_steps_[i].getIntrusivePtr());
  }
  if (lr__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(lr_))
    TORCH_INTERNAL_ASSERT(lr__storage_saved.value().is_alias_of(lr_.storage()));
  if (lr__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(lr_))
    TORCH_INTERNAL_ASSERT(lr__impl_saved == lr_.getIntrusivePtr());
  #endif
}
void _fused_adamw_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto grads_ = unpack(grads, "grads", 1);
  auto exp_avgs_ = unpack(exp_avgs, "exp_avgs", 2);
  auto exp_avg_sqs_ = unpack(exp_avg_sqs, "exp_avg_sqs", 3);
  auto max_exp_avg_sqs_ = unpack(max_exp_avg_sqs, "max_exp_avg_sqs", 4);
  auto state_steps_ = unpack(state_steps, "state_steps", 5);
  auto out_ = unpack(out, "out", 15);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> grads__storage_saved(grads_.size());
  for (const Tensor& tensor : grads_)
    grads__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> grads__impl_saved(grads_.size());
  for (size_t i=0; i<grads_.size(); i++)
    if (grads_[i].defined()) grads__impl_saved[i] = grads_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> exp_avgs__storage_saved(exp_avgs_.size());
  for (const Tensor& tensor : exp_avgs_)
    exp_avgs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> exp_avgs__impl_saved(exp_avgs_.size());
  for (size_t i=0; i<exp_avgs_.size(); i++)
    if (exp_avgs_[i].defined()) exp_avgs__impl_saved[i] = exp_avgs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> exp_avg_sqs__storage_saved(exp_avg_sqs_.size());
  for (const Tensor& tensor : exp_avg_sqs_)
    exp_avg_sqs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> exp_avg_sqs__impl_saved(exp_avg_sqs_.size());
  for (size_t i=0; i<exp_avg_sqs_.size(); i++)
    if (exp_avg_sqs_[i].defined()) exp_avg_sqs__impl_saved[i] = exp_avg_sqs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> max_exp_avg_sqs__storage_saved(max_exp_avg_sqs_.size());
  for (const Tensor& tensor : max_exp_avg_sqs_)
    max_exp_avg_sqs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> max_exp_avg_sqs__impl_saved(max_exp_avg_sqs_.size());
  for (size_t i=0; i<max_exp_avg_sqs_.size(); i++)
    if (max_exp_avg_sqs_[i].defined()) max_exp_avg_sqs__impl_saved[i] = max_exp_avg_sqs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> state_steps__storage_saved(state_steps_.size());
  for (const Tensor& tensor : state_steps_)
    state_steps__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> state_steps__impl_saved(state_steps_.size());
  for (size_t i=0; i<state_steps_.size(); i++)
    if (state_steps_[i].defined()) state_steps__impl_saved[i] = state_steps_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_fused_adamw_outf(ks & c10::after_autograd_keyset, self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<grads_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (grads__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(grads_))
      TORCH_INTERNAL_ASSERT(grads__storage_saved[i].value().is_alias_of(grads_[i].storage()));
  }
  for (size_t i=0; i<grads_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (grads__impl_saved[i] && !at::impl::tensorlist_has_dispatch(grads_))
      TORCH_INTERNAL_ASSERT(grads__impl_saved[i] == grads_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<exp_avgs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avgs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(exp_avgs_))
      TORCH_INTERNAL_ASSERT(exp_avgs__storage_saved[i].value().is_alias_of(exp_avgs_[i].storage()));
  }
  for (size_t i=0; i<exp_avgs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avgs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(exp_avgs_))
      TORCH_INTERNAL_ASSERT(exp_avgs__impl_saved[i] == exp_avgs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avg_sqs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(exp_avg_sqs__storage_saved[i].value().is_alias_of(exp_avg_sqs_[i].storage()));
  }
  for (size_t i=0; i<exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avg_sqs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(exp_avg_sqs__impl_saved[i] == exp_avg_sqs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<max_exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (max_exp_avg_sqs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(max_exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(max_exp_avg_sqs__storage_saved[i].value().is_alias_of(max_exp_avg_sqs_[i].storage()));
  }
  for (size_t i=0; i<max_exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (max_exp_avg_sqs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(max_exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(max_exp_avg_sqs__impl_saved[i] == max_exp_avg_sqs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<state_steps_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (state_steps__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(state_steps_))
      TORCH_INTERNAL_ASSERT(state_steps__storage_saved[i].value().is_alias_of(state_steps_[i].storage()));
  }
  for (size_t i=0; i<state_steps_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (state_steps__impl_saved[i] && !at::impl::tensorlist_has_dispatch(state_steps_))
      TORCH_INTERNAL_ASSERT(state_steps__impl_saved[i] == state_steps_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(grads) || isFwGradDefinedTensorList(exp_avgs) || isFwGradDefinedTensorList(exp_avg_sqs) || isFwGradDefinedTensorList(max_exp_avg_sqs) || isFwGradDefinedTensorList(state_steps) || isFwGradDefined(grad_scale) || isFwGradDefined(found_inf) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _fused_adamw_out that does not support it because it is an out= function");
}
void _fused_adamw_out_tensor_lr_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto grads_ = unpack(grads, "grads", 1);
  auto exp_avgs_ = unpack(exp_avgs, "exp_avgs", 2);
  auto exp_avg_sqs_ = unpack(exp_avg_sqs, "exp_avg_sqs", 3);
  auto max_exp_avg_sqs_ = unpack(max_exp_avg_sqs, "max_exp_avg_sqs", 4);
  auto state_steps_ = unpack(state_steps, "state_steps", 5);
  auto& lr_ = unpack(lr, "lr", 6);
  auto out_ = unpack(out, "out", 15);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> grads__storage_saved(grads_.size());
  for (const Tensor& tensor : grads_)
    grads__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> grads__impl_saved(grads_.size());
  for (size_t i=0; i<grads_.size(); i++)
    if (grads_[i].defined()) grads__impl_saved[i] = grads_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> exp_avgs__storage_saved(exp_avgs_.size());
  for (const Tensor& tensor : exp_avgs_)
    exp_avgs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> exp_avgs__impl_saved(exp_avgs_.size());
  for (size_t i=0; i<exp_avgs_.size(); i++)
    if (exp_avgs_[i].defined()) exp_avgs__impl_saved[i] = exp_avgs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> exp_avg_sqs__storage_saved(exp_avg_sqs_.size());
  for (const Tensor& tensor : exp_avg_sqs_)
    exp_avg_sqs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> exp_avg_sqs__impl_saved(exp_avg_sqs_.size());
  for (size_t i=0; i<exp_avg_sqs_.size(); i++)
    if (exp_avg_sqs_[i].defined()) exp_avg_sqs__impl_saved[i] = exp_avg_sqs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> max_exp_avg_sqs__storage_saved(max_exp_avg_sqs_.size());
  for (const Tensor& tensor : max_exp_avg_sqs_)
    max_exp_avg_sqs__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> max_exp_avg_sqs__impl_saved(max_exp_avg_sqs_.size());
  for (size_t i=0; i<max_exp_avg_sqs_.size(); i++)
    if (max_exp_avg_sqs_[i].defined()) max_exp_avg_sqs__impl_saved[i] = max_exp_avg_sqs_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> state_steps__storage_saved(state_steps_.size());
  for (const Tensor& tensor : state_steps_)
    state_steps__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> state_steps__impl_saved(state_steps_.size());
  for (size_t i=0; i<state_steps_.size(); i++)
    if (state_steps_[i].defined()) state_steps__impl_saved[i] = state_steps_[i].getIntrusivePtr();
  c10::optional<Storage> lr__storage_saved =
    lr_.has_storage() ? c10::optional<Storage>(lr_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> lr__impl_saved;
  if (lr_.defined()) lr__impl_saved = lr_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_fused_adamw_outf(ks & c10::after_autograd_keyset, self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr_, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<grads_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (grads__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(grads_))
      TORCH_INTERNAL_ASSERT(grads__storage_saved[i].value().is_alias_of(grads_[i].storage()));
  }
  for (size_t i=0; i<grads_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (grads__impl_saved[i] && !at::impl::tensorlist_has_dispatch(grads_))
      TORCH_INTERNAL_ASSERT(grads__impl_saved[i] == grads_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<exp_avgs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avgs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(exp_avgs_))
      TORCH_INTERNAL_ASSERT(exp_avgs__storage_saved[i].value().is_alias_of(exp_avgs_[i].storage()));
  }
  for (size_t i=0; i<exp_avgs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avgs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(exp_avgs_))
      TORCH_INTERNAL_ASSERT(exp_avgs__impl_saved[i] == exp_avgs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avg_sqs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(exp_avg_sqs__storage_saved[i].value().is_alias_of(exp_avg_sqs_[i].storage()));
  }
  for (size_t i=0; i<exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (exp_avg_sqs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(exp_avg_sqs__impl_saved[i] == exp_avg_sqs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<max_exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (max_exp_avg_sqs__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(max_exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(max_exp_avg_sqs__storage_saved[i].value().is_alias_of(max_exp_avg_sqs_[i].storage()));
  }
  for (size_t i=0; i<max_exp_avg_sqs_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (max_exp_avg_sqs__impl_saved[i] && !at::impl::tensorlist_has_dispatch(max_exp_avg_sqs_))
      TORCH_INTERNAL_ASSERT(max_exp_avg_sqs__impl_saved[i] == max_exp_avg_sqs_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<state_steps_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (state_steps__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(state_steps_))
      TORCH_INTERNAL_ASSERT(state_steps__storage_saved[i].value().is_alias_of(state_steps_[i].storage()));
  }
  for (size_t i=0; i<state_steps_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (state_steps__impl_saved[i] && !at::impl::tensorlist_has_dispatch(state_steps_))
      TORCH_INTERNAL_ASSERT(state_steps__impl_saved[i] == state_steps_[i].getIntrusivePtr());
  }
  if (lr__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(lr_))
    TORCH_INTERNAL_ASSERT(lr__storage_saved.value().is_alias_of(lr_.storage()));
  if (lr__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(lr_))
    TORCH_INTERNAL_ASSERT(lr__impl_saved == lr_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(grads) || isFwGradDefinedTensorList(exp_avgs) || isFwGradDefinedTensorList(exp_avg_sqs) || isFwGradDefinedTensorList(max_exp_avg_sqs) || isFwGradDefinedTensorList(state_steps) || isFwGradDefined(lr) || isFwGradDefined(grad_scale) || isFwGradDefined(found_inf) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _fused_adamw_out that does not support it because it is an out= function");
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _linalg_det_out_result(c10::DispatchKeySet ks, const at::Tensor & A, at::Tensor & result, at::Tensor & LU, at::Tensor & pivots) {
  auto& A_ = unpack(A, "A", 0);
  auto& result_ = unpack(result, "result", 1);
  auto& LU_ = unpack(LU, "LU", 2);
  auto& pivots_ = unpack(pivots, "pivots", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(A));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( A )) {
    throw_error_out_requires_grad("_linalg_det");
  }
  if (compute_requires_grad( result )) {
    throw_error_out_requires_grad("_linalg_det");
  }
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  c10::optional<Storage> result__storage_saved =
    result_.has_storage() ? c10::optional<Storage>(result_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> result__impl_saved;
  if (result_.defined()) result__impl_saved = result_.getIntrusivePtr();
  c10::optional<Storage> LU__storage_saved =
    LU_.has_storage() ? c10::optional<Storage>(LU_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> LU__impl_saved;
  if (LU_.defined()) LU__impl_saved = LU_.getIntrusivePtr();
  c10::optional<Storage> pivots__storage_saved =
    pivots_.has_storage() ? c10::optional<Storage>(pivots_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> pivots__impl_saved;
  if (pivots_.defined()) pivots__impl_saved = pivots_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_linalg_det_outf(ks & c10::after_autograd_keyset, A_, result_, LU_, pivots_);
  }
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (result__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(result_))
    TORCH_INTERNAL_ASSERT(result__storage_saved.value().is_alias_of(result_.storage()));
  if (result__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result_))
    TORCH_INTERNAL_ASSERT(result__impl_saved == result_.getIntrusivePtr());
  if (LU__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(LU_))
    TORCH_INTERNAL_ASSERT(LU__storage_saved.value().is_alias_of(LU_.storage()));
  if (LU__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU_))
    TORCH_INTERNAL_ASSERT(LU__impl_saved == LU_.getIntrusivePtr());
  if (pivots__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(pivots_))
    TORCH_INTERNAL_ASSERT(pivots__storage_saved.value().is_alias_of(pivots_.storage()));
  if (pivots__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots_))
    TORCH_INTERNAL_ASSERT(pivots__impl_saved == pivots_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( result ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(A) || isFwGradDefined(result) || isFwGradDefined(LU) || isFwGradDefined(pivots))), "Trying to use forward AD with _linalg_det_out that does not support it because it is an out= function");
  return std::forward_as_tuple(result, LU, pivots);
}
::std::tuple<at::Tensor &,at::Tensor &> _linalg_eigh_out_eigenvalues(c10::DispatchKeySet ks, const at::Tensor & A, c10::string_view UPLO, bool compute_v, at::Tensor & eigenvalues, at::Tensor & eigenvectors) {
  auto& A_ = unpack(A, "A", 0);
  auto& eigenvalues_ = unpack(eigenvalues, "eigenvalues", 3);
  auto& eigenvectors_ = unpack(eigenvectors, "eigenvectors", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_eigenvalues_eigenvectors = (isFwGradDefined(A));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( A )) {
    throw_error_out_requires_grad("_linalg_eigh");
  }
  if (compute_requires_grad( eigenvalues, eigenvectors )) {
    throw_error_out_requires_grad("_linalg_eigh");
  }
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  c10::optional<Storage> eigenvalues__storage_saved =
    eigenvalues_.has_storage() ? c10::optional<Storage>(eigenvalues_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> eigenvalues__impl_saved;
  if (eigenvalues_.defined()) eigenvalues__impl_saved = eigenvalues_.getIntrusivePtr();
  c10::optional<Storage> eigenvectors__storage_saved =
    eigenvectors_.has_storage() ? c10::optional<Storage>(eigenvectors_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> eigenvectors__impl_saved;
  if (eigenvectors_.defined()) eigenvectors__impl_saved = eigenvectors_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_linalg_eigh_outf(ks & c10::after_autograd_keyset, A_, UPLO, compute_v, eigenvalues_, eigenvectors_);
  }
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (eigenvalues__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(eigenvalues_))
    TORCH_INTERNAL_ASSERT(eigenvalues__storage_saved.value().is_alias_of(eigenvalues_.storage()));
  if (eigenvalues__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(eigenvalues_))
    TORCH_INTERNAL_ASSERT(eigenvalues__impl_saved == eigenvalues_.getIntrusivePtr());
  if (eigenvectors__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(eigenvectors_))
    TORCH_INTERNAL_ASSERT(eigenvectors__storage_saved.value().is_alias_of(eigenvectors_.storage()));
  if (eigenvectors__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(eigenvectors_))
    TORCH_INTERNAL_ASSERT(eigenvectors__impl_saved == eigenvectors_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( eigenvalues, eigenvectors ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(A) || isFwGradDefined(eigenvalues) || isFwGradDefined(eigenvectors))), "Trying to use forward AD with _linalg_eigh_out that does not support it because it is an out= function");
  return std::forward_as_tuple(eigenvalues, eigenvectors);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _linalg_slogdet(c10::DispatchKeySet ks, const at::Tensor & A) {
  auto& A_ = unpack(A, "A", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_sign_logabsdet = (isFwGradDefined(A));
  std::shared_ptr<LinalgSlogdetBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgSlogdetBackward0>(new LinalgSlogdetBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( A ));
    grad_fn->A_ = SavedVariable(A, false);
  }
  at::Tensor sign;
  at::Tensor logabsdet;
  at::Tensor LU;
  at::Tensor pivots;
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_linalg_slogdet(ks & c10::after_autograd_keyset, A_);
  })();
  std::tie(sign, logabsdet, LU, pivots) = std::move(_tmp);
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (sign.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(sign)) {
    TORCH_INTERNAL_ASSERT(sign.storage().use_count() == 1, "function: _linalg_slogdet");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(sign))
    TORCH_INTERNAL_ASSERT(sign.use_count() <= 1, "function: _linalg_slogdet");
  if (logabsdet.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(logabsdet)) {
    TORCH_INTERNAL_ASSERT(logabsdet.storage().use_count() == 1, "function: _linalg_slogdet");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(logabsdet))
    TORCH_INTERNAL_ASSERT(logabsdet.use_count() <= 1, "function: _linalg_slogdet");
  if (LU.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU)) {
    TORCH_INTERNAL_ASSERT(LU.storage().use_count() == 1, "function: _linalg_slogdet");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU))
    TORCH_INTERNAL_ASSERT(LU.use_count() <= 1, "function: _linalg_slogdet");
  if (pivots.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots)) {
    TORCH_INTERNAL_ASSERT(pivots.storage().use_count() == 1, "function: _linalg_slogdet");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots))
    TORCH_INTERNAL_ASSERT(pivots.use_count() <= 1, "function: _linalg_slogdet");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( sign, logabsdet ), grad_fn);
  }
  c10::optional<::std::tuple<at::Tensor,at::Tensor>> sign_logabsdet_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_sign_logabsdet) {
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      auto A_p = toNonOptPrimal(A);
      sign_logabsdet_new_fw_grad_opt = slogdet_jvp(LU, pivots, A_t, sign, A_p.is_contiguous() && !A_p.is_complex());
  }
  if (sign_logabsdet_new_fw_grad_opt.has_value() && std::get<0>(sign_logabsdet_new_fw_grad_opt.value()).defined()
      && sign.defined()) {
    sign._set_fw_grad(std::get<0>(sign_logabsdet_new_fw_grad_opt.value()), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (sign_logabsdet_new_fw_grad_opt.has_value() && std::get<1>(sign_logabsdet_new_fw_grad_opt.value()).defined()
      && logabsdet.defined()) {
    logabsdet._set_fw_grad(std::get<1>(sign_logabsdet_new_fw_grad_opt.value()), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->LU_ = SavedVariable(LU, true);
    grad_fn->pivots_ = SavedVariable(pivots, true);
    grad_fn->sign_ = SavedVariable(sign, true);
  }
  return std::make_tuple(std::move(sign), std::move(logabsdet), std::move(LU), std::move(pivots));
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _linalg_solve_ex(c10::DispatchKeySet ks, const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors) {
  auto& A_ = unpack(A, "A", 0);
  auto& B_ = unpack(B, "B", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A, B );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(A) || isFwGradDefined(B));
  std::shared_ptr<LinalgSolveExBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgSolveExBackward0>(new LinalgSolveExBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( A, B ));
    grad_fn->A_ = SavedVariable(A, false);
    grad_fn->left = left;
  }
  at::Tensor result;
  at::Tensor LU;
  at::Tensor pivots;
  at::Tensor info;
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  c10::optional<Storage> B__storage_saved =
    B_.has_storage() ? c10::optional<Storage>(B_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> B__impl_saved;
  if (B_.defined()) B__impl_saved = B_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_linalg_solve_ex(ks & c10::after_autograd_keyset, A_, B_, left, check_errors);
  })();
  std::tie(result, LU, pivots, info) = std::move(_tmp);
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (B__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__storage_saved.value().is_alias_of(B_.storage()));
  if (B__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__impl_saved == B_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _linalg_solve_ex");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _linalg_solve_ex");
  if (LU.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU)) {
    TORCH_INTERNAL_ASSERT(LU.storage().use_count() == 1, "function: _linalg_solve_ex");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU))
    TORCH_INTERNAL_ASSERT(LU.use_count() <= 1, "function: _linalg_solve_ex");
  if (pivots.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots)) {
    TORCH_INTERNAL_ASSERT(pivots.storage().use_count() == 1, "function: _linalg_solve_ex");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots))
    TORCH_INTERNAL_ASSERT(pivots.use_count() <= 1, "function: _linalg_solve_ex");
  if (info.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(info)) {
    TORCH_INTERNAL_ASSERT(info.storage().use_count() == 1, "function: _linalg_solve_ex");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(info))
    TORCH_INTERNAL_ASSERT(info.use_count() <= 1, "function: _linalg_solve_ex");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      auto A_p = toNonOptPrimal(A);
      auto B_t_raw = toNonOptFwGrad(B);
      auto B_tensor = toNonOptTensor(B);
      auto B_t = (B_t_raw.defined() || !B_tensor.defined())
        ? B_t_raw : at::_efficientzerotensor(B_tensor.sizes(), B_tensor.options());
      result_new_fw_grad_opt = linalg_solve_jvp(A_t, B_t, result, LU, pivots, left, A_p.is_contiguous() && !A_p.is_complex());
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->LU_ = SavedVariable(LU, true);
    grad_fn->pivots_ = SavedVariable(pivots, true);
    grad_fn->result_ = SavedVariable(result, true);
  }
  return std::make_tuple(std::move(result), std::move(LU), std::move(pivots), std::move(info));
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _linalg_solve_ex_out_result(c10::DispatchKeySet ks, const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors, at::Tensor & result, at::Tensor & LU, at::Tensor & pivots, at::Tensor & info) {
  auto& A_ = unpack(A, "A", 0);
  auto& B_ = unpack(B, "B", 1);
  auto& result_ = unpack(result, "result", 4);
  auto& LU_ = unpack(LU, "LU", 5);
  auto& pivots_ = unpack(pivots, "pivots", 6);
  auto& info_ = unpack(info, "info", 7);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A, B );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(A) || isFwGradDefined(B));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( A, B )) {
    throw_error_out_requires_grad("_linalg_solve_ex");
  }
  if (compute_requires_grad( result )) {
    throw_error_out_requires_grad("_linalg_solve_ex");
  }
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  c10::optional<Storage> B__storage_saved =
    B_.has_storage() ? c10::optional<Storage>(B_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> B__impl_saved;
  if (B_.defined()) B__impl_saved = B_.getIntrusivePtr();
  c10::optional<Storage> result__storage_saved =
    result_.has_storage() ? c10::optional<Storage>(result_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> result__impl_saved;
  if (result_.defined()) result__impl_saved = result_.getIntrusivePtr();
  c10::optional<Storage> LU__storage_saved =
    LU_.has_storage() ? c10::optional<Storage>(LU_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> LU__impl_saved;
  if (LU_.defined()) LU__impl_saved = LU_.getIntrusivePtr();
  c10::optional<Storage> pivots__storage_saved =
    pivots_.has_storage() ? c10::optional<Storage>(pivots_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> pivots__impl_saved;
  if (pivots_.defined()) pivots__impl_saved = pivots_.getIntrusivePtr();
  c10::optional<Storage> info__storage_saved =
    info_.has_storage() ? c10::optional<Storage>(info_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> info__impl_saved;
  if (info_.defined()) info__impl_saved = info_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_linalg_solve_ex_outf(ks & c10::after_autograd_keyset, A_, B_, left, check_errors, result_, LU_, pivots_, info_);
  }
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (B__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__storage_saved.value().is_alias_of(B_.storage()));
  if (B__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__impl_saved == B_.getIntrusivePtr());
  if (result__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(result_))
    TORCH_INTERNAL_ASSERT(result__storage_saved.value().is_alias_of(result_.storage()));
  if (result__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result_))
    TORCH_INTERNAL_ASSERT(result__impl_saved == result_.getIntrusivePtr());
  if (LU__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(LU_))
    TORCH_INTERNAL_ASSERT(LU__storage_saved.value().is_alias_of(LU_.storage()));
  if (LU__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU_))
    TORCH_INTERNAL_ASSERT(LU__impl_saved == LU_.getIntrusivePtr());
  if (pivots__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(pivots_))
    TORCH_INTERNAL_ASSERT(pivots__storage_saved.value().is_alias_of(pivots_.storage()));
  if (pivots__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots_))
    TORCH_INTERNAL_ASSERT(pivots__impl_saved == pivots_.getIntrusivePtr());
  if (info__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(info_))
    TORCH_INTERNAL_ASSERT(info__storage_saved.value().is_alias_of(info_.storage()));
  if (info__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(info_))
    TORCH_INTERNAL_ASSERT(info__impl_saved == info_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( result ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(A) || isFwGradDefined(B) || isFwGradDefined(result) || isFwGradDefined(LU) || isFwGradDefined(pivots) || isFwGradDefined(info))), "Trying to use forward AD with _linalg_solve_ex_out that does not support it because it is an out= function");
  return std::forward_as_tuple(result, LU, pivots, info);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _linalg_svd(c10::DispatchKeySet ks, const at::Tensor & A, bool full_matrices, bool compute_uv, c10::optional<c10::string_view> driver) {
  auto& A_ = unpack(A, "A", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_U_S_Vh = (isFwGradDefined(A));
  std::shared_ptr<LinalgSvdBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgSvdBackward0>(new LinalgSvdBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( A ));
    grad_fn->full_matrices = full_matrices;
  }
  at::Tensor U;
  at::Tensor S;
  at::Tensor Vh;
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_linalg_svd(ks & c10::after_autograd_keyset, A_, full_matrices, compute_uv, driver);
  })();
  std::tie(U, S, Vh) = std::move(_tmp);
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (U.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(U)) {
    TORCH_INTERNAL_ASSERT(U.storage().use_count() == 1, "function: _linalg_svd");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(U))
    TORCH_INTERNAL_ASSERT(U.use_count() <= 1, "function: _linalg_svd");
  if (S.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(S)) {
    TORCH_INTERNAL_ASSERT(S.storage().use_count() == 1, "function: _linalg_svd");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(S))
    TORCH_INTERNAL_ASSERT(S.use_count() <= 1, "function: _linalg_svd");
  if (Vh.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(Vh)) {
    TORCH_INTERNAL_ASSERT(Vh.storage().use_count() == 1, "function: _linalg_svd");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(Vh))
    TORCH_INTERNAL_ASSERT(Vh.use_count() <= 1, "function: _linalg_svd");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( U, S, Vh ), grad_fn);
  }
  c10::optional<::std::tuple<at::Tensor,at::Tensor,at::Tensor>> U_S_Vh_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_U_S_Vh) {
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      U_S_Vh_new_fw_grad_opt = linalg_svd_jvp(A_t, U, S, Vh, full_matrices);
  }
  if (U_S_Vh_new_fw_grad_opt.has_value() && std::get<0>(U_S_Vh_new_fw_grad_opt.value()).defined()
      && U.defined()) {
    U._set_fw_grad(std::get<0>(U_S_Vh_new_fw_grad_opt.value()), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (U_S_Vh_new_fw_grad_opt.has_value() && std::get<1>(U_S_Vh_new_fw_grad_opt.value()).defined()
      && S.defined()) {
    S._set_fw_grad(std::get<1>(U_S_Vh_new_fw_grad_opt.value()), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (U_S_Vh_new_fw_grad_opt.has_value() && std::get<2>(U_S_Vh_new_fw_grad_opt.value()).defined()
      && Vh.defined()) {
    Vh._set_fw_grad(std::get<2>(U_S_Vh_new_fw_grad_opt.value()), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->S_ = SavedVariable(S, true);
    grad_fn->S_sym_argsize_minus_1 = S.sym_size(-1);
    grad_fn->U_ = SavedVariable(U, true);
    grad_fn->Vh_ = SavedVariable(Vh, true);
  }
  return std::make_tuple(std::move(U), std::move(S), std::move(Vh));
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _linalg_svd_out_U(c10::DispatchKeySet ks, const at::Tensor & A, bool full_matrices, bool compute_uv, c10::optional<c10::string_view> driver, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
  auto& A_ = unpack(A, "A", 0);
  auto& U_ = unpack(U, "U", 4);
  auto& S_ = unpack(S, "S", 5);
  auto& Vh_ = unpack(Vh, "Vh", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_U_S_Vh = (isFwGradDefined(A));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( A )) {
    throw_error_out_requires_grad("_linalg_svd");
  }
  if (compute_requires_grad( U, S, Vh )) {
    throw_error_out_requires_grad("_linalg_svd");
  }
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  c10::optional<Storage> U__storage_saved =
    U_.has_storage() ? c10::optional<Storage>(U_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> U__impl_saved;
  if (U_.defined()) U__impl_saved = U_.getIntrusivePtr();
  c10::optional<Storage> S__storage_saved =
    S_.has_storage() ? c10::optional<Storage>(S_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> S__impl_saved;
  if (S_.defined()) S__impl_saved = S_.getIntrusivePtr();
  c10::optional<Storage> Vh__storage_saved =
    Vh_.has_storage() ? c10::optional<Storage>(Vh_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> Vh__impl_saved;
  if (Vh_.defined()) Vh__impl_saved = Vh_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_linalg_svd_outf(ks & c10::after_autograd_keyset, A_, full_matrices, compute_uv, driver, U_, S_, Vh_);
  }
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (U__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(U_))
    TORCH_INTERNAL_ASSERT(U__storage_saved.value().is_alias_of(U_.storage()));
  if (U__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(U_))
    TORCH_INTERNAL_ASSERT(U__impl_saved == U_.getIntrusivePtr());
  if (S__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(S_))
    TORCH_INTERNAL_ASSERT(S__storage_saved.value().is_alias_of(S_.storage()));
  if (S__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(S_))
    TORCH_INTERNAL_ASSERT(S__impl_saved == S_.getIntrusivePtr());
  if (Vh__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(Vh_))
    TORCH_INTERNAL_ASSERT(Vh__storage_saved.value().is_alias_of(Vh_.storage()));
  if (Vh__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(Vh_))
    TORCH_INTERNAL_ASSERT(Vh__impl_saved == Vh_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( U, S, Vh ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(A) || isFwGradDefined(U) || isFwGradDefined(S) || isFwGradDefined(Vh))), "Trying to use forward AD with _linalg_svd_out that does not support it because it is an out= function");
  return std::forward_as_tuple(U, S, Vh);
}
at::Tensor _mps_convolution_transpose(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight );
  
  std::shared_ptr<MpsConvolutionTransposeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MpsConvolutionTransposeBackward0>(new MpsConvolutionTransposeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight ));
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
    grad_fn->output_padding = output_padding.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight))) {
      static c10::OperatorName full_name("aten::_mps_convolution_transpose", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_mps_convolution_transpose", *opt_op, ks, self, weight, padding, output_padding, stride, dilation, groups);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_mps_convolution_transpose_symint(ks & c10::after_autograd_keyset, self_, weight_, padding, output_padding, stride, dilation, groups);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _mps_convolution_transpose");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _mps_convolution_transpose");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_mps_convolution_transpose");
  return result;
}
at::Tensor _nested_tensor_from_mask(c10::DispatchKeySet ks, const at::Tensor & t, const at::Tensor & mask, bool mask_check) {
  auto& t_ = unpack(t, "t", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( t );
  
  std::shared_ptr<NestedTensorFromMaskBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NestedTensorFromMaskBackward0>(new NestedTensorFromMaskBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( t ));
    grad_fn->t_sym_sizes = t.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> t__storage_saved =
    t_.has_storage() ? c10::optional<Storage>(t_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> t__impl_saved;
  if (t_.defined()) t__impl_saved = t_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(t))) {
      static c10::OperatorName full_name("aten::_nested_tensor_from_mask", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_nested_tensor_from_mask", *opt_op, ks, t, mask, mask_check);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_nested_tensor_from_mask(ks & c10::after_autograd_keyset, t_, mask_, mask_check);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (t__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(t_))
    TORCH_INTERNAL_ASSERT(t__storage_saved.value().is_alias_of(t_.storage()));
  if (t__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(t_))
    TORCH_INTERNAL_ASSERT(t__impl_saved == t_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _nested_tensor_from_mask");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _nested_tensor_from_mask");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_nested_tensor_from_mask");
  return result;
}
bool _nested_tensor_from_mask_left_aligned(c10::DispatchKeySet ks, const at::Tensor & t, const at::Tensor & mask) {
  auto& t_ = unpack(t, "t", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  #ifndef NDEBUG
  c10::optional<Storage> t__storage_saved =
    t_.has_storage() ? c10::optional<Storage>(t_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> t__impl_saved;
  if (t_.defined()) t__impl_saved = t_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_nested_tensor_from_mask_left_aligned(ks & c10::after_autograd_keyset, t_, mask_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (t__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(t_))
    TORCH_INTERNAL_ASSERT(t__storage_saved.value().is_alias_of(t_.storage()));
  if (t__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(t_))
    TORCH_INTERNAL_ASSERT(t__impl_saved == t_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor _nested_tensor_size(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_nested_tensor_size(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor _nested_view_from_buffer_copy(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & nested_size, const at::Tensor & nested_strides, const at::Tensor & offsets) {
  auto& self_ = unpack(self, "self", 0);
  auto& nested_size_ = unpack(nested_size, "nested_size", 1);
  auto& nested_strides_ = unpack(nested_strides, "nested_strides", 2);
  auto& offsets_ = unpack(offsets, "offsets", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  check_no_requires_grad(offsets, "offsets", "_nested_view_from_buffer_copy");
  std::shared_ptr<NestedViewFromBufferBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NestedViewFromBufferBackward0_copy>(new NestedViewFromBufferBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> nested_size__storage_saved =
    nested_size_.has_storage() ? c10::optional<Storage>(nested_size_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> nested_size__impl_saved;
  if (nested_size_.defined()) nested_size__impl_saved = nested_size_.getIntrusivePtr();
  c10::optional<Storage> nested_strides__storage_saved =
    nested_strides_.has_storage() ? c10::optional<Storage>(nested_strides_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> nested_strides__impl_saved;
  if (nested_strides_.defined()) nested_strides__impl_saved = nested_strides_.getIntrusivePtr();
  c10::optional<Storage> offsets__storage_saved =
    offsets_.has_storage() ? c10::optional<Storage>(offsets_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> offsets__impl_saved;
  if (offsets_.defined()) offsets__impl_saved = offsets_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(offsets))) {
      static c10::OperatorName full_name("aten::_nested_view_from_buffer_copy", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_nested_view_from_buffer_copy", *opt_op, ks, self, nested_size, nested_strides, offsets);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_nested_view_from_buffer_copy(ks & c10::after_autograd_keyset, self_, nested_size_, nested_strides_, offsets_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (nested_size__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(nested_size_))
    TORCH_INTERNAL_ASSERT(nested_size__storage_saved.value().is_alias_of(nested_size_.storage()));
  if (nested_size__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(nested_size_))
    TORCH_INTERNAL_ASSERT(nested_size__impl_saved == nested_size_.getIntrusivePtr());
  if (nested_strides__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(nested_strides_))
    TORCH_INTERNAL_ASSERT(nested_strides__storage_saved.value().is_alias_of(nested_strides_.storage()));
  if (nested_strides__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(nested_strides_))
    TORCH_INTERNAL_ASSERT(nested_strides__impl_saved == nested_strides_.getIntrusivePtr());
  if (offsets__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(offsets_))
    TORCH_INTERNAL_ASSERT(offsets__storage_saved.value().is_alias_of(offsets_.storage()));
  if (offsets__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(offsets_))
    TORCH_INTERNAL_ASSERT(offsets__impl_saved == offsets_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _nested_view_from_buffer_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _nested_view_from_buffer_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_nested_view_from_buffer_copy");
  return result;
}
int64_t _nnz(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_nnz(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor _reshape_copy(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ReshapeCopyBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReshapeCopyBackward0>(new ReshapeCopyBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_reshape_copy_symint(ks & c10::after_autograd_keyset, self_, size);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _reshape_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _reshape_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_reshape_copy_symint(self_t, size);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _test_autograd_multiple_dispatch_view(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<TestAutogradMultipleDispatchViewBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TestAutogradMultipleDispatchViewBackward0>(new TestAutogradMultipleDispatchViewBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_test_autograd_multiple_dispatch_view", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_test_autograd_multiple_dispatch_view", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowAutograd guard;
      return at::redispatch::_test_autograd_multiple_dispatch_view(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _test_autograd_multiple_dispatch_view");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_test_autograd_multiple_dispatch_view");
  return result;
}
at::Tensor _test_warn_in_autograd(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<TestWarnInAutogradBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TestWarnInAutogradBackward0>(new TestWarnInAutogradBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_test_warn_in_autograd", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_test_warn_in_autograd", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_test_warn_in_autograd(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _test_warn_in_autograd");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _test_warn_in_autograd");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_test_warn_in_autograd");
  return result;
}
at::Tensor _to_sparse_bsr(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<ToSparseBsrBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ToSparseBsrBackward0>(new ToSparseBsrBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_layout = self.layout();
    grad_fn->self_self_sym_blocksize_opt = at::sparse_csr::getSymIntBlockSize(self);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_to_sparse_bsr", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_to_sparse_bsr", *opt_op, ks, self, blocksize, dense_dim);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_to_sparse_bsr(ks & c10::after_autograd_keyset, self_, blocksize, dense_dim);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _to_sparse_bsr");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _to_sparse_bsr");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_to_sparse_bsr");
  return result;
}
at::Tensor _to_sparse_csr(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<int64_t> dense_dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<ToSparseCsrBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ToSparseCsrBackward0>(new ToSparseCsrBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_layout = self.layout();
    grad_fn->self_self_sym_blocksize_opt = at::sparse_csr::getSymIntBlockSize(self);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_to_sparse_csr", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_to_sparse_csr", *opt_op, ks, self, dense_dim);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_to_sparse_csr(ks & c10::after_autograd_keyset, self_, dense_dim);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _to_sparse_csr");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _to_sparse_csr");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_to_sparse_csr");
  return result;
}
at::Tensor _unsafe_view(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UnsafeViewBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnsafeViewBackward0>(new UnsafeViewBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_unsafe_view_symint(ks & c10::after_autograd_keyset, self_, size);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _unsafe_view");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_unsafe_view_symint(self_t, size);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _upsample_bilinear2d_aa(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UpsampleBilinear2DAaBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleBilinear2DAaBackward0>(new UpsampleBilinear2DAaBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->align_corners = align_corners;
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_upsample_bilinear2d_aa_symint(ks & c10::after_autograd_keyset, self_, output_size, align_corners, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _upsample_bilinear2d_aa");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _upsample_bilinear2d_aa");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_upsample_bilinear2d_aa");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_upsample_bilinear2d_aa_symint(self_t, output_size, align_corners, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor acos(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<AcosBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AcosBackward0>(new AcosBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::acos(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: acos");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: acos");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() * -((-self_p * self_p + 1).rsqrt()).conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & adaptive_max_pool3d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& indices_ = unpack(indices, "indices", 2);
  auto& grad_input_ = unpack(grad_input, "grad_input", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self, indices )) {
    throw_error_out_requires_grad("adaptive_max_pool3d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("adaptive_max_pool3d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::adaptive_max_pool3d_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, indices_, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(indices) || isFwGradDefined(grad_input))), "Trying to use forward AD with adaptive_max_pool3d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & add_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("add");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("add");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::add_outf(ks & c10::after_autograd_keyset, self_, other_, alpha, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with add_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & addr_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& vec1_ = unpack(vec1, "vec1", 1);
  auto& vec2_ = unpack(vec2, "vec2", 2);
  auto& out_ = unpack(out, "out", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, vec1, vec2 );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(vec1) || isFwGradDefined(vec2));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, vec1, vec2 )) {
    throw_error_out_requires_grad("addr");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("addr");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> vec1__storage_saved =
    vec1_.has_storage() ? c10::optional<Storage>(vec1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> vec1__impl_saved;
  if (vec1_.defined()) vec1__impl_saved = vec1_.getIntrusivePtr();
  c10::optional<Storage> vec2__storage_saved =
    vec2_.has_storage() ? c10::optional<Storage>(vec2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> vec2__impl_saved;
  if (vec2_.defined()) vec2__impl_saved = vec2_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::addr_outf(ks & c10::after_autograd_keyset, self_, vec1_, vec2_, beta, alpha, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (vec1__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(vec1_))
    TORCH_INTERNAL_ASSERT(vec1__storage_saved.value().is_alias_of(vec1_.storage()));
  if (vec1__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(vec1_))
    TORCH_INTERNAL_ASSERT(vec1__impl_saved == vec1_.getIntrusivePtr());
  if (vec2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(vec2_))
    TORCH_INTERNAL_ASSERT(vec2__storage_saved.value().is_alias_of(vec2_.storage()));
  if (vec2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(vec2_))
    TORCH_INTERNAL_ASSERT(vec2__impl_saved == vec2_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(vec1) || isFwGradDefined(vec2) || isFwGradDefined(out))), "Trying to use forward AD with addr_out that does not support it because it is an out= function");
  return out;
}
at::Tensor alias_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<AliasBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AliasBackward0_copy>(new AliasBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::alias_copy(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: alias_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: alias_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "alias_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t;
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & all_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::all_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with all_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & all_out_dims_out(c10::DispatchKeySet ks, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::all_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with all_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & all_out_all_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::all_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with all_out that does not support it because it is an out= function");
  return out;
}
at::Tensor any_dim(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::any(ks & c10::after_autograd_keyset, self_, dim, keepdim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: any_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: any_dim");
  #endif
  return result;
}
at::Tensor any_dims(c10::DispatchKeySet ks, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::any(ks & c10::after_autograd_keyset, self_, dim, keepdim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: any_dims");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: any_dims");
  #endif
  return result;
}
at::Tensor any(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::any(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: any");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: any");
  #endif
  return result;
}
at::Tensor & argmin_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::argmin_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with argmin_out that does not support it because it is an out= function");
  return out;
}
at::Tensor asin(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<AsinBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AsinBackward0>(new AsinBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::asin(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: asin");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: asin");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() * (-self_p * self_p + 1).rsqrt().conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & asin_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("asin");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("asin");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::asin_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with asin_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & asinh_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("asinh");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("asinh");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::asinh_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with asinh_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & avg_pool2d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& grad_input_ = unpack(grad_input, "grad_input", 8);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self )) {
    throw_error_out_requires_grad("avg_pool2d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("avg_pool2d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::avg_pool2d_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with avg_pool2d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor avg_pool3d(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<AvgPool3DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AvgPool3DBackward0>(new AvgPool3DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->ceil_mode = ceil_mode;
    grad_fn->count_include_pad = count_include_pad;
    grad_fn->divisor_override = divisor_override;
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::avg_pool3d(ks & c10::after_autograd_keyset, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: avg_pool3d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: avg_pool3d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "avg_pool3d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::avg_pool3d(self_t, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor avg_pool3d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<AvgPool3DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AvgPool3DBackwardBackward0>(new AvgPool3DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->ceil_mode = ceil_mode;
    grad_fn->count_include_pad = count_include_pad;
    grad_fn->divisor_override = divisor_override;
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_info = self;
    grad_fn->stride = stride.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::avg_pool3d_backward(ks & c10::after_autograd_keyset, grad_output_, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: avg_pool3d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: avg_pool3d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "avg_pool3d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = avg_pool3d_backward(grad_output_t, self_p, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & binary_cross_entropy_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& target_ = unpack(target, "target", 2);
  auto& grad_input_ = unpack(grad_input, "grad_input", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self, target );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self, target, weight )) {
    throw_error_out_requires_grad("binary_cross_entropy_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("binary_cross_entropy_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::binary_cross_entropy_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, target_, weight, reduction, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target) || isFwGradDefined(weight) || isFwGradDefined(grad_input))), "Trying to use forward AD with binary_cross_entropy_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & bucketize_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& boundaries_ = unpack(boundaries, "boundaries", 1);
  auto& out_ = unpack(out, "out", 4);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> boundaries__storage_saved =
    boundaries_.has_storage() ? c10::optional<Storage>(boundaries_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> boundaries__impl_saved;
  if (boundaries_.defined()) boundaries__impl_saved = boundaries_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::bucketize_outf(ks & c10::after_autograd_keyset, self_, boundaries_, out_int32, right, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (boundaries__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(boundaries_))
    TORCH_INTERNAL_ASSERT(boundaries__storage_saved.value().is_alias_of(boundaries_.storage()));
  if (boundaries__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(boundaries_))
    TORCH_INTERNAL_ASSERT(boundaries__impl_saved == boundaries_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(boundaries) || isFwGradDefined(out))), "Trying to use forward AD with bucketize_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & bucketize_out_Scalar_out(c10::DispatchKeySet ks, const at::Scalar & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
  auto& boundaries_ = unpack(boundaries, "boundaries", 1);
  auto& out_ = unpack(out, "out", 4);
  #ifndef NDEBUG
  c10::optional<Storage> boundaries__storage_saved =
    boundaries_.has_storage() ? c10::optional<Storage>(boundaries_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> boundaries__impl_saved;
  if (boundaries_.defined()) boundaries__impl_saved = boundaries_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::bucketize_outf(ks & c10::after_autograd_keyset, self, boundaries_, out_int32, right, out_);
  }
  #ifndef NDEBUG
  if (boundaries__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(boundaries_))
    TORCH_INTERNAL_ASSERT(boundaries__storage_saved.value().is_alias_of(boundaries_.storage()));
  if (boundaries__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(boundaries_))
    TORCH_INTERNAL_ASSERT(boundaries__impl_saved == boundaries_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(boundaries) || isFwGradDefined(out))), "Trying to use forward AD with bucketize_out that does not support it because it is an out= function");
  return out;
}
at::Tensor ccol_indices(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::ccol_indices(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: ccol_indices");
  #endif
  return result;
}
at::Tensor & ceil_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("ceil");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("ceil");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::ceil_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with ceil_out that does not support it because it is an out= function");
  return out;
}
at::Tensor channel_shuffle(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymInt groups) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<NotImplemented> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("channel_shuffle"), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::channel_shuffle", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("channel_shuffle", *opt_op, ks, self, groups);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::channel_shuffle_symint(ks & c10::after_autograd_keyset, self_, groups);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: channel_shuffle");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "channel_shuffle");
  return result;
}
at::Tensor & cholesky_solve_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & input2, bool upper, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& input2_ = unpack(input2, "input2", 1);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, input2 );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(input2));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, input2 )) {
    throw_error_out_requires_grad("cholesky_solve");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("cholesky_solve");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> input2__storage_saved =
    input2_.has_storage() ? c10::optional<Storage>(input2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input2__impl_saved;
  if (input2_.defined()) input2__impl_saved = input2_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::cholesky_solve_outf(ks & c10::after_autograd_keyset, self_, input2_, upper, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (input2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input2_))
    TORCH_INTERNAL_ASSERT(input2__storage_saved.value().is_alias_of(input2_.storage()));
  if (input2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input2_))
    TORCH_INTERNAL_ASSERT(input2__impl_saved == input2_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(input2) || isFwGradDefined(out))), "Trying to use forward AD with cholesky_solve_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & clamp_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("clamp");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("clamp");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::clamp_outf(ks & c10::after_autograd_keyset, self_, min, max, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with clamp_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & clamp_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, min, max );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(min) || isFwGradDefined(max));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, min, max )) {
    throw_error_out_requires_grad("clamp");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("clamp");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::clamp_outf(ks & c10::after_autograd_keyset, self_, min, max, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(min) || isFwGradDefined(max) || isFwGradDefined(out))), "Trying to use forward AD with clamp_out that does not support it because it is an out= function");
  return out;
}
at::Tensor col_indices_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::col_indices_copy(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: col_indices_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: col_indices_copy");
  #endif
  return result;
}
at::Tensor constant_pad_nd(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef pad, const at::Scalar & value) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ConstantPadNdBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ConstantPadNdBackward0>(new ConstantPadNdBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->pad = pad.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::constant_pad_nd_symint(ks & c10::after_autograd_keyset, self_, pad, value);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: constant_pad_nd");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: constant_pad_nd");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = constant_pad_nd_symint(self_t, pad, 0);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> convolution_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, at::OptionalSymIntArrayRef bias_sizes, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array<bool,3> output_mask) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& input_ = unpack(input, "input", 1);
  auto& weight_ = unpack(weight, "weight", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, input, weight );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(grad_output) || isFwGradDefined(weight));
  [[maybe_unused]] auto _any_has_forward_grad_result1 = (isFwGradDefined(grad_output) || isFwGradDefined(input));
  [[maybe_unused]] auto _any_has_forward_grad_result2 = (isFwGradDefined(grad_output));
  std::shared_ptr<ConvolutionBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ConvolutionBackwardBackward0>(new ConvolutionBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, input, weight ));
    grad_fn->dilation = dilation.vec();
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->groups = groups;
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->output_padding = output_padding.vec();
    grad_fn->padding = padding.vec();
    grad_fn->stride = stride.vec();
    grad_fn->transposed = transposed;
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::convolution_backward_symint(ks & c10::after_autograd_keyset, grad_output_, input_, weight_, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, output_mask);
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: convolution_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: convolution_backward");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: convolution_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: convolution_backward");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: convolution_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: convolution_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "convolution_backward");
  throw_error_for_complex_autograd(result1, "convolution_backward");
  throw_error_for_complex_autograd(result2, "convolution_backward");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto input_p = toNonOptPrimal(input);
      auto weight_t_raw = toNonOptFwGrad(weight);
      auto weight_tensor = toNonOptTensor(weight);
      auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
        ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
      auto weight_p = toNonOptPrimal(weight);
      result0_new_fw_grad_opt = std::get<0>(convolution_backward_symint(grad_output_p, input_p, weight_t, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, {true, false, false})) + std::get<0>(convolution_backward_symint(grad_output_t, input_p, weight_p, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, {true, false, false}));
  }
  c10::optional<at::Tensor> result1_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result1 && (result1.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto input_t_raw = toNonOptFwGrad(input);
      auto input_tensor = toNonOptTensor(input);
      auto input_t = (input_t_raw.defined() || !input_tensor.defined())
        ? input_t_raw : at::_efficientzerotensor(input_tensor.sizes(), input_tensor.options());
      auto input_p = toNonOptPrimal(input);
      auto weight_p = toNonOptPrimal(weight);
      result1_new_fw_grad_opt = std::get<1>(convolution_backward_symint(grad_output_p, input_t, weight_p, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, {false, true, false})) + std::get<1>(convolution_backward_symint(grad_output_t, input_p, weight_p, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, {false, true, false}));
  }
  c10::optional<at::Tensor> result2_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result2 && (result2.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result2_new_fw_grad_opt = convolution_backward_jvp_grad_bias(grad_output_t, result2);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (result1_new_fw_grad_opt.has_value() && result1_new_fw_grad_opt.value().defined() && result1.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result1._set_fw_grad(result1_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (result2_new_fw_grad_opt.has_value() && result2_new_fw_grad_opt.value().defined() && result2.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result2._set_fw_grad(result2_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
at::Tensor convolution_overrideable(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  std::shared_ptr<ConvolutionOverrideableBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ConvolutionOverrideableBackward0>(new ConvolutionOverrideableBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->output_padding = output_padding.vec();
    grad_fn->padding = padding.vec();
    grad_fn->stride = stride.vec();
    grad_fn->transposed = transposed;
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::convolution_overrideable", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("convolution_overrideable", *opt_op, ks, input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::convolution_overrideable_symint(ks & c10::after_autograd_keyset, input_, weight_, bias, stride, padding, dilation, transposed, output_padding, groups);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: convolution_overrideable");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: convolution_overrideable");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "convolution_overrideable");
  return result;
}
at::Tensor copysign_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<CopysignBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CopysignBackward0>(new CopysignBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_info = other;
    if (grad_fn->should_compute_output(0)) {
      grad_fn->self_ = SavedVariable(self, false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::copysign(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: copysign_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: copysign_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "copysign");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = copysign_tensor_self_backward(self_t, self_p, result);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor copysign_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<CopysignBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CopysignBackward1>(new CopysignBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::copysign(ks & c10::after_autograd_keyset, self_, other);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: copysign_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: copysign_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "copysign");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (copysign_tensor_self_backward(self_t.conj(), self_p, result)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & copysign__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<CopysignBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CopysignBackward0>(new CopysignBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_info = other;
    if (grad_fn->should_compute_output(0)) {
      if (!original_self.has_value()) original_self = self.clone();
      grad_fn->self_ = SavedVariable(original_self.value(), false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::copysign_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(copysign_tensor_self_backward(original_self_t, original_self_p, self_p)) : copysign_tensor_self_backward(original_self_t, original_self_p, self_p);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor & copysign__Scalar(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<CopysignBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CopysignBackward1>(new CopysignBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::copysign_(ks & c10::after_autograd_keyset, self_, other);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((copysign_tensor_self_backward(original_self_t.conj(), original_self_p, self_p)).conj()) : (copysign_tensor_self_backward(original_self_t.conj(), original_self_p, self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor & copysign_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("copysign");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("copysign");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::copysign_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with copysign_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & copysign_out_Scalar_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("copysign");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("copysign");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::copysign_outf(ks & c10::after_autograd_keyset, self_, other, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with copysign_out that does not support it because it is an out= function");
  return out;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> cudnn_batch_norm(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias));
  check_no_requires_grad(running_mean, "running_mean", "cudnn_batch_norm");
  check_no_requires_grad(running_var, "running_var", "cudnn_batch_norm");
  std::shared_ptr<CudnnBatchNormBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CudnnBatchNormBackward0>(new CudnnBatchNormBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->epsilon = epsilon;
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->running_mean_ = SavedVariable(running_mean, false);
    grad_fn->running_var_ = SavedVariable(running_var, false);
    grad_fn->training = training;
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  at::Tensor result3;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::cudnn_batch_norm(ks & c10::after_autograd_keyset, input_, weight_, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
  })();
  std::tie(result0, result1, result2, result3) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: cudnn_batch_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: cudnn_batch_norm");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: cudnn_batch_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: cudnn_batch_norm");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: cudnn_batch_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: cudnn_batch_norm");
  if (result3.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result3)) {
    TORCH_INTERNAL_ASSERT(result3.storage().use_count() == 1, "function: cudnn_batch_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result3))
    TORCH_INTERNAL_ASSERT(result3.use_count() <= 1, "function: cudnn_batch_norm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "cudnn_batch_norm");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto input_t_raw = toNonOptFwGrad(input);
      auto input_tensor = toNonOptTensor(input);
      auto input_t = (input_t_raw.defined() || !input_tensor.defined())
        ? input_t_raw : at::_efficientzerotensor(input_tensor.sizes(), input_tensor.options());
      auto input_p = toNonOptPrimal(input);
      auto weight_t_raw = toNonOptFwGrad(weight);
      auto weight_tensor = toNonOptTensor(weight);
      auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
        ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
      auto weight_p = toNonOptPrimal(weight);
      auto bias_t_raw = toNonOptFwGrad(bias);
      auto bias_tensor = toNonOptTensor(bias);
      auto bias_t = (bias_t_raw.defined() || !bias_tensor.defined())
        ? bias_t_raw : at::_efficientzerotensor(bias_tensor.sizes(), bias_tensor.options());
      auto bias_p = toNonOptPrimal(bias);
      result0_new_fw_grad_opt = batch_norm_jvp(input_p, input_t, weight_p, weight_t, bias_p, bias_t, running_mean, running_var, result1, result2, training, epsilon);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
    grad_fn->result3_ = SavedVariable(result3, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3));
}
::std::tuple<at::Tensor &,at::Tensor &> cummin_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
  auto& self_ = unpack(self, "self", 0);
  auto& values_ = unpack(values, "values", 2);
  auto& indices_ = unpack(indices, "indices", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("cummin");
  }
  if (compute_requires_grad( values )) {
    throw_error_out_requires_grad("cummin");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> values__storage_saved =
    values_.has_storage() ? c10::optional<Storage>(values_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> values__impl_saved;
  if (values_.defined()) values__impl_saved = values_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::cummin_outf(ks & c10::after_autograd_keyset, self_, dim, values_, indices_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__storage_saved.value().is_alias_of(values_.storage()));
  if (values__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__impl_saved == values_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( values ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(values) || isFwGradDefined(indices))), "Trying to use forward AD with cummin_out that does not support it because it is an out= function");
  return std::forward_as_tuple(values, indices);
}
at::Tensor deg2rad(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Deg2RadBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Deg2RadBackward0>(new Deg2RadBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::deg2rad(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: deg2rad");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: deg2rad");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "deg2rad");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (deg2rad_backward(self_t.conj())).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & deg2rad_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<Deg2RadBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Deg2RadBackward0>(new Deg2RadBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::deg2rad_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((deg2rad_backward(self_t.conj())).conj()) : (deg2rad_backward(self_t.conj())).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & deg2rad_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("deg2rad");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("deg2rad");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::deg2rad_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with deg2rad_out that does not support it because it is an out= function");
  return out;
}
at::Tensor diagonal_copy(c10::DispatchKeySet ks, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<DiagonalBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DiagonalBackward0_copy>(new DiagonalBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim1 = dim1;
    grad_fn->dim2 = dim2;
    grad_fn->offset = offset;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::diagonal_copy(ks & c10::after_autograd_keyset, self_, offset, dim1, dim2);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: diagonal_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: diagonal_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::diagonal(self_t, offset, dim1, dim2);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & digamma_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("digamma");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("digamma");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::digamma_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with digamma_out that does not support it because it is an out= function");
  return out;
}
at::Tensor dist(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, const at::Scalar & p) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<DistBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DistBackward0>(new DistBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->p = p;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::dist(ks & c10::after_autograd_keyset, self_, other_, p);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: dist");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: dist");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "dist");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      result_new_fw_grad_opt = norm_jvp(self_p - other_p, self_t - other_t, p, result, {}, false);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & dot_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & tensor, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& tensor_ = unpack(tensor, "tensor", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, tensor );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(tensor));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, tensor )) {
    throw_error_out_requires_grad("dot");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("dot");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> tensor__storage_saved =
    tensor_.has_storage() ? c10::optional<Storage>(tensor_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> tensor__impl_saved;
  if (tensor_.defined()) tensor__impl_saved = tensor_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::dot_outf(ks & c10::after_autograd_keyset, self_, tensor_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (tensor__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(tensor_))
    TORCH_INTERNAL_ASSERT(tensor__storage_saved.value().is_alias_of(tensor_.storage()));
  if (tensor__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(tensor_))
    TORCH_INTERNAL_ASSERT(tensor__impl_saved == tensor_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(tensor) || isFwGradDefined(out))), "Trying to use forward AD with dot_out that does not support it because it is an out= function");
  return out;
}
at::Tensor elu(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<EluBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<EluBackward0>(new EluBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->alpha = alpha;
    grad_fn->input_scale = input_scale;
    grad_fn->scale = scale;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::elu(ks & c10::after_autograd_keyset, self_, alpha, scale, input_scale);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: elu");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: elu");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "elu");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (elu_backward(self_t.conj(), alpha, scale, input_scale, /* is_result */ false, self_p)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & elu_(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<EluBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<EluBackward1>(new EluBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->alpha = alpha;
    grad_fn->input_scale = input_scale;
    grad_fn->scale = scale;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::elu_(ks & c10::after_autograd_keyset, self_, alpha, scale, input_scale);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t.copy_(elu_backward(original_self_t, alpha, scale, input_scale, /* is_result */ true, self_p));
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor & elu_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_or_result_ = unpack(self_or_result, "self_or_result", 5);
  auto& grad_input_ = unpack(grad_input, "grad_input", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self_or_result );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self_or_result));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self_or_result )) {
    throw_error_out_requires_grad("elu_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("elu_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self_or_result__storage_saved =
    self_or_result_.has_storage() ? c10::optional<Storage>(self_or_result_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self_or_result__impl_saved;
  if (self_or_result_.defined()) self_or_result__impl_saved = self_or_result_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::elu_backward_outf(ks & c10::after_autograd_keyset, grad_output_, alpha, scale, input_scale, is_result, self_or_result_, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self_or_result__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_or_result_))
    TORCH_INTERNAL_ASSERT(self_or_result__storage_saved.value().is_alias_of(self_or_result_.storage()));
  if (self_or_result__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_or_result_))
    TORCH_INTERNAL_ASSERT(self_or_result__impl_saved == self_or_result_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self_or_result) || isFwGradDefined(grad_input))), "Trying to use forward AD with elu_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor embedding(c10::DispatchKeySet ks, const at::Tensor & weight, const at::Tensor & indices, c10::SymInt padding_idx, bool scale_grad_by_freq, bool sparse) {
  auto& weight_ = unpack(weight, "weight", 0);
  auto& indices_ = unpack(indices, "indices", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( weight );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(weight));
  std::shared_ptr<EmbeddingBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<EmbeddingBackward0>(new EmbeddingBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( weight ));
    grad_fn->indices_ = SavedVariable(indices, false);
    grad_fn->padding_idx = padding_idx;
    grad_fn->scale_grad_by_freq = scale_grad_by_freq;
    grad_fn->sparse = sparse;
    grad_fn->weight_sym_argsize_0 = weight.sym_size(0);
  }
  #ifndef NDEBUG
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::embedding_symint(ks & c10::after_autograd_keyset, weight_, indices_, padding_idx, scale_grad_by_freq, sparse);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: embedding");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: embedding");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "embedding");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto weight_t_raw = toNonOptFwGrad(weight);
      auto weight_tensor = toNonOptTensor(weight);
      auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
        ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
      result_new_fw_grad_opt = at::embedding_symint(weight_t, indices, padding_idx, scale_grad_by_freq, sparse);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & empty_like_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::empty_like_outf(ks & c10::after_autograd_keyset, self_, memory_format, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with empty_like_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & erf_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("erf");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("erf");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::erf_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with erf_out that does not support it because it is an out= function");
  return out;
}
at::Tensor erfc(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ErfcBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ErfcBackward0>(new ErfcBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::erfc(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: erfc");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: erfc");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "erfc");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (-2.0 / sqrt(M_PI) * exp(-(self_p.pow(2))) * self_t.conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & erfc_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("erfc");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("erfc");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::erfc_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with erfc_out that does not support it because it is an out= function");
  return out;
}
at::Tensor erfinv(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ErfinvBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ErfinvBackward0>(new ErfinvBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::erfinv(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: erfinv");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: erfinv");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "erfinv");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (0.5 * sqrt(M_PI) * exp(self_p.erfinv().pow(2)) * self_t.conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & erfinv_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<ErfinvBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ErfinvBackward0>(new ErfinvBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::erfinv_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((0.5 * sqrt(M_PI) * exp(original_self_p.erfinv().pow(2)) * original_self_t.conj()).conj()) : (0.5 * sqrt(M_PI) * exp(original_self_p.erfinv().pow(2)) * original_self_t.conj()).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & fmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("fmax");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("fmax");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::fmax_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with fmax_out that does not support it because it is an out= function");
  return out;
}
at::Tensor fmod_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<FmodBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FmodBackward0>(new FmodBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::fmod(ks & c10::after_autograd_keyset, self_, other);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: fmod_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: fmod_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "fmod");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (self_t.conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor fmod_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<FmodBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FmodBackward1>(new FmodBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    if (grad_fn->should_compute_output(1)) {
      grad_fn->other_ = SavedVariable(other, false);
    }
    if (grad_fn->should_compute_output(1)) {
      grad_fn->self_ = SavedVariable(self, false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::fmod(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: fmod_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: fmod_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "fmod");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      result_new_fw_grad_opt = self_t - other_t * self_p.div(other_p, /*rounding_mode=*/"trunc");
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool2d_out_output(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
  auto& self_ = unpack(self, "self", 0);
  auto& random_samples_ = unpack(random_samples, "random_samples", 3);
  auto& output_ = unpack(output, "output", 4);
  auto& indices_ = unpack(indices, "indices", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, random_samples )) {
    throw_error_out_requires_grad("fractional_max_pool2d");
  }
  if (compute_requires_grad( output )) {
    throw_error_out_requires_grad("fractional_max_pool2d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> random_samples__storage_saved =
    random_samples_.has_storage() ? c10::optional<Storage>(random_samples_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> random_samples__impl_saved;
  if (random_samples_.defined()) random_samples__impl_saved = random_samples_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::fractional_max_pool2d_outf(ks & c10::after_autograd_keyset, self_, kernel_size, output_size, random_samples_, output_, indices_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (random_samples__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(random_samples_))
    TORCH_INTERNAL_ASSERT(random_samples__storage_saved.value().is_alias_of(random_samples_.storage()));
  if (random_samples__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(random_samples_))
    TORCH_INTERNAL_ASSERT(random_samples__impl_saved == random_samples_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( output ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(random_samples) || isFwGradDefined(output) || isFwGradDefined(indices))), "Trying to use forward AD with fractional_max_pool2d_out that does not support it because it is an out= function");
  return std::forward_as_tuple(output, indices);
}
at::Tensor & fractional_max_pool3d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& indices_ = unpack(indices, "indices", 4);
  auto& grad_input_ = unpack(grad_input, "grad_input", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self, indices )) {
    throw_error_out_requires_grad("fractional_max_pool3d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("fractional_max_pool3d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::fractional_max_pool3d_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, kernel_size, output_size, indices_, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(indices) || isFwGradDefined(grad_input))), "Trying to use forward AD with fractional_max_pool3d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool3d_out_output(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
  auto& self_ = unpack(self, "self", 0);
  auto& random_samples_ = unpack(random_samples, "random_samples", 3);
  auto& output_ = unpack(output, "output", 4);
  auto& indices_ = unpack(indices, "indices", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, random_samples )) {
    throw_error_out_requires_grad("fractional_max_pool3d");
  }
  if (compute_requires_grad( output )) {
    throw_error_out_requires_grad("fractional_max_pool3d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> random_samples__storage_saved =
    random_samples_.has_storage() ? c10::optional<Storage>(random_samples_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> random_samples__impl_saved;
  if (random_samples_.defined()) random_samples__impl_saved = random_samples_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::fractional_max_pool3d_outf(ks & c10::after_autograd_keyset, self_, kernel_size, output_size, random_samples_, output_, indices_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (random_samples__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(random_samples_))
    TORCH_INTERNAL_ASSERT(random_samples__storage_saved.value().is_alias_of(random_samples_.storage()));
  if (random_samples__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(random_samples_))
    TORCH_INTERNAL_ASSERT(random_samples__impl_saved == random_samples_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( output ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(random_samples) || isFwGradDefined(output) || isFwGradDefined(indices))), "Trying to use forward AD with fractional_max_pool3d_out that does not support it because it is an out= function");
  return std::forward_as_tuple(output, indices);
}
::std::tuple<at::Tensor &,at::Tensor &> frexp_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & mantissa, at::Tensor & exponent) {
  auto& self_ = unpack(self, "self", 0);
  auto& mantissa_ = unpack(mantissa, "mantissa", 1);
  auto& exponent_ = unpack(exponent, "exponent", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_mantissa = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("frexp");
  }
  if (compute_requires_grad( mantissa )) {
    throw_error_out_requires_grad("frexp");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mantissa__storage_saved =
    mantissa_.has_storage() ? c10::optional<Storage>(mantissa_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mantissa__impl_saved;
  if (mantissa_.defined()) mantissa__impl_saved = mantissa_.getIntrusivePtr();
  c10::optional<Storage> exponent__storage_saved =
    exponent_.has_storage() ? c10::optional<Storage>(exponent_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> exponent__impl_saved;
  if (exponent_.defined()) exponent__impl_saved = exponent_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::frexp_outf(ks & c10::after_autograd_keyset, self_, mantissa_, exponent_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mantissa__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mantissa_))
    TORCH_INTERNAL_ASSERT(mantissa__storage_saved.value().is_alias_of(mantissa_.storage()));
  if (mantissa__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mantissa_))
    TORCH_INTERNAL_ASSERT(mantissa__impl_saved == mantissa_.getIntrusivePtr());
  if (exponent__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(exponent_))
    TORCH_INTERNAL_ASSERT(exponent__storage_saved.value().is_alias_of(exponent_.storage()));
  if (exponent__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(exponent_))
    TORCH_INTERNAL_ASSERT(exponent__impl_saved == exponent_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( mantissa ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(mantissa) || isFwGradDefined(exponent))), "Trying to use forward AD with frexp_out that does not support it because it is an out= function");
  return std::forward_as_tuple(mantissa, exponent);
}
at::Tensor full_like(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::full_like(ks & c10::after_autograd_keyset, self_, fill_value, dtype, layout, device, pin_memory, memory_format);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: full_like");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: full_like");
  #endif
  return result;
}
at::Tensor & ge_out_Scalar_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::ge_outf(ks & c10::after_autograd_keyset, self_, other, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with ge_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & ge_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::ge_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with ge_out that does not support it because it is an out= function");
  return out;
}
::std::tuple<at::Tensor,at::Tensor> geqrf(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<GeqrfBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GeqrfBackward0>(new GeqrfBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  at::Tensor a;
  at::Tensor tau;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::geqrf", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("geqrf", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::geqrf(ks & c10::after_autograd_keyset, self_);
    }
  })();
  std::tie(a, tau) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (a.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(a)) {
    TORCH_INTERNAL_ASSERT(a.storage().use_count() == 1, "function: geqrf");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(a))
    TORCH_INTERNAL_ASSERT(a.use_count() <= 1, "function: geqrf");
  if (tau.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(tau)) {
    TORCH_INTERNAL_ASSERT(tau.storage().use_count() == 1, "function: geqrf");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(tau))
    TORCH_INTERNAL_ASSERT(tau.use_count() <= 1, "function: geqrf");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( a, tau ), grad_fn);
  }
  throw_error_for_complex_autograd(a, "geqrf");
  throw_error_for_complex_autograd(tau, "geqrf");
  return std::make_tuple(std::move(a), std::move(tau));
}
at::Tensor glu_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, int64_t dim) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<GluBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GluBackwardBackward0>(new GluBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->dim = dim;
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::glu_backward(ks & c10::after_autograd_keyset, grad_output_, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: glu_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: glu_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "glu_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = glu_backward_jvp(result, grad_output_p, self_p, grad_output_t, self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor grid_sampler_2d(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  auto& input_ = unpack(input, "input", 0);
  auto& grid_ = unpack(grid, "grid", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, grid );
  
  std::shared_ptr<GridSampler2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GridSampler2DBackward0>(new GridSampler2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, grid ));
    grad_fn->align_corners = align_corners;
    grad_fn->grid_ = SavedVariable(grid, false);
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->interpolation_mode = interpolation_mode;
    grad_fn->padding_mode = padding_mode;
  }
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> grid__storage_saved =
    grid_.has_storage() ? c10::optional<Storage>(grid_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grid__impl_saved;
  if (grid_.defined()) grid__impl_saved = grid_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input) || isFwGradDefined(grid))) {
      static c10::OperatorName full_name("aten::grid_sampler_2d", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("grid_sampler_2d", *opt_op, ks, input, grid, interpolation_mode, padding_mode, align_corners);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::grid_sampler_2d(ks & c10::after_autograd_keyset, input_, grid_, interpolation_mode, padding_mode, align_corners);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (grid__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grid_))
    TORCH_INTERNAL_ASSERT(grid__storage_saved.value().is_alias_of(grid_.storage()));
  if (grid__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grid_))
    TORCH_INTERNAL_ASSERT(grid__impl_saved == grid_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: grid_sampler_2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: grid_sampler_2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "grid_sampler_2d");
  return result;
}
at::Tensor & gt__Scalar(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<GtBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GtBackward0>(new GtBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::gt_(ks & c10::after_autograd_keyset, self_, other);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & gt__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<GtBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GtBackward1>(new GtBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_info = other;
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::gt_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor hardshrink_backward(c10::DispatchKeySet ks, const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd) {
  auto& grad_out_ = unpack(grad_out, "grad_out", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_out, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_out));
  std::shared_ptr<HardshrinkBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<HardshrinkBackwardBackward0>(new HardshrinkBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_out, self ));
    grad_fn->lambd = lambd;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_out__storage_saved =
    grad_out_.has_storage() ? c10::optional<Storage>(grad_out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_out__impl_saved;
  if (grad_out_.defined()) grad_out__impl_saved = grad_out_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::hardshrink_backward(ks & c10::after_autograd_keyset, grad_out_, self_, lambd);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_out_))
    TORCH_INTERNAL_ASSERT(grad_out__storage_saved.value().is_alias_of(grad_out_.storage()));
  if (grad_out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_out_))
    TORCH_INTERNAL_ASSERT(grad_out__impl_saved == grad_out_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: hardshrink_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: hardshrink_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "hardshrink_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_out_t_raw = toNonOptFwGrad(grad_out);
      auto grad_out_tensor = toNonOptTensor(grad_out);
      auto grad_out_t = (grad_out_t_raw.defined() || !grad_out_tensor.defined())
        ? grad_out_t_raw : at::_efficientzerotensor(grad_out_tensor.sizes(), grad_out_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = at::where((self_p > lambd).logical_or(self_p < -lambd), grad_out_t, at::zeros({}, result.options()).expand_as(result));
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & hardshrink_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
  auto& grad_out_ = unpack(grad_out, "grad_out", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& grad_input_ = unpack(grad_input, "grad_input", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_out, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_out));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_out, self )) {
    throw_error_out_requires_grad("hardshrink_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("hardshrink_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_out__storage_saved =
    grad_out_.has_storage() ? c10::optional<Storage>(grad_out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_out__impl_saved;
  if (grad_out_.defined()) grad_out__impl_saved = grad_out_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::hardshrink_backward_outf(ks & c10::after_autograd_keyset, grad_out_, self_, lambd, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_out_))
    TORCH_INTERNAL_ASSERT(grad_out__storage_saved.value().is_alias_of(grad_out_.storage()));
  if (grad_out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_out_))
    TORCH_INTERNAL_ASSERT(grad_out__impl_saved == grad_out_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_out) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with hardshrink_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & hardswish_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<HardswishBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<HardswishBackward0>(new HardswishBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::hardswish_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((hardswish_backward(original_self_t.conj(), original_self_p)).conj()) : (hardswish_backward(original_self_t.conj(), original_self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & hardtanh_(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<HardtanhBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<HardtanhBackward0>(new HardtanhBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->max_val = max_val;
    grad_fn->min_val = min_val;
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::hardtanh_(ks & c10::after_autograd_keyset, self_, min_val, max_val);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((hardtanh_backward(original_self_t.conj(), original_self_p, min_val, max_val)).conj()) : (hardtanh_backward(original_self_t.conj(), original_self_p, min_val, max_val)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor hardtanh_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<HardtanhBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<HardtanhBackwardBackward0>(new HardtanhBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->max_val = max_val;
    grad_fn->min_val = min_val;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::hardtanh_backward(ks & c10::after_autograd_keyset, grad_output_, self_, min_val, max_val);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: hardtanh_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: hardtanh_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "hardtanh_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = at::where((self_p > min_val).logical_and(self_p < max_val), grad_output_t, at::zeros({}, result.options()).expand_as(result));
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & huber_loss_backward_out_out(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& target_ = unpack(target, "target", 2);
  auto& grad_input_ = unpack(grad_input, "grad_input", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self, target );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self, target )) {
    throw_error_out_requires_grad("huber_loss_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("huber_loss_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::huber_loss_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, target_, reduction, delta, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target) || isFwGradDefined(grad_input))), "Trying to use forward AD with huber_loss_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & huber_loss_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& target_ = unpack(target, "target", 1);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, target );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(target));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, target )) {
    throw_error_out_requires_grad("huber_loss");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("huber_loss");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::huber_loss_outf(ks & c10::after_autograd_keyset, self_, target_, reduction, delta, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(target) || isFwGradDefined(out))), "Trying to use forward AD with huber_loss_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & i0_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<I0Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<I0Backward0>(new I0Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::i0_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * at::special_i1(original_self_p)).conj()) : (original_self_t.conj() * at::special_i1(original_self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & igamma_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("igamma");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("igamma");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::igamma_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with igamma_out that does not support it because it is an out= function");
  return out;
}
at::Tensor im2col(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Im2ColBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Im2ColBackward0>(new Im2ColBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dilation = dilation.vec();
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_sym_argsize_minus_1 = self.sym_size(-1);
    grad_fn->self_sym_argsize_minus_2 = self.sym_size(-2);
    grad_fn->stride = stride.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::im2col(ks & c10::after_autograd_keyset, self_, kernel_size, dilation, padding, stride);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: im2col");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: im2col");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::im2col(self_t, kernel_size, dilation, padding, stride);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor index_add(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  auto& source_ = unpack(source, "source", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, source );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(source));
  std::shared_ptr<IndexAddBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<IndexAddBackward0>(new IndexAddBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, source ));
    grad_fn->alpha = alpha;
    grad_fn->dim = dim;
    if (grad_fn->should_compute_output(1)) {
      grad_fn->index_ = SavedVariable(index, false);
    }
    if (grad_fn->should_compute_output(1)) {
      grad_fn->source_ = SavedVariable(source, false);
    }
    grad_fn->source_dim = source.dim();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> source__storage_saved =
    source_.has_storage() ? c10::optional<Storage>(source_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> source__impl_saved;
  if (source_.defined()) source__impl_saved = source_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::index_add(ks & c10::after_autograd_keyset, self_, dim, index_, source_, alpha);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (source__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__storage_saved.value().is_alias_of(source_.storage()));
  if (source__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__impl_saved == source_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: index_add");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: index_add");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto source_t_raw = toNonOptFwGrad(source);
      auto source_tensor = toNonOptTensor(source);
      auto source_t = (source_t_raw.defined() || !source_tensor.defined())
        ? source_t_raw : at::_efficientzerotensor(source_tensor.sizes(), source_tensor.options());
      result_new_fw_grad_opt = at::index_add(self_t, dim, index, maybe_multiply(source_t, alpha));
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & index_add_(c10::DispatchKeySet ks, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  auto& source_ = unpack(source, "source", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, source );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(source));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<IndexAddBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<IndexAddBackward0>(new IndexAddBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, source ));
    grad_fn->alpha = alpha;
    grad_fn->dim = dim;
    if (grad_fn->should_compute_output(1)) {
      grad_fn->index_ = SavedVariable(index, false);
    }
    if (grad_fn->should_compute_output(1)) {
      grad_fn->source_ = SavedVariable(source, false);
    }
    grad_fn->source_dim = source.dim();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> source__storage_saved =
    source_.has_storage() ? c10::optional<Storage>(source_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> source__impl_saved;
  if (source_.defined()) source__impl_saved = source_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::index_add_(ks & c10::after_autograd_keyset, self_, dim, index_, source_, alpha);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (source__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__storage_saved.value().is_alias_of(source_.storage()));
  if (source__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__impl_saved == source_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto source_t_raw = toNonOptFwGrad(source);
      auto source_tensor = toNonOptTensor(source);
      auto source_t = (source_t_raw.defined() || !source_tensor.defined())
        ? source_t_raw : at::_efficientzerotensor(source_tensor.sizes(), source_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(at::index_add(self_t, dim, index, maybe_multiply(source_t, alpha))) : at::index_add(self_t, dim, index, maybe_multiply(source_t, alpha));
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor index_put(c10::DispatchKeySet ks, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
  auto& self_ = unpack(self, "self", 0);
  auto& values_ = unpack(values, "values", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, values );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(values));
  check_no_requires_grad(indices, "indices", "index_put");
  std::shared_ptr<IndexPutBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<IndexPutBackward0>(new IndexPutBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, values ));
    grad_fn->accumulate = accumulate;
    grad_fn->indices_ = make_saved_variable_list(indices, false);
    grad_fn->values_info = values;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> indices_storage_saved(indices.size());
  for (const c10::optional<Tensor>& tensor : indices)
    indices_storage_saved.push_back(
      tensor.has_value() && tensor->has_storage() ? c10::optional<Storage>(tensor->storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> indices_impl_saved(indices.size());
  for (size_t i=0; i<indices.size(); i++) {
    c10::optional<Tensor> t = indices[i];
    if (t.has_value() && t->defined()) indices_impl_saved[i] = t->getIntrusivePtr();
  }
  c10::optional<Storage> values__storage_saved =
    values_.has_storage() ? c10::optional<Storage>(values_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> values__impl_saved;
  if (values_.defined()) values__impl_saved = values_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::index_put(ks & c10::after_autograd_keyset, self_, indices, values_, accumulate);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<indices.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (indices_storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(indices))
      TORCH_INTERNAL_ASSERT(indices_storage_saved[i].value().is_alias_of(
          static_cast<c10::optional<Tensor>>(indices[i])->storage()));
  }
  for (size_t i=0; i<indices.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (indices_impl_saved[i])
      TORCH_INTERNAL_ASSERT(
        indices_impl_saved[i] == static_cast<c10::optional<Tensor>>(indices[i])->getIntrusivePtr());
  }
  if (values__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__storage_saved.value().is_alias_of(values_.storage()));
  if (values__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__impl_saved == values_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: index_put");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: index_put");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto values_t_raw = toNonOptFwGrad(values);
      auto values_tensor = toNonOptTensor(values);
      auto values_t = (values_t_raw.defined() || !values_tensor.defined())
        ? values_t_raw : at::_efficientzerotensor(values_tensor.sizes(), values_tensor.options());
      result_new_fw_grad_opt = self_t.index_put(indices, values_t, accumulate);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & index_reduce_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, c10::string_view reduce, bool include_self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  auto& source_ = unpack(source, "source", 3);
  auto& out_ = unpack(out, "out", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, source );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, source )) {
    throw_error_out_requires_grad("index_reduce");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("index_reduce");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> source__storage_saved =
    source_.has_storage() ? c10::optional<Storage>(source_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> source__impl_saved;
  if (source_.defined()) source__impl_saved = source_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::index_reduce_outf(ks & c10::after_autograd_keyset, self_, dim, index_, source_, reduce, include_self, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (source__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__storage_saved.value().is_alias_of(source_.storage()));
  if (source__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__impl_saved == source_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(source) || isFwGradDefined(out))), "Trying to use forward AD with index_reduce_out that does not support it because it is an out= function");
  return out;
}
bool is_same_size(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::is_same_size(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor isneginf(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::isneginf(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: isneginf");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: isneginf");
  #endif
  return result;
}
at::Tensor & leaky_relu_(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & negative_slope) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<LeakyReluBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LeakyReluBackward1>(new LeakyReluBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->negative_slope = negative_slope;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::leaky_relu_(ks & c10::after_autograd_keyset, self_, negative_slope);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t.copy_(leaky_relu_backward(original_self_t.conj(), self_p, negative_slope, true).conj());
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor leaky_relu_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<LeakyReluBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LeakyReluBackwardBackward0>(new LeakyReluBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->negative_slope = negative_slope;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::leaky_relu_backward(ks & c10::after_autograd_keyset, grad_output_, self_, negative_slope, self_is_result);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: leaky_relu_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: leaky_relu_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "leaky_relu_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = leaky_relu_backward(grad_output_t, self_p, negative_slope, self_is_result);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_lu_out_out(c10::DispatchKeySet ks, const at::Tensor & A, bool pivot, at::Tensor & P, at::Tensor & L, at::Tensor & U) {
  auto& A_ = unpack(A, "A", 0);
  auto& P_ = unpack(P, "P", 2);
  auto& L_ = unpack(L, "L", 3);
  auto& U_ = unpack(U, "U", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_L = (isFwGradDefined(A));
  [[maybe_unused]] auto _any_has_forward_grad_U = (isFwGradDefined(A));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( A )) {
    throw_error_out_requires_grad("linalg_lu");
  }
  if (compute_requires_grad( L, U )) {
    throw_error_out_requires_grad("linalg_lu");
  }
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  c10::optional<Storage> P__storage_saved =
    P_.has_storage() ? c10::optional<Storage>(P_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> P__impl_saved;
  if (P_.defined()) P__impl_saved = P_.getIntrusivePtr();
  c10::optional<Storage> L__storage_saved =
    L_.has_storage() ? c10::optional<Storage>(L_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> L__impl_saved;
  if (L_.defined()) L__impl_saved = L_.getIntrusivePtr();
  c10::optional<Storage> U__storage_saved =
    U_.has_storage() ? c10::optional<Storage>(U_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> U__impl_saved;
  if (U_.defined()) U__impl_saved = U_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::linalg_lu_outf(ks & c10::after_autograd_keyset, A_, pivot, P_, L_, U_);
  }
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (P__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(P_))
    TORCH_INTERNAL_ASSERT(P__storage_saved.value().is_alias_of(P_.storage()));
  if (P__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(P_))
    TORCH_INTERNAL_ASSERT(P__impl_saved == P_.getIntrusivePtr());
  if (L__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(L_))
    TORCH_INTERNAL_ASSERT(L__storage_saved.value().is_alias_of(L_.storage()));
  if (L__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(L_))
    TORCH_INTERNAL_ASSERT(L__impl_saved == L_.getIntrusivePtr());
  if (U__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(U_))
    TORCH_INTERNAL_ASSERT(U__storage_saved.value().is_alias_of(U_.storage()));
  if (U__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(U_))
    TORCH_INTERNAL_ASSERT(U__impl_saved == U_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( L, U ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(A) || isFwGradDefined(P) || isFwGradDefined(L) || isFwGradDefined(U))), "Trying to use forward AD with linalg_lu_out that does not support it because it is an out= function");
  return std::forward_as_tuple(P, L, U);
}
at::Tensor linalg_lu_solve(c10::DispatchKeySet ks, const at::Tensor & LU, const at::Tensor & pivots, const at::Tensor & B, bool left, bool adjoint) {
  auto& LU_ = unpack(LU, "LU", 0);
  auto& pivots_ = unpack(pivots, "pivots", 1);
  auto& B_ = unpack(B, "B", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( LU, B );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(LU) || isFwGradDefined(B));
  check_no_requires_grad(pivots, "pivots", "linalg_lu_solve");
  std::shared_ptr<LinalgLuSolveBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgLuSolveBackward0>(new LinalgLuSolveBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( LU, B ));
    grad_fn->LU_ = SavedVariable(LU, false);
    grad_fn->adjoint = adjoint;
    grad_fn->left = left;
    grad_fn->pivots_ = SavedVariable(pivots, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> LU__storage_saved =
    LU_.has_storage() ? c10::optional<Storage>(LU_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> LU__impl_saved;
  if (LU_.defined()) LU__impl_saved = LU_.getIntrusivePtr();
  c10::optional<Storage> pivots__storage_saved =
    pivots_.has_storage() ? c10::optional<Storage>(pivots_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> pivots__impl_saved;
  if (pivots_.defined()) pivots__impl_saved = pivots_.getIntrusivePtr();
  c10::optional<Storage> B__storage_saved =
    B_.has_storage() ? c10::optional<Storage>(B_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> B__impl_saved;
  if (B_.defined()) B__impl_saved = B_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::linalg_lu_solve(ks & c10::after_autograd_keyset, LU_, pivots_, B_, left, adjoint);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (LU__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(LU_))
    TORCH_INTERNAL_ASSERT(LU__storage_saved.value().is_alias_of(LU_.storage()));
  if (LU__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU_))
    TORCH_INTERNAL_ASSERT(LU__impl_saved == LU_.getIntrusivePtr());
  if (pivots__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(pivots_))
    TORCH_INTERNAL_ASSERT(pivots__storage_saved.value().is_alias_of(pivots_.storage()));
  if (pivots__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots_))
    TORCH_INTERNAL_ASSERT(pivots__impl_saved == pivots_.getIntrusivePtr());
  if (B__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__storage_saved.value().is_alias_of(B_.storage()));
  if (B__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__impl_saved == B_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: linalg_lu_solve");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: linalg_lu_solve");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto LU_t_raw = toNonOptFwGrad(LU);
      auto LU_tensor = toNonOptTensor(LU);
      auto LU_t = (LU_t_raw.defined() || !LU_tensor.defined())
        ? LU_t_raw : at::_efficientzerotensor(LU_tensor.sizes(), LU_tensor.options());
      auto LU_p = toNonOptPrimal(LU);
      auto B_t_raw = toNonOptFwGrad(B);
      auto B_tensor = toNonOptTensor(B);
      auto B_t = (B_t_raw.defined() || !B_tensor.defined())
        ? B_t_raw : at::_efficientzerotensor(B_tensor.sizes(), B_tensor.options());
      result_new_fw_grad_opt = linalg_lu_solve_jvp(result, LU_p, pivots, LU_t, B_t, left, adjoint);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & linalg_lu_solve_out_out(c10::DispatchKeySet ks, const at::Tensor & LU, const at::Tensor & pivots, const at::Tensor & B, bool left, bool adjoint, at::Tensor & out) {
  auto& LU_ = unpack(LU, "LU", 0);
  auto& pivots_ = unpack(pivots, "pivots", 1);
  auto& B_ = unpack(B, "B", 2);
  auto& out_ = unpack(out, "out", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( LU, B );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(LU) || isFwGradDefined(B));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( LU, pivots, B )) {
    throw_error_out_requires_grad("linalg_lu_solve");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("linalg_lu_solve");
  }
  #ifndef NDEBUG
  c10::optional<Storage> LU__storage_saved =
    LU_.has_storage() ? c10::optional<Storage>(LU_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> LU__impl_saved;
  if (LU_.defined()) LU__impl_saved = LU_.getIntrusivePtr();
  c10::optional<Storage> pivots__storage_saved =
    pivots_.has_storage() ? c10::optional<Storage>(pivots_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> pivots__impl_saved;
  if (pivots_.defined()) pivots__impl_saved = pivots_.getIntrusivePtr();
  c10::optional<Storage> B__storage_saved =
    B_.has_storage() ? c10::optional<Storage>(B_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> B__impl_saved;
  if (B_.defined()) B__impl_saved = B_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::linalg_lu_solve_outf(ks & c10::after_autograd_keyset, LU_, pivots_, B_, left, adjoint, out_);
  }
  #ifndef NDEBUG
  if (LU__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(LU_))
    TORCH_INTERNAL_ASSERT(LU__storage_saved.value().is_alias_of(LU_.storage()));
  if (LU__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU_))
    TORCH_INTERNAL_ASSERT(LU__impl_saved == LU_.getIntrusivePtr());
  if (pivots__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(pivots_))
    TORCH_INTERNAL_ASSERT(pivots__storage_saved.value().is_alias_of(pivots_.storage()));
  if (pivots__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(pivots_))
    TORCH_INTERNAL_ASSERT(pivots__impl_saved == pivots_.getIntrusivePtr());
  if (B__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__storage_saved.value().is_alias_of(B_.storage()));
  if (B__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(B_))
    TORCH_INTERNAL_ASSERT(B__impl_saved == B_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(LU) || isFwGradDefined(pivots) || isFwGradDefined(B) || isFwGradDefined(out))), "Trying to use forward AD with linalg_lu_solve_out that does not support it because it is an out= function");
  return out;
}
::std::tuple<at::Tensor,at::Tensor> linalg_qr(c10::DispatchKeySet ks, const at::Tensor & A, c10::string_view mode) {
  auto& A_ = unpack(A, "A", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_Q_R = (isFwGradDefined(A));
  std::shared_ptr<LinalgQrBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgQrBackward0>(new LinalgQrBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( A ));
    grad_fn->mode = std::string(mode);
  }
  at::Tensor Q;
  at::Tensor R;
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::linalg_qr(ks & c10::after_autograd_keyset, A_, mode);
  })();
  std::tie(Q, R) = std::move(_tmp);
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (Q.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(Q)) {
    TORCH_INTERNAL_ASSERT(Q.storage().use_count() == 1, "function: linalg_qr");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(Q))
    TORCH_INTERNAL_ASSERT(Q.use_count() <= 1, "function: linalg_qr");
  if (R.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(R)) {
    TORCH_INTERNAL_ASSERT(R.storage().use_count() == 1, "function: linalg_qr");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(R))
    TORCH_INTERNAL_ASSERT(R.use_count() <= 1, "function: linalg_qr");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( Q, R ), grad_fn);
  }
  c10::optional<::std::tuple<at::Tensor,at::Tensor>> Q_R_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_Q_R) {
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      Q_R_new_fw_grad_opt = linalg_qr_jvp(A_t, Q, R, mode);
  }
  if (Q_R_new_fw_grad_opt.has_value() && std::get<0>(Q_R_new_fw_grad_opt.value()).defined()
      && Q.defined()) {
    Q._set_fw_grad(std::get<0>(Q_R_new_fw_grad_opt.value()), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (Q_R_new_fw_grad_opt.has_value() && std::get<1>(Q_R_new_fw_grad_opt.value()).defined()
      && R.defined()) {
    R._set_fw_grad(std::get<1>(Q_R_new_fw_grad_opt.value()), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->Q_ = SavedVariable(Q, true);
    grad_fn->R_ = SavedVariable(R, true);
  }
  return std::make_tuple(std::move(Q), std::move(R));
}
at::Tensor & linalg_vector_norm_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("linalg_vector_norm");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("linalg_vector_norm");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::linalg_vector_norm_outf(ks & c10::after_autograd_keyset, self_, ord, dim, keepdim, dtype, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with linalg_vector_norm_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & log1p_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<Log1PBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Log1PBackward0>(new Log1PBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::log1p_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((log1p_backward(original_self_t.conj(), original_self_p)).conj()) : (log1p_backward(original_self_t.conj(), original_self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & log2_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<Log2Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Log2Backward0>(new Log2Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::log2_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() / (original_self_p.conj() * 0.6931471805599453)).conj()) : (original_self_t.conj() / (original_self_p.conj() * 0.6931471805599453)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & log2_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("log2");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("log2");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::log2_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with log2_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & log_normal_(c10::DispatchKeySet ks, at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<LogNormalBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LogNormalBackward0>(new LogNormalBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::log_normal_(ks & c10::after_autograd_keyset, self_, mean, std, generator);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor logical_xor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::logical_xor(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: logical_xor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: logical_xor");
  #endif
  return result;
}
at::Tensor & logit_(c10::DispatchKeySet ks, at::Tensor & self, c10::optional<double> eps) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<LogitBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LogitBackward0>(new LogitBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->eps = eps;
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::logit_(ks & c10::after_autograd_keyset, self_, eps);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((GradMode::is_enabled() ? infinitely_differentiable_logit_backward(original_self_t.conj(), original_self_p, eps) : logit_backward(original_self_t.conj(), original_self_p, eps)).conj()) : (GradMode::is_enabled() ? infinitely_differentiable_logit_backward(original_self_t.conj(), original_self_p, eps) : logit_backward(original_self_t.conj(), original_self_p, eps)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & logit_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("logit");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("logit");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::logit_outf(ks & c10::after_autograd_keyset, self_, eps, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with logit_out that does not support it because it is an out= function");
  return out;
}
void lstm_mps_backward_out_out(c10::DispatchKeySet ks, const c10::optional<at::Tensor> & grad_y, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & z_state, const at::Tensor & cell_state_fwd, const at::Tensor & input, const at::Tensor & layersOutputs, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first, at::Tensor & out0, at::TensorList out1, at::TensorList out2) {
  auto& z_state_ = unpack(z_state, "z_state", 3);
  auto& cell_state_fwd_ = unpack(cell_state_fwd, "cell_state_fwd", 4);
  auto& input_ = unpack(input, "input", 5);
  auto& layersOutputs_ = unpack(layersOutputs, "layersOutputs", 6);
  auto hx_ = unpack(hx, "hx", 7);
  auto params_ = unpack(params, "params", 8);
  auto& out0_ = unpack(out0, "out0", 15);
  auto out1_ = unpack(out1, "out1", 16);
  auto out2_ = unpack(out2, "out2", 17);
  #ifndef NDEBUG
  c10::optional<Storage> z_state__storage_saved =
    z_state_.has_storage() ? c10::optional<Storage>(z_state_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> z_state__impl_saved;
  if (z_state_.defined()) z_state__impl_saved = z_state_.getIntrusivePtr();
  c10::optional<Storage> cell_state_fwd__storage_saved =
    cell_state_fwd_.has_storage() ? c10::optional<Storage>(cell_state_fwd_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> cell_state_fwd__impl_saved;
  if (cell_state_fwd_.defined()) cell_state_fwd__impl_saved = cell_state_fwd_.getIntrusivePtr();
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> layersOutputs__storage_saved =
    layersOutputs_.has_storage() ? c10::optional<Storage>(layersOutputs_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> layersOutputs__impl_saved;
  if (layersOutputs_.defined()) layersOutputs__impl_saved = layersOutputs_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> hx__storage_saved(hx_.size());
  for (const Tensor& tensor : hx_)
    hx__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> hx__impl_saved(hx_.size());
  for (size_t i=0; i<hx_.size(); i++)
    if (hx_[i].defined()) hx__impl_saved[i] = hx_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> params__storage_saved(params_.size());
  for (const Tensor& tensor : params_)
    params__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> params__impl_saved(params_.size());
  for (size_t i=0; i<params_.size(); i++)
    if (params_[i].defined()) params__impl_saved[i] = params_[i].getIntrusivePtr();
  c10::optional<Storage> out0__storage_saved =
    out0_.has_storage() ? c10::optional<Storage>(out0_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out0__impl_saved;
  if (out0_.defined()) out0__impl_saved = out0_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out1__storage_saved(out1_.size());
  for (const Tensor& tensor : out1_)
    out1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out1__impl_saved(out1_.size());
  for (size_t i=0; i<out1_.size(); i++)
    if (out1_[i].defined()) out1__impl_saved[i] = out1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out2__storage_saved(out2_.size());
  for (const Tensor& tensor : out2_)
    out2__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out2__impl_saved(out2_.size());
  for (size_t i=0; i<out2_.size(); i++)
    if (out2_[i].defined()) out2__impl_saved[i] = out2_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::lstm_mps_backward_outf(ks & c10::after_autograd_keyset, grad_y, grad_hy, grad_cy, z_state_, cell_state_fwd_, input_, layersOutputs_, hx_, params_, has_biases, num_layers, dropout, train, bidirectional, batch_first, out0_, out1_, out2_);
  }
  #ifndef NDEBUG
  if (z_state__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(z_state_))
    TORCH_INTERNAL_ASSERT(z_state__storage_saved.value().is_alias_of(z_state_.storage()));
  if (z_state__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(z_state_))
    TORCH_INTERNAL_ASSERT(z_state__impl_saved == z_state_.getIntrusivePtr());
  if (cell_state_fwd__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(cell_state_fwd_))
    TORCH_INTERNAL_ASSERT(cell_state_fwd__storage_saved.value().is_alias_of(cell_state_fwd_.storage()));
  if (cell_state_fwd__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(cell_state_fwd_))
    TORCH_INTERNAL_ASSERT(cell_state_fwd__impl_saved == cell_state_fwd_.getIntrusivePtr());
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (layersOutputs__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(layersOutputs_))
    TORCH_INTERNAL_ASSERT(layersOutputs__storage_saved.value().is_alias_of(layersOutputs_.storage()));
  if (layersOutputs__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(layersOutputs_))
    TORCH_INTERNAL_ASSERT(layersOutputs__impl_saved == layersOutputs_.getIntrusivePtr());
  for (size_t i=0; i<hx_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (hx__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(hx_))
      TORCH_INTERNAL_ASSERT(hx__storage_saved[i].value().is_alias_of(hx_[i].storage()));
  }
  for (size_t i=0; i<hx_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (hx__impl_saved[i] && !at::impl::tensorlist_has_dispatch(hx_))
      TORCH_INTERNAL_ASSERT(hx__impl_saved[i] == hx_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<params_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (params__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(params_))
      TORCH_INTERNAL_ASSERT(params__storage_saved[i].value().is_alias_of(params_[i].storage()));
  }
  for (size_t i=0; i<params_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (params__impl_saved[i] && !at::impl::tensorlist_has_dispatch(params_))
      TORCH_INTERNAL_ASSERT(params__impl_saved[i] == params_[i].getIntrusivePtr());
  }
  if (out0__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out0_))
    TORCH_INTERNAL_ASSERT(out0__storage_saved.value().is_alias_of(out0_.storage()));
  if (out0__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out0_))
    TORCH_INTERNAL_ASSERT(out0__impl_saved == out0_.getIntrusivePtr());
  for (size_t i=0; i<out1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out1_))
      TORCH_INTERNAL_ASSERT(out1__storage_saved[i].value().is_alias_of(out1_[i].storage()));
  }
  for (size_t i=0; i<out1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out1_))
      TORCH_INTERNAL_ASSERT(out1__impl_saved[i] == out1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out2__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out2_))
      TORCH_INTERNAL_ASSERT(out2__storage_saved[i].value().is_alias_of(out2_[i].storage()));
  }
  for (size_t i=0; i<out2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out2__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out2_))
      TORCH_INTERNAL_ASSERT(out2__impl_saved[i] == out2_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_y) || isFwGradDefined(grad_hy) || isFwGradDefined(grad_cy) || isFwGradDefined(z_state) || isFwGradDefined(cell_state_fwd) || isFwGradDefined(input) || isFwGradDefined(layersOutputs) || isFwGradDefinedTensorList(hx) || isFwGradDefinedTensorList(params) || isFwGradDefined(out0) || isFwGradDefinedTensorList(out1) || isFwGradDefinedTensorList(out2))), "Trying to use forward AD with lstm_mps_backward_out that does not support it because it is an out= function");
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> lu_unpack_out_out(c10::DispatchKeySet ks, const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots, at::Tensor & P, at::Tensor & L, at::Tensor & U) {
  auto& LU_data_ = unpack(LU_data, "LU_data", 0);
  auto& LU_pivots_ = unpack(LU_pivots, "LU_pivots", 1);
  auto& P_ = unpack(P, "P", 4);
  auto& L_ = unpack(L, "L", 5);
  auto& U_ = unpack(U, "U", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( LU_data );
  
  [[maybe_unused]] auto _any_has_forward_grad_L = (isFwGradDefined(LU_data));
  [[maybe_unused]] auto _any_has_forward_grad_U = (isFwGradDefined(LU_data));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( LU_data )) {
    throw_error_out_requires_grad("lu_unpack");
  }
  if (compute_requires_grad( L, U )) {
    throw_error_out_requires_grad("lu_unpack");
  }
  #ifndef NDEBUG
  c10::optional<Storage> LU_data__storage_saved =
    LU_data_.has_storage() ? c10::optional<Storage>(LU_data_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> LU_data__impl_saved;
  if (LU_data_.defined()) LU_data__impl_saved = LU_data_.getIntrusivePtr();
  c10::optional<Storage> LU_pivots__storage_saved =
    LU_pivots_.has_storage() ? c10::optional<Storage>(LU_pivots_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> LU_pivots__impl_saved;
  if (LU_pivots_.defined()) LU_pivots__impl_saved = LU_pivots_.getIntrusivePtr();
  c10::optional<Storage> P__storage_saved =
    P_.has_storage() ? c10::optional<Storage>(P_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> P__impl_saved;
  if (P_.defined()) P__impl_saved = P_.getIntrusivePtr();
  c10::optional<Storage> L__storage_saved =
    L_.has_storage() ? c10::optional<Storage>(L_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> L__impl_saved;
  if (L_.defined()) L__impl_saved = L_.getIntrusivePtr();
  c10::optional<Storage> U__storage_saved =
    U_.has_storage() ? c10::optional<Storage>(U_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> U__impl_saved;
  if (U_.defined()) U__impl_saved = U_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::lu_unpack_outf(ks & c10::after_autograd_keyset, LU_data_, LU_pivots_, unpack_data, unpack_pivots, P_, L_, U_);
  }
  #ifndef NDEBUG
  if (LU_data__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(LU_data_))
    TORCH_INTERNAL_ASSERT(LU_data__storage_saved.value().is_alias_of(LU_data_.storage()));
  if (LU_data__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU_data_))
    TORCH_INTERNAL_ASSERT(LU_data__impl_saved == LU_data_.getIntrusivePtr());
  if (LU_pivots__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(LU_pivots_))
    TORCH_INTERNAL_ASSERT(LU_pivots__storage_saved.value().is_alias_of(LU_pivots_.storage()));
  if (LU_pivots__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU_pivots_))
    TORCH_INTERNAL_ASSERT(LU_pivots__impl_saved == LU_pivots_.getIntrusivePtr());
  if (P__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(P_))
    TORCH_INTERNAL_ASSERT(P__storage_saved.value().is_alias_of(P_.storage()));
  if (P__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(P_))
    TORCH_INTERNAL_ASSERT(P__impl_saved == P_.getIntrusivePtr());
  if (L__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(L_))
    TORCH_INTERNAL_ASSERT(L__storage_saved.value().is_alias_of(L_.storage()));
  if (L__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(L_))
    TORCH_INTERNAL_ASSERT(L__impl_saved == L_.getIntrusivePtr());
  if (U__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(U_))
    TORCH_INTERNAL_ASSERT(U__storage_saved.value().is_alias_of(U_.storage()));
  if (U__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(U_))
    TORCH_INTERNAL_ASSERT(U__impl_saved == U_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( L, U ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(LU_data) || isFwGradDefined(P) || isFwGradDefined(L) || isFwGradDefined(U))), "Trying to use forward AD with lu_unpack_out that does not support it because it is an out= function");
  return std::forward_as_tuple(P, L, U);
}
at::Tensor masked_fill_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
  auto& self_ = unpack(self, "self", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<MaskedFillBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaskedFillBackward0>(new MaskedFillBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->mask_ = SavedVariable(mask, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::masked_fill(ks & c10::after_autograd_keyset, self_, mask_, value);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: masked_fill_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: masked_fill_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t.masked_fill(mask, 0);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor masked_fill_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
  auto& self_ = unpack(self, "self", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  auto& value_ = unpack(value, "value", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, value );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(value));
  std::shared_ptr<MaskedFillBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaskedFillBackward1>(new MaskedFillBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, value ));
    grad_fn->mask_ = SavedVariable(mask, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  c10::optional<Storage> value__storage_saved =
    value_.has_storage() ? c10::optional<Storage>(value_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> value__impl_saved;
  if (value_.defined()) value__impl_saved = value_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::masked_fill(ks & c10::after_autograd_keyset, self_, mask_, value_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (value__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__storage_saved.value().is_alias_of(value_.storage()));
  if (value__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__impl_saved == value_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: masked_fill_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: masked_fill_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto value_t_raw = toNonOptFwGrad(value);
      auto value_tensor = toNonOptTensor(value);
      auto value_t = (value_t_raw.defined() || !value_tensor.defined())
        ? value_t_raw : at::_efficientzerotensor(value_tensor.sizes(), value_tensor.options());
      result_new_fw_grad_opt = self_t.masked_fill(mask, value_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor masked_scatter(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
  auto& self_ = unpack(self, "self", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  auto& source_ = unpack(source, "source", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, source );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(source));
  std::shared_ptr<MaskedScatterBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaskedScatterBackward0>(new MaskedScatterBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, source ));
    grad_fn->mask_ = SavedVariable(mask, false);
    grad_fn->source_sym_sizes = source.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  c10::optional<Storage> source__storage_saved =
    source_.has_storage() ? c10::optional<Storage>(source_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> source__impl_saved;
  if (source_.defined()) source__impl_saved = source_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::masked_scatter(ks & c10::after_autograd_keyset, self_, mask_, source_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (source__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__storage_saved.value().is_alias_of(source_.storage()));
  if (source__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__impl_saved == source_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: masked_scatter");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: masked_scatter");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto source_t_raw = toNonOptFwGrad(source);
      auto source_tensor = toNonOptTensor(source);
      auto source_t = (source_t_raw.defined() || !source_tensor.defined())
        ? source_t_raw : at::_efficientzerotensor(source_tensor.sizes(), source_tensor.options());
      result_new_fw_grad_opt = self_t.masked_scatter(mask, source_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & masked_scatter_(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
  auto& self_ = unpack(self, "self", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  auto& source_ = unpack(source, "source", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, source );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(source));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<MaskedScatterBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaskedScatterBackward0>(new MaskedScatterBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, source ));
    grad_fn->mask_ = SavedVariable(mask, false);
    grad_fn->source_sym_sizes = source.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  c10::optional<Storage> source__storage_saved =
    source_.has_storage() ? c10::optional<Storage>(source_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> source__impl_saved;
  if (source_.defined()) source__impl_saved = source_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::masked_scatter_(ks & c10::after_autograd_keyset, self_, mask_, source_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (source__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__storage_saved.value().is_alias_of(source_.storage()));
  if (source__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__impl_saved == source_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto source_t_raw = toNonOptFwGrad(source);
      auto source_tensor = toNonOptTensor(source);
      auto source_t = (source_t_raw.defined() || !source_tensor.defined())
        ? source_t_raw : at::_efficientzerotensor(source_tensor.sizes(), source_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.masked_scatter_(mask, source_t) : self_t.masked_scatter(mask, source_t);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor matmul(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  std::shared_ptr<MatmulBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MatmulBackward0>(new MatmulBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(other))) {
      static c10::OperatorName full_name("aten::matmul", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("matmul", *opt_op, ks, self, other);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::matmul(ks & c10::after_autograd_keyset, self_, other_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: matmul");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: matmul");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "matmul");
  return result;
}
at::Tensor max_pool2d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<MaxPool2DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaxPool2DBackwardBackward0>(new MaxPool2DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::max_pool2d_backward(ks & c10::after_autograd_keyset, grad_output_, self_, kernel_size, stride, padding, dilation, ceil_mode);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: max_pool2d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: max_pool2d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "max_pool2d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::max_pool2d_backward(grad_output_t, self_t, kernel_size, stride, padding, dilation, ceil_mode);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor max_pool2d_with_indices_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& indices_ = unpack(indices, "indices", 7);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<MaxPool2DWithIndicesBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaxPool2DWithIndicesBackwardBackward0>(new MaxPool2DWithIndicesBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->indices_ = SavedVariable(indices, false);
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::max_pool2d_with_indices_backward(ks & c10::after_autograd_keyset, grad_output_, self_, kernel_size, stride, padding, dilation, ceil_mode, indices_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: max_pool2d_with_indices_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: max_pool2d_with_indices_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "max_pool2d_with_indices_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::max_pool2d_with_indices_backward(grad_output_t, self_t, kernel_size, stride, padding, dilation, ceil_mode, indices);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & max_pool2d_with_indices_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& indices_ = unpack(indices, "indices", 7);
  auto& grad_input_ = unpack(grad_input, "grad_input", 8);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self )) {
    throw_error_out_requires_grad("max_pool2d_with_indices_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("max_pool2d_with_indices_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::max_pool2d_with_indices_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, kernel_size, stride, padding, dilation, ceil_mode, indices_, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with max_pool2d_with_indices_backward_out that does not support it because it is an out= function");
  return grad_input;
}
::std::tuple<at::Tensor,at::Tensor> max_pool3d_with_indices(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(self));
  std::shared_ptr<MaxPool3DWithIndicesBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaxPool3DWithIndicesBackward0>(new MaxPool3DWithIndicesBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->ceil_mode = ceil_mode;
    grad_fn->dilation = dilation.vec();
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::max_pool3d_with_indices(ks & c10::after_autograd_keyset, self_, kernel_size, stride, padding, dilation, ceil_mode);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: max_pool3d_with_indices");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: max_pool3d_with_indices");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: max_pool3d_with_indices");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: max_pool3d_with_indices");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "max_pool3d_with_indices");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result0_new_fw_grad_opt = gather(self_t.flatten(-3), -1, result1.flatten(-3)).view_as(result1);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor & max_pool3d_with_indices_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& indices_ = unpack(indices, "indices", 7);
  auto& grad_input_ = unpack(grad_input, "grad_input", 8);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self )) {
    throw_error_out_requires_grad("max_pool3d_with_indices_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("max_pool3d_with_indices_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::max_pool3d_with_indices_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, kernel_size, stride, padding, dilation, ceil_mode, indices_, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with max_pool3d_with_indices_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & mean_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("mean");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("mean");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::mean_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, dtype, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with mean_out that does not support it because it is an out= function");
  return out;
}
at::Tensor median(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<MedianBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MedianBackward0>(new MedianBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::median(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: median");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: median");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "median");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = evenly_read_jvp(self_t, self_p, result);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor> median_dim(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<MedianBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MedianBackward1>(new MedianBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  at::Tensor values;
  at::Tensor indices;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::median(ks & c10::after_autograd_keyset, self_, dim, keepdim);
  })();
  std::tie(values, indices) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values)) {
    TORCH_INTERNAL_ASSERT(values.storage().use_count() == 1, "function: median_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values))
    TORCH_INTERNAL_ASSERT(values.use_count() <= 1, "function: median_dim");
  if (indices.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices)) {
    TORCH_INTERNAL_ASSERT(indices.storage().use_count() == 1, "function: median_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices))
    TORCH_INTERNAL_ASSERT(indices.use_count() <= 1, "function: median_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( values ), grad_fn);
  }
  throw_error_for_complex_autograd(values, "median");
  c10::optional<at::Tensor> values_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_values && (values.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      values_new_fw_grad_opt = gather_with_keepdimed_indices(self_t, dim, indices, keepdim);
  }
  if (values_new_fw_grad_opt.has_value() && values_new_fw_grad_opt.value().defined() && values.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    values._set_fw_grad(values_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->indices_ = SavedVariable(indices, true);
  }
  return std::make_tuple(std::move(values), std::move(indices));
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_batch_norm(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  check_no_requires_grad(running_mean, "running_mean", "miopen_batch_norm");
  check_no_requires_grad(running_var, "running_var", "miopen_batch_norm");
  std::shared_ptr<MiopenBatchNormBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MiopenBatchNormBackward0>(new MiopenBatchNormBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->epsilon = epsilon;
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->running_mean_ = SavedVariable(running_mean, false);
    grad_fn->running_var_ = SavedVariable(running_var, false);
    grad_fn->training = training;
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias) || isFwGradDefined(running_mean) || isFwGradDefined(running_var))) {
      static c10::OperatorName full_name("aten::miopen_batch_norm", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor>>("miopen_batch_norm", *opt_op, ks, input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::miopen_batch_norm(ks & c10::after_autograd_keyset, input_, weight_, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
    }
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: miopen_batch_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: miopen_batch_norm");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: miopen_batch_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: miopen_batch_norm");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: miopen_batch_norm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: miopen_batch_norm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "miopen_batch_norm");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
at::Tensor miopen_convolution_transpose(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<MiopenConvolutionTransposeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MiopenConvolutionTransposeBackward0>(new MiopenConvolutionTransposeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight, bias ));
    grad_fn->bias_sym_sizes_opt = bias.has_value() ? c10::optional<c10::SymIntArrayRef>(bias->sym_sizes()) : c10::nullopt;
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
    grad_fn->output_padding = output_padding.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::miopen_convolution_transpose", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("miopen_convolution_transpose", *opt_op, ks, self, weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::miopen_convolution_transpose_symint(ks & c10::after_autograd_keyset, self_, weight_, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: miopen_convolution_transpose");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: miopen_convolution_transpose");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "miopen_convolution_transpose");
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,::std::vector<at::Tensor>> miopen_rnn_backward(c10::DispatchKeySet ks, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask) {
  auto& input_ = unpack(input, "input", 0);
  auto weight_ = unpack(weight, "weight", 1);
  auto& weight_buf_ = unpack(weight_buf, "weight_buf", 3);
  auto& hx_ = unpack(hx, "hx", 4);
  auto& output_ = unpack(output, "output", 6);
  auto& reserve_ = unpack(reserve, "reserve", 19);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, reserve );
  
  std::shared_ptr<NotImplemented> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("miopen_rnn_backward"), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, reserve ));
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  ::std::vector<at::Tensor> result3;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> weight__storage_saved(weight_.size());
  for (const Tensor& tensor : weight_)
    weight__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> weight__impl_saved(weight_.size());
  for (size_t i=0; i<weight_.size(); i++)
    if (weight_[i].defined()) weight__impl_saved[i] = weight_[i].getIntrusivePtr();
  c10::optional<Storage> weight_buf__storage_saved =
    weight_buf_.has_storage() ? c10::optional<Storage>(weight_buf_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight_buf__impl_saved;
  if (weight_buf_.defined()) weight_buf__impl_saved = weight_buf_.getIntrusivePtr();
  c10::optional<Storage> hx__storage_saved =
    hx_.has_storage() ? c10::optional<Storage>(hx_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> hx__impl_saved;
  if (hx_.defined()) hx__impl_saved = hx_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> reserve__storage_saved =
    reserve_.has_storage() ? c10::optional<Storage>(reserve_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> reserve__impl_saved;
  if (reserve_.defined()) reserve__impl_saved = reserve_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input) || isFwGradDefinedTensorList(weight) || isFwGradDefined(weight_buf) || isFwGradDefined(hx) || isFwGradDefined(cx) || isFwGradDefined(output) || isFwGradDefined(grad_output) || isFwGradDefined(grad_hy) || isFwGradDefined(grad_cy) || isFwGradDefined(reserve))) {
      static c10::OperatorName full_name("aten::miopen_rnn_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor, ::std::vector<at::Tensor>>>("miopen_rnn_backward", *opt_op, ks, input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::miopen_rnn_backward(ks & c10::after_autograd_keyset, input_, weight_, weight_stride0, weight_buf_, hx_, cx, output_, grad_output, grad_hy, grad_cy, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve_, output_mask);
    }
  })();
  std::tie(result0, result1, result2, result3) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  for (size_t i=0; i<weight_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weight__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(weight_))
      TORCH_INTERNAL_ASSERT(weight__storage_saved[i].value().is_alias_of(weight_[i].storage()));
  }
  for (size_t i=0; i<weight_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weight__impl_saved[i] && !at::impl::tensorlist_has_dispatch(weight_))
      TORCH_INTERNAL_ASSERT(weight__impl_saved[i] == weight_[i].getIntrusivePtr());
  }
  if (weight_buf__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_buf_))
    TORCH_INTERNAL_ASSERT(weight_buf__storage_saved.value().is_alias_of(weight_buf_.storage()));
  if (weight_buf__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_buf_))
    TORCH_INTERNAL_ASSERT(weight_buf__impl_saved == weight_buf_.getIntrusivePtr());
  if (hx__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__storage_saved.value().is_alias_of(hx_.storage()));
  if (hx__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__impl_saved == hx_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (reserve__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(reserve_))
    TORCH_INTERNAL_ASSERT(reserve__storage_saved.value().is_alias_of(reserve_.storage()));
  if (reserve__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(reserve_))
    TORCH_INTERNAL_ASSERT(reserve__impl_saved == reserve_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: miopen_rnn_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: miopen_rnn_backward");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: miopen_rnn_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: miopen_rnn_backward");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: miopen_rnn_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: miopen_rnn_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2, result3 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "miopen_rnn_backward");
  throw_error_for_complex_autograd(result1, "miopen_rnn_backward");
  throw_error_for_complex_autograd(result2, "miopen_rnn_backward");
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3));
}
at::Tensor mkldnn_adaptive_avg_pool2d(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef output_size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<MkldnnAdaptiveAvgPool2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MkldnnAdaptiveAvgPool2DBackward0>(new MkldnnAdaptiveAvgPool2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::mkldnn_adaptive_avg_pool2d", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("mkldnn_adaptive_avg_pool2d", *opt_op, ks, self, output_size);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::mkldnn_adaptive_avg_pool2d(ks & c10::after_autograd_keyset, self_, output_size);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mkldnn_adaptive_avg_pool2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mkldnn_adaptive_avg_pool2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "mkldnn_adaptive_avg_pool2d");
  return result;
}
at::Tensor & nan_to_num_(c10::DispatchKeySet ks, at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<NanToNumBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NanToNumBackward0>(new NanToNumBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::nan_to_num_(ks & c10::after_autograd_keyset, self_, nan, posinf, neginf);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * at::isfinite(original_self_p)).conj()) : (original_self_t.conj() * at::isfinite(original_self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor nanmedian(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<NanmedianBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NanmedianBackward0>(new NanmedianBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::nanmedian(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: nanmedian");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: nanmedian");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "nanmedian");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = evenly_read_jvp(self_t, self_p, result);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor> nanmedian_dim(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<NanmedianBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NanmedianBackward1>(new NanmedianBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  at::Tensor values;
  at::Tensor indices;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::nanmedian(ks & c10::after_autograd_keyset, self_, dim, keepdim);
  })();
  std::tie(values, indices) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values)) {
    TORCH_INTERNAL_ASSERT(values.storage().use_count() == 1, "function: nanmedian_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values))
    TORCH_INTERNAL_ASSERT(values.use_count() <= 1, "function: nanmedian_dim");
  if (indices.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices)) {
    TORCH_INTERNAL_ASSERT(indices.storage().use_count() == 1, "function: nanmedian_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices))
    TORCH_INTERNAL_ASSERT(indices.use_count() <= 1, "function: nanmedian_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( values ), grad_fn);
  }
  throw_error_for_complex_autograd(values, "nanmedian");
  c10::optional<at::Tensor> values_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_values && (values.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      values_new_fw_grad_opt = gather_with_keepdimed_indices(self_t, dim, indices, keepdim);
  }
  if (values_new_fw_grad_opt.has_value() && values_new_fw_grad_opt.value().defined() && values.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    values._set_fw_grad(values_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->indices_ = SavedVariable(indices, true);
  }
  return std::make_tuple(std::move(values), std::move(indices));
}
at::Tensor & neg_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<NegBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NegBackward0>(new NegBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::neg_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((self_t.conj().neg()).conj()) : (self_t.conj().neg()).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
::std::tuple<at::Tensor &,at::Tensor &> nll_loss2d_forward_out_output(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & output, at::Tensor & total_weight) {
  auto& self_ = unpack(self, "self", 0);
  auto& target_ = unpack(target, "target", 1);
  auto& output_ = unpack(output, "output", 5);
  auto& total_weight_ = unpack(total_weight, "total_weight", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_output = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, weight )) {
    throw_error_out_requires_grad("nll_loss2d_forward");
  }
  if (compute_requires_grad( output )) {
    throw_error_out_requires_grad("nll_loss2d_forward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> total_weight__storage_saved =
    total_weight_.has_storage() ? c10::optional<Storage>(total_weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> total_weight__impl_saved;
  if (total_weight_.defined()) total_weight__impl_saved = total_weight_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::nll_loss2d_forward_symint_outf(ks & c10::after_autograd_keyset, self_, target_, weight, reduction, ignore_index, output_, total_weight_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (total_weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(total_weight_))
    TORCH_INTERNAL_ASSERT(total_weight__storage_saved.value().is_alias_of(total_weight_.storage()));
  if (total_weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(total_weight_))
    TORCH_INTERNAL_ASSERT(total_weight__impl_saved == total_weight_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( output ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(output) || isFwGradDefined(total_weight))), "Trying to use forward AD with nll_loss2d_forward_out that does not support it because it is an out= function");
  return std::forward_as_tuple(output, total_weight);
}
at::Tensor & nll_loss_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& target_ = unpack(target, "target", 2);
  auto& total_weight_ = unpack(total_weight, "total_weight", 6);
  auto& grad_input_ = unpack(grad_input, "grad_input", 7);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self, weight, total_weight )) {
    throw_error_out_requires_grad("nll_loss_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("nll_loss_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> total_weight__storage_saved =
    total_weight_.has_storage() ? c10::optional<Storage>(total_weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> total_weight__impl_saved;
  if (total_weight_.defined()) total_weight__impl_saved = total_weight_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::nll_loss_backward_symint_outf(ks & c10::after_autograd_keyset, grad_output_, self_, target_, weight, reduction, ignore_index, total_weight_, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (total_weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(total_weight_))
    TORCH_INTERNAL_ASSERT(total_weight__storage_saved.value().is_alias_of(total_weight_.storage()));
  if (total_weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(total_weight_))
    TORCH_INTERNAL_ASSERT(total_weight__impl_saved == total_weight_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(total_weight) || isFwGradDefined(grad_input))), "Trying to use forward AD with nll_loss_backward_out that does not support it because it is an out= function");
  return grad_input;
}
::std::tuple<at::Tensor &,at::Tensor &> nll_loss_forward_out_output(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & output, at::Tensor & total_weight) {
  auto& self_ = unpack(self, "self", 0);
  auto& target_ = unpack(target, "target", 1);
  auto& output_ = unpack(output, "output", 5);
  auto& total_weight_ = unpack(total_weight, "total_weight", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_output = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, weight )) {
    throw_error_out_requires_grad("nll_loss_forward");
  }
  if (compute_requires_grad( output )) {
    throw_error_out_requires_grad("nll_loss_forward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> total_weight__storage_saved =
    total_weight_.has_storage() ? c10::optional<Storage>(total_weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> total_weight__impl_saved;
  if (total_weight_.defined()) total_weight__impl_saved = total_weight_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::nll_loss_forward_symint_outf(ks & c10::after_autograd_keyset, self_, target_, weight, reduction, ignore_index, output_, total_weight_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (total_weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(total_weight_))
    TORCH_INTERNAL_ASSERT(total_weight__storage_saved.value().is_alias_of(total_weight_.storage()));
  if (total_weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(total_weight_))
    TORCH_INTERNAL_ASSERT(total_weight__impl_saved == total_weight_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( output ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(output) || isFwGradDefined(total_weight))), "Trying to use forward AD with nll_loss_forward_out that does not support it because it is an out= function");
  return std::forward_as_tuple(output, total_weight);
}
at::Tensor permute_copy(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dims) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<PermuteBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PermuteBackward0_copy>(new PermuteBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dims = dims.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::permute_copy(ks & c10::after_autograd_keyset, self_, dims);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: permute_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: permute_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "permute_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::permute(self_t, dims);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor poisson(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<PoissonBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PoissonBackward0>(new PoissonBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::poisson(ks & c10::after_autograd_keyset, self_, generator);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: poisson");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: poisson");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "poisson");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (zeros_like(self_p)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & pow_out_Tensor_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& exponent_ = unpack(exponent, "exponent", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, exponent );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(exponent));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, exponent )) {
    throw_error_out_requires_grad("pow");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("pow");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> exponent__storage_saved =
    exponent_.has_storage() ? c10::optional<Storage>(exponent_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> exponent__impl_saved;
  if (exponent_.defined()) exponent__impl_saved = exponent_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::pow_outf(ks & c10::after_autograd_keyset, self_, exponent_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (exponent__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(exponent_))
    TORCH_INTERNAL_ASSERT(exponent__storage_saved.value().is_alias_of(exponent_.storage()));
  if (exponent__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(exponent_))
    TORCH_INTERNAL_ASSERT(exponent__impl_saved == exponent_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(exponent) || isFwGradDefined(out))), "Trying to use forward AD with pow_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & pow_out_Scalar_out(c10::DispatchKeySet ks, const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
  auto& exponent_ = unpack(exponent, "exponent", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( exponent );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(exponent));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( exponent )) {
    throw_error_out_requires_grad("pow");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("pow");
  }
  #ifndef NDEBUG
  c10::optional<Storage> exponent__storage_saved =
    exponent_.has_storage() ? c10::optional<Storage>(exponent_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> exponent__impl_saved;
  if (exponent_.defined()) exponent__impl_saved = exponent_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::pow_outf(ks & c10::after_autograd_keyset, self, exponent_, out_);
  }
  #ifndef NDEBUG
  if (exponent__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(exponent_))
    TORCH_INTERNAL_ASSERT(exponent__storage_saved.value().is_alias_of(exponent_.storage()));
  if (exponent__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(exponent_))
    TORCH_INTERNAL_ASSERT(exponent__impl_saved == exponent_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(exponent) || isFwGradDefined(out))), "Trying to use forward AD with pow_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & pow_out_Tensor_Scalar_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("pow");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("pow");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::pow_outf(ks & c10::after_autograd_keyset, self_, exponent, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with pow_out that does not support it because it is an out= function");
  return out;
}
at::Tensor prod(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ProdBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ProdBackward0>(new ProdBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::prod(ks & c10::after_autograd_keyset, self_, dtype);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: prod");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: prod");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (prod_backward(at::ones({}, result.options()).expand_as(result), self_p.to(result.scalar_type()), result) * self_t.conj()).sum().conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor prod_dim_int(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ProdBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ProdBackward1>(new ProdBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::prod(ks & c10::after_autograd_keyset, self_, dim, keepdim, dtype);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: prod_dim_int");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: prod_dim_int");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (prod_backward(at::ones({}, result.options()).expand_as(result), self_p.to(result.scalar_type()), result, dim, keepdim) * self_t.conj()).sum(dim, keepdim).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & put_(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 1);
  auto& source_ = unpack(source, "source", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, source );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(source));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<PutBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PutBackward0>(new PutBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, source ));
    grad_fn->accumulate = accumulate;
    grad_fn->index_ = SavedVariable(index, false);
    if (grad_fn->should_compute_output(1)) {
      grad_fn->source_ = SavedVariable(source, false);
    }
    grad_fn->source_info = source;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> source__storage_saved =
    source_.has_storage() ? c10::optional<Storage>(source_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> source__impl_saved;
  if (source_.defined()) source__impl_saved = source_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::put_(ks & c10::after_autograd_keyset, self_, index_, source_, accumulate);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (source__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__storage_saved.value().is_alias_of(source_.storage()));
  if (source__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__impl_saved == source_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto source_t_raw = toNonOptFwGrad(source);
      auto source_tensor = toNonOptTensor(source);
      auto source_t = (source_t_raw.defined() || !source_tensor.defined())
        ? source_t_raw : at::_efficientzerotensor(source_tensor.sizes(), source_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.put_(index, source_t, accumulate) : self_t.put(index, source_t, accumulate);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
int64_t q_per_channel_axis(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::q_per_channel_axis(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  return result;
}
double q_scale(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::q_scale(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor & rad2deg_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<Rad2DegBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Rad2DegBackward0>(new Rad2DegBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::rad2deg_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((rad2deg_backward(self_t.conj())).conj()) : (rad2deg_backward(self_t.conj())).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & rand_like_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::rand_like_outf(ks & c10::after_autograd_keyset, self_, memory_format, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with rand_like_out that does not support it because it is an out= function");
  return out;
}
at::Tensor reflection_pad2d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef padding) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ReflectionPad2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReflectionPad2DBackward0>(new ReflectionPad2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::reflection_pad2d_symint(ks & c10::after_autograd_keyset, self_, padding);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: reflection_pad2d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: reflection_pad2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::reflection_pad2d_symint(self_t, padding);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor relu(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ReluBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReluBackward0>(new ReluBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::relu(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: relu");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: relu");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "relu");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (threshold_backward(self_t.conj(), result, 0)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor remainder_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<RemainderBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RemainderBackward0>(new RemainderBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::remainder(ks & c10::after_autograd_keyset, self_, other);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: remainder_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: remainder_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "remainder");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (self_t.conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor remainder_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<RemainderBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RemainderBackward1>(new RemainderBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    if (grad_fn->should_compute_output(1)) {
      grad_fn->other_ = SavedVariable(other, false);
    }
    if (grad_fn->should_compute_output(1)) {
      grad_fn->self_ = SavedVariable(self, false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::remainder(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: remainder_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: remainder_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "remainder");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      result_new_fw_grad_opt = self_t - other_t * self_p.div(other_p, /*rounding_mode=*/"floor");
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor renorm(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<RenormBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RenormBackward0>(new RenormBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->maxnorm = maxnorm;
    grad_fn->p = p;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::renorm(ks & c10::after_autograd_keyset, self_, p, dim, maxnorm);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: renorm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: renorm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = renorm_jvp(self_p, self_t, p, dim, maxnorm);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & renorm_(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<RenormBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RenormBackward0>(new RenormBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->maxnorm = maxnorm;
    grad_fn->p = p;
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::renorm_(ks & c10::after_autograd_keyset, self_, p, dim, maxnorm);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(renorm_jvp(original_self_p, original_self_t, p, dim, maxnorm)) : renorm_jvp(original_self_p, original_self_t, p, dim, maxnorm);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & replication_pad3d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& grad_input_ = unpack(grad_input, "grad_input", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self )) {
    throw_error_out_requires_grad("replication_pad3d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("replication_pad3d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::replication_pad3d_backward_symint_outf(ks & c10::after_autograd_keyset, grad_output_, self_, padding, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with replication_pad3d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & round_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<RoundBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RoundBackward0>(new RoundBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::round_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((zeros_like(self_t.conj())).conj()) : (zeros_like(self_t.conj())).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & round__decimals(c10::DispatchKeySet ks, at::Tensor & self, int64_t decimals) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<RoundBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RoundBackward1>(new RoundBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::round_(ks & c10::after_autograd_keyset, self_, decimals);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((zeros_like(self_t.conj())).conj()) : (zeros_like(self_t.conj())).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & scatter_reduce_out_two_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, bool include_self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  auto& src_ = unpack(src, "src", 3);
  auto& out_ = unpack(out, "out", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, src );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(src));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, src )) {
    throw_error_out_requires_grad("scatter_reduce");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("scatter_reduce");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> src__storage_saved =
    src_.has_storage() ? c10::optional<Storage>(src_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> src__impl_saved;
  if (src_.defined()) src__impl_saved = src_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::scatter_reduce_outf(ks & c10::after_autograd_keyset, self_, dim, index_, src_, reduce, include_self, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (src__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__storage_saved.value().is_alias_of(src_.storage()));
  if (src__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__impl_saved == src_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(src) || isFwGradDefined(out))), "Trying to use forward AD with scatter_reduce_out that does not support it because it is an out= function");
  return out;
}
at::Tensor searchsorted_Tensor(c10::DispatchKeySet ks, const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right, c10::optional<c10::string_view> side, const c10::optional<at::Tensor> & sorter) {
  auto& sorted_sequence_ = unpack(sorted_sequence, "sorted_sequence", 0);
  auto& self_ = unpack(self, "self", 1);
  #ifndef NDEBUG
  c10::optional<Storage> sorted_sequence__storage_saved =
    sorted_sequence_.has_storage() ? c10::optional<Storage>(sorted_sequence_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> sorted_sequence__impl_saved;
  if (sorted_sequence_.defined()) sorted_sequence__impl_saved = sorted_sequence_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::searchsorted(ks & c10::after_autograd_keyset, sorted_sequence_, self_, out_int32, right, side, sorter);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (sorted_sequence__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(sorted_sequence_))
    TORCH_INTERNAL_ASSERT(sorted_sequence__storage_saved.value().is_alias_of(sorted_sequence_.storage()));
  if (sorted_sequence__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(sorted_sequence_))
    TORCH_INTERNAL_ASSERT(sorted_sequence__impl_saved == sorted_sequence_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: searchsorted_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: searchsorted_Tensor");
  #endif
  return result;
}
at::Tensor searchsorted_Scalar(c10::DispatchKeySet ks, const at::Tensor & sorted_sequence, const at::Scalar & self, bool out_int32, bool right, c10::optional<c10::string_view> side, const c10::optional<at::Tensor> & sorter) {
  auto& sorted_sequence_ = unpack(sorted_sequence, "sorted_sequence", 0);
  #ifndef NDEBUG
  c10::optional<Storage> sorted_sequence__storage_saved =
    sorted_sequence_.has_storage() ? c10::optional<Storage>(sorted_sequence_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> sorted_sequence__impl_saved;
  if (sorted_sequence_.defined()) sorted_sequence__impl_saved = sorted_sequence_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::searchsorted(ks & c10::after_autograd_keyset, sorted_sequence_, self, out_int32, right, side, sorter);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (sorted_sequence__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(sorted_sequence_))
    TORCH_INTERNAL_ASSERT(sorted_sequence__storage_saved.value().is_alias_of(sorted_sequence_.storage()));
  if (sorted_sequence__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(sorted_sequence_))
    TORCH_INTERNAL_ASSERT(sorted_sequence__impl_saved == sorted_sequence_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: searchsorted_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: searchsorted_Scalar");
  #endif
  return result;
}
at::Tensor & searchsorted_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right, c10::optional<c10::string_view> side, const c10::optional<at::Tensor> & sorter, at::Tensor & out) {
  auto& sorted_sequence_ = unpack(sorted_sequence, "sorted_sequence", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& out_ = unpack(out, "out", 6);
  #ifndef NDEBUG
  c10::optional<Storage> sorted_sequence__storage_saved =
    sorted_sequence_.has_storage() ? c10::optional<Storage>(sorted_sequence_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> sorted_sequence__impl_saved;
  if (sorted_sequence_.defined()) sorted_sequence__impl_saved = sorted_sequence_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::searchsorted_outf(ks & c10::after_autograd_keyset, sorted_sequence_, self_, out_int32, right, side, sorter, out_);
  }
  #ifndef NDEBUG
  if (sorted_sequence__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(sorted_sequence_))
    TORCH_INTERNAL_ASSERT(sorted_sequence__storage_saved.value().is_alias_of(sorted_sequence_.storage()));
  if (sorted_sequence__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(sorted_sequence_))
    TORCH_INTERNAL_ASSERT(sorted_sequence__impl_saved == sorted_sequence_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(sorted_sequence) || isFwGradDefined(self) || isFwGradDefined(sorter) || isFwGradDefined(out))), "Trying to use forward AD with searchsorted_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & searchsorted_out_Scalar_out(c10::DispatchKeySet ks, const at::Tensor & sorted_sequence, const at::Scalar & self, bool out_int32, bool right, c10::optional<c10::string_view> side, const c10::optional<at::Tensor> & sorter, at::Tensor & out) {
  auto& sorted_sequence_ = unpack(sorted_sequence, "sorted_sequence", 0);
  auto& out_ = unpack(out, "out", 6);
  #ifndef NDEBUG
  c10::optional<Storage> sorted_sequence__storage_saved =
    sorted_sequence_.has_storage() ? c10::optional<Storage>(sorted_sequence_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> sorted_sequence__impl_saved;
  if (sorted_sequence_.defined()) sorted_sequence__impl_saved = sorted_sequence_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::searchsorted_outf(ks & c10::after_autograd_keyset, sorted_sequence_, self, out_int32, right, side, sorter, out_);
  }
  #ifndef NDEBUG
  if (sorted_sequence__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(sorted_sequence_))
    TORCH_INTERNAL_ASSERT(sorted_sequence__storage_saved.value().is_alias_of(sorted_sequence_.storage()));
  if (sorted_sequence__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(sorted_sequence_))
    TORCH_INTERNAL_ASSERT(sorted_sequence__impl_saved == sorted_sequence_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(sorted_sequence) || isFwGradDefined(sorter) || isFwGradDefined(out))), "Trying to use forward AD with searchsorted_out that does not support it because it is an out= function");
  return out;
}
at::Tensor select_copy_int(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, c10::SymInt index) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SelectBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SelectBackward0_copy>(new SelectBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->index = index;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::select_copy_symint(ks & c10::after_autograd_keyset, self_, dim, index);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: select_copy_int");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: select_copy_int");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "select_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::select_symint(self_t, dim, index);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor select_scatter(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::SymInt index) {
  auto& self_ = unpack(self, "self", 0);
  auto& src_ = unpack(src, "src", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, src );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(src));
  std::shared_ptr<SelectScatterBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SelectScatterBackward0>(new SelectScatterBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, src ));
    grad_fn->dim = dim;
    grad_fn->index = index;
    grad_fn->src_info = src;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> src__storage_saved =
    src_.has_storage() ? c10::optional<Storage>(src_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> src__impl_saved;
  if (src_.defined()) src__impl_saved = src_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::select_scatter_symint(ks & c10::after_autograd_keyset, self_, src_, dim, index);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (src__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__storage_saved.value().is_alias_of(src_.storage()));
  if (src__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__impl_saved == src_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: select_scatter");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: select_scatter");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "select_scatter");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto src_t_raw = toNonOptFwGrad(src);
      auto src_tensor = toNonOptTensor(src);
      auto src_t = (src_t_raw.defined() || !src_tensor.defined())
        ? src_t_raw : at::_efficientzerotensor(src_tensor.sizes(), src_tensor.options());
      result_new_fw_grad_opt = at::select_scatter_symint(self_t, src_t, dim, index);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor signbit(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::signbit(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: signbit");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: signbit");
  #endif
  return result;
}
at::Tensor & silu_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<SiluBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SiluBackward0>(new SiluBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::silu_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((GradMode::is_enabled() ? infinitely_differentiable_silu_backward(original_self_t.conj(), original_self_p) : silu_backward(original_self_t.conj(), original_self_p)).conj()) : (GradMode::is_enabled() ? infinitely_differentiable_silu_backward(original_self_t.conj(), original_self_p) : silu_backward(original_self_t.conj(), original_self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor sinc(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SincBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SincBackward0>(new SincBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::sinc(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sinc");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sinc");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (sinc_backward(self_t.conj(), self_p)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & sinc_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<SincBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SincBackward0>(new SincBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::sinc_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((sinc_backward(original_self_t.conj(), original_self_p)).conj()) : (sinc_backward(original_self_t.conj(), original_self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor slice_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SliceBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SliceBackward0>(new SliceBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->end = end;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
    grad_fn->start = start;
    grad_fn->step = step;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::slice_symint(ks & c10::after_autograd_keyset, self_, dim, start, end, step);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: slice_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::slice_symint(self_t, dim, start, end, step);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor slice_copy_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SliceBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SliceBackward0_copy>(new SliceBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->end = end;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
    grad_fn->start = start;
    grad_fn->step = step;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::slice_copy_symint(ks & c10::after_autograd_keyset, self_, dim, start, end, step);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: slice_copy_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: slice_copy_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "slice_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::slice_symint(self_t, dim, start, end, step);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor slow_conv3d_forward(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<SlowConv3DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SlowConv3DBackward0>(new SlowConv3DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight, bias ));
    grad_fn->bias_sym_sizes_opt = bias.has_value() ? c10::optional<c10::SymIntArrayRef>(bias->sym_sizes()) : c10::nullopt;
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::slow_conv3d_forward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("slow_conv3d_forward", *opt_op, ks, self, weight, kernel_size, bias, stride, padding);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::slow_conv3d_forward_symint(ks & c10::after_autograd_keyset, self_, weight_, kernel_size, bias, stride, padding);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: slow_conv3d_forward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "slow_conv3d_forward");
  return result;
}
at::Tensor slow_conv_transpose3d(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<SlowConvTranspose3DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SlowConvTranspose3DBackward0>(new SlowConvTranspose3DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight, bias ));
    grad_fn->bias_sym_sizes_opt = bias.has_value() ? c10::optional<c10::SymIntArrayRef>(bias->sym_sizes()) : c10::nullopt;
    grad_fn->dilation = dilation.vec();
    grad_fn->output_padding = output_padding.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::slow_conv_transpose3d", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("slow_conv_transpose3d", *opt_op, ks, self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::slow_conv_transpose3d_symint(ks & c10::after_autograd_keyset, self_, weight_, kernel_size, bias, stride, padding, output_padding, dilation);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: slow_conv_transpose3d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: slow_conv_transpose3d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "slow_conv_transpose3d");
  return result;
}
at::Tensor smooth_l1_loss(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
  auto& self_ = unpack(self, "self", 0);
  auto& target_ = unpack(target, "target", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, target );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(target));
  std::shared_ptr<SmoothL1LossBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SmoothL1LossBackward0>(new SmoothL1LossBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, target ));
    grad_fn->beta = beta;
    grad_fn->reduction = reduction;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->target_ = SavedVariable(target, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::smooth_l1_loss(ks & c10::after_autograd_keyset, self_, target_, reduction, beta);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: smooth_l1_loss");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: smooth_l1_loss");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "smooth_l1_loss");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto target_t_raw = toNonOptFwGrad(target);
      auto target_tensor = toNonOptTensor(target);
      auto target_t = (target_t_raw.defined() || !target_tensor.defined())
        ? target_t_raw : at::_efficientzerotensor(target_tensor.sizes(), target_tensor.options());
      auto target_p = toNonOptPrimal(target);
      result_new_fw_grad_opt = apply_loss_reduction(smooth_l1_loss_backward(self_t.conj(), self_p, target_p, at::Reduction::None, beta).conj() + smooth_l1_loss_backward(target_t.conj(), target_p, self_p, at::Reduction::None, beta).conj(), reduction);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & soft_margin_loss_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& target_ = unpack(target, "target", 2);
  auto& grad_input_ = unpack(grad_input, "grad_input", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self, target )) {
    throw_error_out_requires_grad("soft_margin_loss_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("soft_margin_loss_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::soft_margin_loss_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, target_, reduction, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target) || isFwGradDefined(grad_input))), "Trying to use forward AD with soft_margin_loss_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor softplus(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SoftplusBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SoftplusBackward0>(new SoftplusBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->beta = beta;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->threshold = threshold;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::softplus(ks & c10::after_autograd_keyset, self_, beta, threshold);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: softplus");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: softplus");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "softplus");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (softplus_backward(self_t.conj(), self_p, beta, threshold)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & softplus_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("softplus");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("softplus");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::softplus_outf(ks & c10::after_autograd_keyset, self_, beta, threshold, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with softplus_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & softshrink_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& grad_input_ = unpack(grad_input, "grad_input", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self )) {
    throw_error_out_requires_grad("softshrink_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("softshrink_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::softshrink_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, lambd, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with softshrink_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & softshrink_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("softshrink");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("softshrink");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::softshrink_outf(ks & c10::after_autograd_keyset, self_, lambd, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with softshrink_out that does not support it because it is an out= function");
  return out;
}
at::Tensor sparse_compressed_tensor_comp_plain_value_size(c10::DispatchKeySet ks, const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  auto& compressed_indices_ = unpack(compressed_indices, "compressed_indices", 0);
  auto& plain_indices_ = unpack(plain_indices, "plain_indices", 1);
  auto& values_ = unpack(values, "values", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( values );
  
  std::shared_ptr<SparseCompressedTensorBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SparseCompressedTensorBackward0>(new SparseCompressedTensorBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( values ));
    grad_fn->values_ = SavedVariable(values, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> compressed_indices__storage_saved =
    compressed_indices_.has_storage() ? c10::optional<Storage>(compressed_indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> compressed_indices__impl_saved;
  if (compressed_indices_.defined()) compressed_indices__impl_saved = compressed_indices_.getIntrusivePtr();
  c10::optional<Storage> plain_indices__storage_saved =
    plain_indices_.has_storage() ? c10::optional<Storage>(plain_indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> plain_indices__impl_saved;
  if (plain_indices_.defined()) plain_indices__impl_saved = plain_indices_.getIntrusivePtr();
  c10::optional<Storage> values__storage_saved =
    values_.has_storage() ? c10::optional<Storage>(values_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> values__impl_saved;
  if (values_.defined()) values__impl_saved = values_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(values))) {
      static c10::OperatorName full_name("aten::sparse_compressed_tensor", "comp_plain_value_size");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("sparse_compressed_tensor", *opt_op, ks, compressed_indices, plain_indices, values, size, dtype, layout, device, pin_memory);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::sparse_compressed_tensor_symint(ks & c10::after_autograd_keyset, compressed_indices_, plain_indices_, values_, size, dtype, layout, device, pin_memory);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (compressed_indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(compressed_indices_))
    TORCH_INTERNAL_ASSERT(compressed_indices__storage_saved.value().is_alias_of(compressed_indices_.storage()));
  if (compressed_indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(compressed_indices_))
    TORCH_INTERNAL_ASSERT(compressed_indices__impl_saved == compressed_indices_.getIntrusivePtr());
  if (plain_indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(plain_indices_))
    TORCH_INTERNAL_ASSERT(plain_indices__storage_saved.value().is_alias_of(plain_indices_.storage()));
  if (plain_indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(plain_indices_))
    TORCH_INTERNAL_ASSERT(plain_indices__impl_saved == plain_indices_.getIntrusivePtr());
  if (values__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__storage_saved.value().is_alias_of(values_.storage()));
  if (values__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__impl_saved == values_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sparse_compressed_tensor_comp_plain_value_size");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sparse_compressed_tensor_comp_plain_value_size");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "sparse_compressed_tensor");
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
int64_t sparse_dim(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::sparse_dim(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor special_chebyshev_polynomial_t(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_chebyshev_polynomial_t(ks & c10::after_autograd_keyset, x_, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_chebyshev_polynomial_t");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_chebyshev_polynomial_t");
  #endif
  return result;
}
at::Tensor special_chebyshev_polynomial_t_x_scalar(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n) {
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_chebyshev_polynomial_t(ks & c10::after_autograd_keyset, x, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_chebyshev_polynomial_t_x_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_chebyshev_polynomial_t_x_scalar");
  #endif
  return result;
}
at::Tensor special_chebyshev_polynomial_t_n_scalar(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n) {
  auto& x_ = unpack(x, "x", 0);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_chebyshev_polynomial_t(ks & c10::after_autograd_keyset, x_, n);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_chebyshev_polynomial_t_n_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_chebyshev_polynomial_t_n_scalar");
  #endif
  return result;
}
at::Tensor & special_chebyshev_polynomial_v_out_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_chebyshev_polynomial_v_outf(ks & c10::after_autograd_keyset, x_, n_, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_chebyshev_polynomial_v_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_chebyshev_polynomial_v_out_x_scalar_out(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_chebyshev_polynomial_v_outf(ks & c10::after_autograd_keyset, x, n_, out_);
  }
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_chebyshev_polynomial_v_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_chebyshev_polynomial_v_out_n_scalar_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_chebyshev_polynomial_v_outf(ks & c10::after_autograd_keyset, x_, n, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_chebyshev_polynomial_v_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_chebyshev_polynomial_w_out_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_chebyshev_polynomial_w_outf(ks & c10::after_autograd_keyset, x_, n_, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_chebyshev_polynomial_w_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_chebyshev_polynomial_w_out_x_scalar_out(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_chebyshev_polynomial_w_outf(ks & c10::after_autograd_keyset, x, n_, out_);
  }
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_chebyshev_polynomial_w_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_chebyshev_polynomial_w_out_n_scalar_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_chebyshev_polynomial_w_outf(ks & c10::after_autograd_keyset, x_, n, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_chebyshev_polynomial_w_out that does not support it because it is an out= function");
  return out;
}
at::Tensor special_erfcx(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SpecialErfcxBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SpecialErfcxBackward0>(new SpecialErfcxBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_erfcx(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_erfcx");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_erfcx");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "special_erfcx");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = ((2.0 * self_p * result - 2.0 / sqrt(M_PI)) * self_t.conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & special_erfcx_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("special_erfcx");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("special_erfcx");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_erfcx_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with special_erfcx_out that does not support it because it is an out= function");
  return out;
}
at::Tensor special_i0e(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SpecialI0EBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SpecialI0EBackward0>(new SpecialI0EBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_i0e(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_i0e");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_i0e");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "special_i0e");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() * (at::special_i1e(self_p) - self_p.sgn() * result)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor special_i1(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SpecialI1Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SpecialI1Backward0>(new SpecialI1Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_i1(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_i1");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_i1");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "special_i1");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (i1_backward(self_t.conj(), self_p, result)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & special_legendre_polynomial_p_out_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_legendre_polynomial_p_outf(ks & c10::after_autograd_keyset, x_, n_, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_legendre_polynomial_p_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_legendre_polynomial_p_out_x_scalar_out(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_legendre_polynomial_p_outf(ks & c10::after_autograd_keyset, x, n_, out_);
  }
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_legendre_polynomial_p_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_legendre_polynomial_p_out_n_scalar_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_legendre_polynomial_p_outf(ks & c10::after_autograd_keyset, x_, n, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_legendre_polynomial_p_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_log_ndtr_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("special_log_ndtr");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("special_log_ndtr");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_log_ndtr_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with special_log_ndtr_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_modified_bessel_i0_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_modified_bessel_i0_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_modified_bessel_i0_out that does not support it because it is an out= function");
  return out;
}
at::Tensor special_scaled_modified_bessel_k1(c10::DispatchKeySet ks, const at::Tensor & x) {
  auto& x_ = unpack(x, "x", 0);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_scaled_modified_bessel_k1(ks & c10::after_autograd_keyset, x_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_scaled_modified_bessel_k1");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_scaled_modified_bessel_k1");
  #endif
  return result;
}
at::Tensor & special_shifted_chebyshev_polynomial_v_out_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_v_outf(ks & c10::after_autograd_keyset, x_, n_, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_v_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_v_out_x_scalar_out(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_v_outf(ks & c10::after_autograd_keyset, x, n_, out_);
  }
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_v_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_v_out_n_scalar_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_v_outf(ks & c10::after_autograd_keyset, x_, n, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_v_out that does not support it because it is an out= function");
  return out;
}
::std::vector<at::Tensor> split_with_sizes_copy(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SplitWithSizesBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SplitWithSizesBackward0_copy>(new SplitWithSizesBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_options = self.options();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
    grad_fn->split_sizes = split_sizes.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::split_with_sizes_copy_symint(ks & c10::after_autograd_keyset, self_, split_sizes, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<::std::vector<at::Tensor>> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::split_with_sizes_symint(self_t, split_sizes, dim);
  }
  if (result_new_fw_grad_opt.has_value()) {
    auto result_new_fw_grad = result_new_fw_grad_opt.value();
    TORCH_INTERNAL_ASSERT(result.size() == result_new_fw_grad.size());
    for (const auto i : c10::irange(result.size())) {
      if (result_new_fw_grad[i].defined() && result[i].defined()) {
        // The hardcoded 0 here will need to be updated once we support multiple levels.
        result[i]._set_fw_grad(result_new_fw_grad[i], /* level */ 0, /* is_inplace_op */ false);
      }
    }
  }
  return result;
}
at::Tensor sub_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<SubBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SubBackward0>(new SubBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->alpha = alpha;
    grad_fn->other_scalar_type = other.scalar_type();
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::sub(ks & c10::after_autograd_keyset, self_, other_, alpha);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sub_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sub_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      result_new_fw_grad_opt = self_t - maybe_multiply(other_t, alpha);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor sub_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SubBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SubBackward1>(new SubBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::sub(ks & c10::after_autograd_keyset, self_, other, alpha);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sub_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sub_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (handle_r_to_c(self_p.scalar_type(), self_t.conj())).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor t_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<TBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TBackward0_copy>(new TBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::t_copy(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: t_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: t_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "t_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::t(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor take(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & index) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<TakeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TakeBackward0>(new TakeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->index_ = SavedVariable(index, false);
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::take(ks & c10::after_autograd_keyset, self_, index_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: take");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: take");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::take(self_t, index);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor tan(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<TanBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TanBackward0>(new TanBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::tan(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: tan");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: tan");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (self_t.conj() * (1 + result.pow(2)).conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & tan_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("tan");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("tan");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::tan_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with tan_out that does not support it because it is an out= function");
  return out;
}
at::Tensor trace(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<TraceBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TraceBackward0>(new TraceBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::trace(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: trace");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: trace");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::trace(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & trunc_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("trunc");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("trunc");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::trunc_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with trunc_out that does not support it because it is an out= function");
  return out;
}
::std::vector<at::Tensor> unbind_copy_int(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UnbindBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnbindBackward0_copy>(new UnbindBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::unbind_copy(ks & c10::after_autograd_keyset, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<::std::vector<at::Tensor>> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::unbind(self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value()) {
    auto result_new_fw_grad = result_new_fw_grad_opt.value();
    TORCH_INTERNAL_ASSERT(result.size() == result_new_fw_grad.size());
    for (const auto i : c10::irange(result.size())) {
      if (result_new_fw_grad[i].defined() && result[i].defined()) {
        // The hardcoded 0 here will need to be updated once we support multiple levels.
        result[i]._set_fw_grad(result_new_fw_grad[i], /* level */ 0, /* is_inplace_op */ false);
      }
    }
  }
  return result;
}
at::Tensor unfold(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dimension, int64_t size, int64_t step) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UnfoldBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnfoldBackward0>(new UnfoldBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dimension = dimension;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
    grad_fn->size = size;
    grad_fn->step = step;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::unfold(ks & c10::after_autograd_keyset, self_, dimension, size, step);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: unfold");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t.unfold(dimension, size, step);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_dim_consecutive(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool return_inverse, bool return_counts) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<UniqueDimConsecutiveBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UniqueDimConsecutiveBackward0>(new UniqueDimConsecutiveBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::unique_dim_consecutive", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor>>("unique_dim_consecutive", *opt_op, ks, self, dim, return_inverse, return_counts);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::unique_dim_consecutive(ks & c10::after_autograd_keyset, self_, dim, return_inverse, return_counts);
    }
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: unique_dim_consecutive");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: unique_dim_consecutive");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: unique_dim_consecutive");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: unique_dim_consecutive");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: unique_dim_consecutive");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: unique_dim_consecutive");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "unique_dim_consecutive");
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
void unsafe_split_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymInt split_size, int64_t dim, at::TensorList out) {
  auto& self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::unsafe_split_symint_outf(ks & c10::after_autograd_keyset, self_, split_size, dim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with unsafe_split_out that does not support it because it is an out= function");
}
::std::vector<at::Tensor> unsafe_split_with_sizes(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UnsafeSplitWithSizesBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnsafeSplitWithSizesBackward0>(new UnsafeSplitWithSizesBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_options = self.options();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
    grad_fn->split_sizes = split_sizes.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::unsafe_split_with_sizes_symint(ks & c10::after_autograd_keyset, self_, split_sizes, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<::std::vector<at::Tensor>> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::unsafe_split_with_sizes_symint(self_t, split_sizes, dim);
  }
  if (result_new_fw_grad_opt.has_value()) {
    auto result_new_fw_grad = result_new_fw_grad_opt.value();
    TORCH_INTERNAL_ASSERT(result.size() == result_new_fw_grad.size());
    for (const auto i : c10::irange(result.size())) {
      if (result_new_fw_grad[i].defined() && result[i].defined()) {
        // The hardcoded 0 here will need to be updated once we support multiple levels.
        result[i]._set_fw_grad(result_new_fw_grad[i], /* level */ 0, /* is_inplace_op */ false);
      }
    }
  }
  return result;
}
at::Tensor unsqueeze(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UnsqueezeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnsqueezeBackward0>(new UnsqueezeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::unsqueeze(ks & c10::after_autograd_keyset, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: unsqueeze");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::unsqueeze(self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & unsqueeze_(c10::DispatchKeySet ks, at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<UnsqueezeBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnsqueezeBackward1>(new UnsqueezeBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::unsqueeze_(ks & c10::after_autograd_keyset, self_, dim);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.unsqueeze_(dim);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor upsample_linear1d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<UpsampleLinear1DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleLinear1DBackwardBackward0>(new UpsampleLinear1DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output ));
    grad_fn->align_corners = align_corners;
    grad_fn->output_size = output_size.vec();
    grad_fn->scales = scales;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_linear1d_backward_symint(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, align_corners, scales);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_linear1d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_linear1d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_linear1d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result_new_fw_grad_opt = at::upsample_linear1d_backward_symint(grad_output_t, output_size, input_size, align_corners, scales);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor upsample_nearest1d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<UpsampleNearest1DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleNearest1DBackwardBackward0>(new UpsampleNearest1DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output ));
    grad_fn->output_size = output_size.vec();
    grad_fn->scales = scales;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_nearest1d_backward_symint(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, scales);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_nearest1d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_nearest1d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_nearest1d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result_new_fw_grad_opt = at::upsample_nearest1d_backward_symint(grad_output_t, output_size, input_size, scales);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor upsample_nearest2d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<UpsampleNearest2DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleNearest2DBackwardBackward0>(new UpsampleNearest2DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output ));
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_nearest2d_backward_symint(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_nearest2d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_nearest2d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_nearest2d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result_new_fw_grad_opt = at::upsample_nearest2d_backward_symint(grad_output_t, output_size, input_size, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & upsample_trilinear3d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("upsample_trilinear3d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("upsample_trilinear3d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::upsample_trilinear3d_symint_outf(ks & c10::after_autograd_keyset, self_, output_size, align_corners, scales_d, scales_h, scales_w, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with upsample_trilinear3d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & vdot_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("vdot");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("vdot");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::vdot_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with vdot_out that does not support it because it is an out= function");
  return out;
}
at::Tensor view_as_complex(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ViewAsComplexBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ViewAsComplexBackward0>(new ViewAsComplexBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::view_as_complex(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: view_as_complex");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::view_as_complex(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor view_as_real(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ViewAsRealBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ViewAsRealBackward0>(new ViewAsRealBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::view_as_real(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: view_as_real");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "view_as_real");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::view_as_real(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & zeros_like_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::zeros_like_outf(ks & c10::after_autograd_keyset, self_, memory_format, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with zeros_like_out that does not support it because it is an out= function");
  return out;
}
}
}

namespace {

TORCH_LIBRARY_IMPL(aten, AutogradCUDA, m) {
m.impl("_test_autograd_multiple_dispatch_view",
       TORCH_FN(VariableType::_test_autograd_multiple_dispatch_view_AutogradCUDA)
);
}

TORCH_LIBRARY_IMPL(aten, AutogradNestedTensor, m) {
m.impl("select_copy.int",
       TORCH_FN(VariableType::select_copy_int_AutogradNestedTensor)
);
m.impl("split_with_sizes_copy",
       TORCH_FN(VariableType::split_with_sizes_copy_AutogradNestedTensor)
);
m.impl("unbind_copy.int",
       TORCH_FN(VariableType::unbind_copy_int_AutogradNestedTensor)
);
}

TORCH_LIBRARY_IMPL(aten, Autograd, m) {
m.impl("_adaptive_avg_pool2d",
       TORCH_FN(VariableType::_adaptive_avg_pool2d)
);
m.impl("_addmm_activation", torch::autograd::autogradNotImplementedFallback());
m.impl("_assert_async",
       TORCH_FN(VariableType::_assert_async)
);
m.impl("_assert_async.msg",
       TORCH_FN(VariableType::_assert_async_msg)
);
m.impl("_compute_linear_combination", torch::autograd::autogradNotImplementedFallback());
m.impl("_convert_weight_to_int4pack", torch::autograd::autogradNotImplementedFallback());
m.impl("_copy_from", torch::autograd::autogradNotImplementedFallback());
m.impl("_copy_from.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_cslt_compress", torch::autograd::autogradNotImplementedFallback());
m.impl("_cslt_sparse_mm_search",
       TORCH_FN(VariableType::_cslt_sparse_mm_search)
);
m.impl("_ctc_loss.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_ctc_loss.Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("_dimI",
       TORCH_FN(VariableType::_dimI)
);
m.impl("_efficientzerotensor",
       TORCH_FN(VariableType::_efficientzerotensor)
);
m.impl("_embedding_bag_dense_backward",
       TORCH_FN(VariableType::_embedding_bag_dense_backward)
);
m.impl("_fft_c2r",
       TORCH_FN(VariableType::_fft_c2r)
);
m.impl("_foobar", torch::autograd::autogradNotImplementedFallback());
m.impl("_foreach_abs",
       TORCH_FN(VariableType::_foreach_abs)
);
m.impl("_foreach_add_.Scalar",
       TORCH_FN(VariableType::_foreach_add__Scalar)
);
m.impl("_foreach_add_.List",
       TORCH_FN(VariableType::_foreach_add__List)
);
m.impl("_foreach_add_.ScalarList",
       TORCH_FN(VariableType::_foreach_add__ScalarList)
);
m.impl("_foreach_add_.Tensor",
       TORCH_FN(VariableType::_foreach_add__Tensor)
);
m.impl("_foreach_addcdiv.Scalar_out",
       TORCH_FN(VariableType::_foreach_addcdiv_out_Scalar_out)
);
m.impl("_foreach_addcdiv.ScalarList_out",
       TORCH_FN(VariableType::_foreach_addcdiv_out_ScalarList_out)
);
m.impl("_foreach_addcdiv.Tensor_out",
       TORCH_FN(VariableType::_foreach_addcdiv_out_Tensor_out)
);
m.impl("_foreach_atan_",
       TORCH_FN(VariableType::_foreach_atan_)
);
m.impl("_foreach_clamp_max.Scalar",
       TORCH_FN(VariableType::_foreach_clamp_max_Scalar)
);
m.impl("_foreach_clamp_max.List",
       TORCH_FN(VariableType::_foreach_clamp_max_List)
);
m.impl("_foreach_clamp_max.ScalarList",
       TORCH_FN(VariableType::_foreach_clamp_max_ScalarList)
);
m.impl("_foreach_clamp_min.Scalar_out",
       TORCH_FN(VariableType::_foreach_clamp_min_out_Scalar_out)
);
m.impl("_foreach_clamp_min.List_out",
       TORCH_FN(VariableType::_foreach_clamp_min_out_List_out)
);
m.impl("_foreach_clamp_min.ScalarList_out",
       TORCH_FN(VariableType::_foreach_clamp_min_out_ScalarList_out)
);
m.impl("_foreach_copy", torch::autograd::autogradNotImplementedFallback());
m.impl("_foreach_erf_",
       TORCH_FN(VariableType::_foreach_erf_)
);
m.impl("_foreach_erf.out",
       TORCH_FN(VariableType::_foreach_erf_out_out)
);
m.impl("_foreach_erfc_",
       TORCH_FN(VariableType::_foreach_erfc_)
);
m.impl("_foreach_exp_",
       TORCH_FN(VariableType::_foreach_exp_)
);
m.impl("_foreach_exp.out",
       TORCH_FN(VariableType::_foreach_exp_out_out)
);
m.impl("_foreach_expm1",
       TORCH_FN(VariableType::_foreach_expm1)
);
m.impl("_foreach_floor.out",
       TORCH_FN(VariableType::_foreach_floor_out_out)
);
m.impl("_foreach_lerp_.List",
       TORCH_FN(VariableType::_foreach_lerp__List)
);
m.impl("_foreach_lerp_.Scalar",
       TORCH_FN(VariableType::_foreach_lerp__Scalar)
);
m.impl("_foreach_lerp.List_out",
       TORCH_FN(VariableType::_foreach_lerp_out_List_out)
);
m.impl("_foreach_lerp.Scalar_out",
       TORCH_FN(VariableType::_foreach_lerp_out_Scalar_out)
);
m.impl("_foreach_log10",
       TORCH_FN(VariableType::_foreach_log10)
);
m.impl("_foreach_log10.out",
       TORCH_FN(VariableType::_foreach_log10_out_out)
);
m.impl("_foreach_log2.out",
       TORCH_FN(VariableType::_foreach_log2_out_out)
);
m.impl("_foreach_maximum_.Scalar",
       TORCH_FN(VariableType::_foreach_maximum__Scalar)
);
m.impl("_foreach_maximum_.List",
       TORCH_FN(VariableType::_foreach_maximum__List)
);
m.impl("_foreach_maximum_.ScalarList",
       TORCH_FN(VariableType::_foreach_maximum__ScalarList)
);
m.impl("_foreach_sigmoid_",
       TORCH_FN(VariableType::_foreach_sigmoid_)
);
m.impl("_foreach_sign",
       TORCH_FN(VariableType::_foreach_sign)
);
m.impl("_foreach_sign_",
       TORCH_FN(VariableType::_foreach_sign_)
);
m.impl("_foreach_sin.out",
       TORCH_FN(VariableType::_foreach_sin_out_out)
);
m.impl("_foreach_sinh",
       TORCH_FN(VariableType::_foreach_sinh)
);
m.impl("_foreach_sqrt_",
       TORCH_FN(VariableType::_foreach_sqrt_)
);
m.impl("_foreach_sub_.Scalar",
       TORCH_FN(VariableType::_foreach_sub__Scalar)
);
m.impl("_foreach_sub_.List",
       TORCH_FN(VariableType::_foreach_sub__List)
);
m.impl("_foreach_sub_.ScalarList",
       TORCH_FN(VariableType::_foreach_sub__ScalarList)
);
m.impl("_foreach_tan",
       TORCH_FN(VariableType::_foreach_tan)
);
m.impl("_foreach_tan.out",
       TORCH_FN(VariableType::_foreach_tan_out_out)
);
m.impl("_foreach_zero.out",
       TORCH_FN(VariableType::_foreach_zero_out_out)
);
m.impl("_functional_assert_async.msg", torch::autograd::autogradNotImplementedFallback());
m.impl("_fused_adamw_",
       TORCH_FN(VariableType::_fused_adamw_)
);
m.impl("_fused_adamw_.tensor_lr",
       TORCH_FN(VariableType::_fused_adamw__tensor_lr)
);
m.impl("_fused_adamw.out",
       TORCH_FN(VariableType::_fused_adamw_out_out)
);
m.impl("_fused_adamw.tensor_lr_out",
       TORCH_FN(VariableType::_fused_adamw_out_tensor_lr_out)
);
m.impl("_fw_primal_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_grid_sampler_2d_cpu_fallback.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_histogramdd_from_bin_cts", torch::autograd::autogradNotImplementedFallback());
m.impl("_indices_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_int_mm.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_linalg_det.result",
       TORCH_FN(VariableType::_linalg_det_out_result)
);
m.impl("_linalg_eigh.eigenvalues",
       TORCH_FN(VariableType::_linalg_eigh_out_eigenvalues)
);
m.impl("_linalg_slogdet",
       TORCH_FN(VariableType::_linalg_slogdet)
);
m.impl("_linalg_solve_ex",
       TORCH_FN(VariableType::_linalg_solve_ex)
);
m.impl("_linalg_solve_ex.result",
       TORCH_FN(VariableType::_linalg_solve_ex_out_result)
);
m.impl("_linalg_svd",
       TORCH_FN(VariableType::_linalg_svd)
);
m.impl("_linalg_svd.U",
       TORCH_FN(VariableType::_linalg_svd_out_U)
);
m.impl("_make_dual_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_make_per_channel_quantized_tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("_make_per_channel_quantized_tensor.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_make_per_tensor_quantized_tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("_masked_scale", torch::autograd::autogradNotImplementedFallback());
m.impl("_masked_softmax_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("_mkldnn_reshape.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_mkldnn_transpose_", torch::autograd::autogradNotImplementedFallback());
m.impl("_mps_convolution_transpose",
       TORCH_FN(VariableType::_mps_convolution_transpose)
);
m.impl("_mps_convolution_transpose.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_native_batch_norm_legit_no_training.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_nested_tensor_from_mask",
       TORCH_FN(VariableType::_nested_tensor_from_mask)
);
m.impl("_nested_tensor_from_mask_left_aligned",
       TORCH_FN(VariableType::_nested_tensor_from_mask_left_aligned)
);
m.impl("_nested_tensor_from_tensor_list.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_nested_tensor_size",
       TORCH_FN(VariableType::_nested_tensor_size)
);
m.impl("_nested_view_from_buffer_copy",
       TORCH_FN(VariableType::_nested_view_from_buffer_copy)
);
m.impl("_nnz",
       TORCH_FN(VariableType::_nnz)
);
m.impl("_reshape_alias_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_reshape_copy",
       TORCH_FN(VariableType::_reshape_copy)
);
m.impl("_sample_dirichlet.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_scaled_dot_product_flash_attention_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_addmm.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_csr_sum.dim_dtype_out", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_softmax_backward_data.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_softmax.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_sum.dim_out", torch::autograd::autogradNotImplementedFallback());
m.impl("_stack.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_test_autograd_multiple_dispatch_view",
       TORCH_FN(VariableType::_test_autograd_multiple_dispatch_view)
);
m.impl("_test_optional_floatlist.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_test_optional_intlist.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_test_warn_in_autograd",
       TORCH_FN(VariableType::_test_warn_in_autograd)
);
m.impl("_to_sparse_bsr",
       TORCH_FN(VariableType::_to_sparse_bsr)
);
m.impl("_to_sparse_csr",
       TORCH_FN(VariableType::_to_sparse_csr)
);
m.impl("_to_sparse_csr.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_to_sparse.sparse_dim_out", torch::autograd::autogradNotImplementedFallback());
m.impl("_to_sparse.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_triton_multi_head_attention.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_triton_scaled_dot_attention", torch::autograd::autogradNotImplementedFallback());
m.impl("_unique.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_unsafe_view",
       TORCH_FN(VariableType::_unsafe_view)
);
m.impl("_upsample_bilinear2d_aa",
       TORCH_FN(VariableType::_upsample_bilinear2d_aa)
);
m.impl("_values_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_weight_norm_interface_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("acos",
       TORCH_FN(VariableType::acos)
);
m.impl("adaptive_max_pool3d_backward.grad_input",
       TORCH_FN(VariableType::adaptive_max_pool3d_backward_out_grad_input)
);
m.impl("add.out",
       TORCH_FN(VariableType::add_out_out)
);
m.impl("add.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("addr.out",
       TORCH_FN(VariableType::addr_out_out)
);
m.impl("alias_copy",
       TORCH_FN(VariableType::alias_copy)
);
m.impl("all.out",
       TORCH_FN(VariableType::all_out_out)
);
m.impl("all.dims_out",
       TORCH_FN(VariableType::all_out_dims_out)
);
m.impl("all.all_out",
       TORCH_FN(VariableType::all_out_all_out)
);
m.impl("any.dim",
       TORCH_FN(VariableType::any_dim)
);
m.impl("any.dims",
       TORCH_FN(VariableType::any_dims)
);
m.impl("any",
       TORCH_FN(VariableType::any)
);
m.impl("argmin.out",
       TORCH_FN(VariableType::argmin_out_out)
);
m.impl("asin",
       TORCH_FN(VariableType::asin)
);
m.impl("asin.out",
       TORCH_FN(VariableType::asin_out_out)
);
m.impl("asinh.out",
       TORCH_FN(VariableType::asinh_out_out)
);
m.impl("avg_pool2d_backward.grad_input",
       TORCH_FN(VariableType::avg_pool2d_backward_out_grad_input)
);
m.impl("avg_pool3d",
       TORCH_FN(VariableType::avg_pool3d)
);
m.impl("avg_pool3d_backward",
       TORCH_FN(VariableType::avg_pool3d_backward)
);
m.impl("batch_norm_elemt", torch::autograd::autogradNotImplementedFallback());
m.impl("binary_cross_entropy_backward.grad_input",
       TORCH_FN(VariableType::binary_cross_entropy_backward_out_grad_input)
);
m.impl("bincount.out", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_and_.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_and_.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_not_", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_or.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_or.Scalar_Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_or.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_or.Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_or.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_or.Scalar_Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_right_shift_.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_right_shift_.Tensor_Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("bucketize.Tensor_out",
       TORCH_FN(VariableType::bucketize_out_Tensor_out)
);
m.impl("bucketize.Scalar_out",
       TORCH_FN(VariableType::bucketize_out_Scalar_out)
);
m.impl("cauchy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("ccol_indices",
       TORCH_FN(VariableType::ccol_indices)
);
m.impl("ceil.out",
       TORCH_FN(VariableType::ceil_out_out)
);
m.impl("channel_shuffle",
       TORCH_FN(VariableType::channel_shuffle)
);
m.impl("cholesky_solve.out",
       TORCH_FN(VariableType::cholesky_solve_out_out)
);
m.impl("clamp.out",
       TORCH_FN(VariableType::clamp_out_out)
);
m.impl("clamp.Tensor_out",
       TORCH_FN(VariableType::clamp_out_Tensor_out)
);
m.impl("clone.out", torch::autograd::autogradNotImplementedFallback());
m.impl("col_indices_copy",
       TORCH_FN(VariableType::col_indices_copy)
);
m.impl("constant_pad_nd",
       TORCH_FN(VariableType::constant_pad_nd)
);
m.impl("constant_pad_nd.out", torch::autograd::autogradNotImplementedFallback());
m.impl("convolution_backward",
       TORCH_FN(VariableType::convolution_backward)
);
m.impl("convolution_overrideable",
       TORCH_FN(VariableType::convolution_overrideable)
);
m.impl("copysign.Tensor",
       TORCH_FN(VariableType::copysign_Tensor)
);
m.impl("copysign.Scalar",
       TORCH_FN(VariableType::copysign_Scalar)
);
m.impl("copysign_.Tensor",
       TORCH_FN(VariableType::copysign__Tensor)
);
m.impl("copysign_.Scalar",
       TORCH_FN(VariableType::copysign__Scalar)
);
m.impl("copysign.out",
       TORCH_FN(VariableType::copysign_out_out)
);
m.impl("copysign.Scalar_out",
       TORCH_FN(VariableType::copysign_out_Scalar_out)
);
m.impl("cudnn_batch_norm",
       TORCH_FN(VariableType::cudnn_batch_norm)
);
m.impl("cudnn_convolution.out", torch::autograd::autogradNotImplementedFallback());
m.impl("cummin.out",
       TORCH_FN(VariableType::cummin_out_out)
);
m.impl("deg2rad",
       TORCH_FN(VariableType::deg2rad)
);
m.impl("deg2rad_",
       TORCH_FN(VariableType::deg2rad_)
);
m.impl("deg2rad.out",
       TORCH_FN(VariableType::deg2rad_out_out)
);
m.impl("diagonal_copy",
       TORCH_FN(VariableType::diagonal_copy)
);
m.impl("diagonal_scatter.out", torch::autograd::autogradNotImplementedFallback());
m.impl("digamma.out",
       TORCH_FN(VariableType::digamma_out_out)
);
m.impl("dist",
       TORCH_FN(VariableType::dist)
);
m.impl("dot.out",
       TORCH_FN(VariableType::dot_out_out)
);
m.impl("elu",
       TORCH_FN(VariableType::elu)
);
m.impl("elu_",
       TORCH_FN(VariableType::elu_)
);
m.impl("elu_backward.grad_input",
       TORCH_FN(VariableType::elu_backward_out_grad_input)
);
m.impl("embedding",
       TORCH_FN(VariableType::embedding)
);
m.impl("empty_like.out",
       TORCH_FN(VariableType::empty_like_out_out)
);
m.impl("erf.out",
       TORCH_FN(VariableType::erf_out_out)
);
m.impl("erfc",
       TORCH_FN(VariableType::erfc)
);
m.impl("erfc.out",
       TORCH_FN(VariableType::erfc_out_out)
);
m.impl("erfinv",
       TORCH_FN(VariableType::erfinv)
);
m.impl("erfinv_",
       TORCH_FN(VariableType::erfinv_)
);
m.impl("floor_divide", torch::autograd::autogradNotImplementedFallback());
m.impl("floor_divide.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("floor_divide.out", torch::autograd::autogradNotImplementedFallback());
m.impl("floor_divide.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("fmax.out",
       TORCH_FN(VariableType::fmax_out_out)
);
m.impl("fmod.Scalar",
       TORCH_FN(VariableType::fmod_Scalar)
);
m.impl("fmod.Tensor",
       TORCH_FN(VariableType::fmod_Tensor)
);
m.impl("fractional_max_pool2d.output",
       TORCH_FN(VariableType::fractional_max_pool2d_out_output)
);
m.impl("fractional_max_pool3d_backward.grad_input",
       TORCH_FN(VariableType::fractional_max_pool3d_backward_out_grad_input)
);
m.impl("fractional_max_pool3d.output",
       TORCH_FN(VariableType::fractional_max_pool3d_out_output)
);
m.impl("frexp.Tensor_out",
       TORCH_FN(VariableType::frexp_out_Tensor_out)
);
m.impl("from_file.out", torch::autograd::autogradNotImplementedFallback());
m.impl("full.names", torch::autograd::autogradNotImplementedFallback());
m.impl("full", torch::autograd::autogradNotImplementedFallback());
m.impl("full_like",
       TORCH_FN(VariableType::full_like)
);
m.impl("ge.Scalar_out",
       TORCH_FN(VariableType::ge_out_Scalar_out)
);
m.impl("ge.Tensor_out",
       TORCH_FN(VariableType::ge_out_Tensor_out)
);
m.impl("geqrf",
       TORCH_FN(VariableType::geqrf)
);
m.impl("glu_backward",
       TORCH_FN(VariableType::glu_backward)
);
m.impl("grid_sampler_2d",
       TORCH_FN(VariableType::grid_sampler_2d)
);
m.impl("grid_sampler_2d.out", torch::autograd::autogradNotImplementedFallback());
m.impl("gt_.Scalar",
       TORCH_FN(VariableType::gt__Scalar)
);
m.impl("gt_.Tensor",
       TORCH_FN(VariableType::gt__Tensor)
);
m.impl("hardshrink_backward",
       TORCH_FN(VariableType::hardshrink_backward)
);
m.impl("hardshrink_backward.grad_input",
       TORCH_FN(VariableType::hardshrink_backward_out_grad_input)
);
m.impl("hardsigmoid_backward.grad_input", torch::autograd::autogradNotImplementedFallback());
m.impl("hardswish_",
       TORCH_FN(VariableType::hardswish_)
);
m.impl("hardswish_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("hardtanh_",
       TORCH_FN(VariableType::hardtanh_)
);
m.impl("hardtanh_backward",
       TORCH_FN(VariableType::hardtanh_backward)
);
m.impl("heaviside", torch::autograd::autogradNotImplementedFallback());
m.impl("huber_loss_backward.out",
       TORCH_FN(VariableType::huber_loss_backward_out_out)
);
m.impl("huber_loss.out",
       TORCH_FN(VariableType::huber_loss_out_out)
);
m.impl("i0_",
       TORCH_FN(VariableType::i0_)
);
m.impl("igamma.out",
       TORCH_FN(VariableType::igamma_out_out)
);
m.impl("im2col",
       TORCH_FN(VariableType::im2col)
);
m.impl("index_add",
       TORCH_FN(VariableType::index_add)
);
m.impl("index_add_",
       TORCH_FN(VariableType::index_add_)
);
m.impl("index_put",
       TORCH_FN(VariableType::index_put)
);
m.impl("index_put.out", torch::autograd::autogradNotImplementedFallback());
m.impl("index_reduce.out",
       TORCH_FN(VariableType::index_reduce_out_out)
);
m.impl("int_repr.out", torch::autograd::autogradNotImplementedFallback());
m.impl("is_same_size",
       TORCH_FN(VariableType::is_same_size)
);
m.impl("isneginf",
       TORCH_FN(VariableType::isneginf)
);
m.impl("kaiser_window", torch::autograd::autogradNotImplementedFallback());
m.impl("kaiser_window.periodic", torch::autograd::autogradNotImplementedFallback());
m.impl("kaiser_window.beta", torch::autograd::autogradNotImplementedFallback());
m.impl("leaky_relu_",
       TORCH_FN(VariableType::leaky_relu_)
);
m.impl("leaky_relu_backward",
       TORCH_FN(VariableType::leaky_relu_backward)
);
m.impl("linalg_lu.out",
       TORCH_FN(VariableType::linalg_lu_out_out)
);
m.impl("linalg_lu_solve",
       TORCH_FN(VariableType::linalg_lu_solve)
);
m.impl("linalg_lu_solve.out",
       TORCH_FN(VariableType::linalg_lu_solve_out_out)
);
m.impl("linalg_matrix_exp.out", torch::autograd::autogradNotImplementedFallback());
m.impl("linalg_qr",
       TORCH_FN(VariableType::linalg_qr)
);
m.impl("linalg_vector_norm.out",
       TORCH_FN(VariableType::linalg_vector_norm_out_out)
);
m.impl("linear_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("log1p_",
       TORCH_FN(VariableType::log1p_)
);
m.impl("log2_",
       TORCH_FN(VariableType::log2_)
);
m.impl("log2.out",
       TORCH_FN(VariableType::log2_out_out)
);
m.impl("log_normal_",
       TORCH_FN(VariableType::log_normal_)
);
m.impl("log_softmax.int_out", torch::autograd::autogradNotImplementedFallback());
m.impl("logical_xor",
       TORCH_FN(VariableType::logical_xor)
);
m.impl("logit_",
       TORCH_FN(VariableType::logit_)
);
m.impl("logit.out",
       TORCH_FN(VariableType::logit_out_out)
);
m.impl("lstm_mps_backward.out",
       TORCH_FN(VariableType::lstm_mps_backward_out_out)
);
m.impl("lu_unpack.out",
       TORCH_FN(VariableType::lu_unpack_out_out)
);
m.impl("masked_fill.Scalar",
       TORCH_FN(VariableType::masked_fill_Scalar)
);
m.impl("masked_fill.Tensor",
       TORCH_FN(VariableType::masked_fill_Tensor)
);
m.impl("masked_fill.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("masked_fill.Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("masked_scatter",
       TORCH_FN(VariableType::masked_scatter)
);
m.impl("masked_scatter_",
       TORCH_FN(VariableType::masked_scatter_)
);
m.impl("matmul",
       TORCH_FN(VariableType::matmul)
);
m.impl("matmul_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("max_pool2d_backward",
       TORCH_FN(VariableType::max_pool2d_backward)
);
m.impl("max_pool2d_with_indices_backward",
       TORCH_FN(VariableType::max_pool2d_with_indices_backward)
);
m.impl("max_pool2d_with_indices_backward.grad_input",
       TORCH_FN(VariableType::max_pool2d_with_indices_backward_out_grad_input)
);
m.impl("max_pool3d_with_indices",
       TORCH_FN(VariableType::max_pool3d_with_indices)
);
m.impl("max_pool3d_with_indices_backward.grad_input",
       TORCH_FN(VariableType::max_pool3d_with_indices_backward_out_grad_input)
);
m.impl("mean.out",
       TORCH_FN(VariableType::mean_out_out)
);
m.impl("median",
       TORCH_FN(VariableType::median)
);
m.impl("median.dim",
       TORCH_FN(VariableType::median_dim)
);
m.impl("miopen_batch_norm",
       TORCH_FN(VariableType::miopen_batch_norm)
);
m.impl("miopen_convolution_add_relu", torch::autograd::autogradNotImplementedFallback());
m.impl("miopen_convolution_transpose",
       TORCH_FN(VariableType::miopen_convolution_transpose)
);
m.impl("miopen_rnn_backward",
       TORCH_FN(VariableType::miopen_rnn_backward)
);
m.impl("mkldnn_adaptive_avg_pool2d",
       TORCH_FN(VariableType::mkldnn_adaptive_avg_pool2d)
);
m.impl("mkldnn_linear.out", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_max_pool2d_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_max_pool2d_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_reorder_conv3d_weight", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_rnn_layer.out", torch::autograd::autogradNotImplementedFallback());
m.impl("mps_convolution_transpose_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("multi_margin_loss_backward.grad_input", torch::autograd::autogradNotImplementedFallback());
m.impl("nan_to_num_",
       TORCH_FN(VariableType::nan_to_num_)
);
m.impl("nanmedian",
       TORCH_FN(VariableType::nanmedian)
);
m.impl("nanmedian.dim",
       TORCH_FN(VariableType::nanmedian_dim)
);
m.impl("native_group_norm_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("native_layer_norm.out", torch::autograd::autogradNotImplementedFallback());
m.impl("neg_",
       TORCH_FN(VariableType::neg_)
);
m.impl("nll_loss2d_forward.output",
       TORCH_FN(VariableType::nll_loss2d_forward_out_output)
);
m.impl("nll_loss_backward.grad_input",
       TORCH_FN(VariableType::nll_loss_backward_out_grad_input)
);
m.impl("nll_loss_forward.output",
       TORCH_FN(VariableType::nll_loss_forward_out_output)
);
m.impl("permute_copy",
       TORCH_FN(VariableType::permute_copy)
);
m.impl("poisson",
       TORCH_FN(VariableType::poisson)
);
m.impl("pow.Tensor_Tensor_out",
       TORCH_FN(VariableType::pow_out_Tensor_Tensor_out)
);
m.impl("pow.Scalar_out",
       TORCH_FN(VariableType::pow_out_Scalar_out)
);
m.impl("pow.Tensor_Scalar_out",
       TORCH_FN(VariableType::pow_out_Tensor_Scalar_out)
);
m.impl("prod",
       TORCH_FN(VariableType::prod)
);
m.impl("prod.dim_int",
       TORCH_FN(VariableType::prod_dim_int)
);
m.impl("put_",
       TORCH_FN(VariableType::put_)
);
m.impl("q_per_channel_axis",
       TORCH_FN(VariableType::q_per_channel_axis)
);
m.impl("q_per_channel_zero_points.out", torch::autograd::autogradNotImplementedFallback());
m.impl("q_scale",
       TORCH_FN(VariableType::q_scale)
);
m.impl("rad2deg_",
       TORCH_FN(VariableType::rad2deg_)
);
m.impl("rand_like.out",
       TORCH_FN(VariableType::rand_like_out_out)
);
m.impl("rand.out", torch::autograd::autogradNotImplementedFallback());
m.impl("rand.names_out", torch::autograd::autogradNotImplementedFallback());
m.impl("rand.generator_with_names_out", torch::autograd::autogradNotImplementedFallback());
m.impl("randint_like.out", torch::autograd::autogradNotImplementedFallback());
m.impl("randint_like.low_dtype_out", torch::autograd::autogradNotImplementedFallback());
m.impl("randint.out", torch::autograd::autogradNotImplementedFallback());
m.impl("randint.generator_out", torch::autograd::autogradNotImplementedFallback());
m.impl("randint.low_out", torch::autograd::autogradNotImplementedFallback());
m.impl("randint.low_generator_out", torch::autograd::autogradNotImplementedFallback());
m.impl("randperm", torch::autograd::autogradNotImplementedFallback());
m.impl("randperm.generator", torch::autograd::autogradNotImplementedFallback());
m.impl("range.out_", torch::autograd::autogradNotImplementedFallback());
m.impl("range.out", torch::autograd::autogradNotImplementedFallback());
m.impl("reflection_pad2d",
       TORCH_FN(VariableType::reflection_pad2d)
);
m.impl("relu",
       TORCH_FN(VariableType::relu)
);
m.impl("remainder.Scalar",
       TORCH_FN(VariableType::remainder_Scalar)
);
m.impl("remainder.Tensor",
       TORCH_FN(VariableType::remainder_Tensor)
);
m.impl("remainder.Scalar_Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("renorm",
       TORCH_FN(VariableType::renorm)
);
m.impl("renorm_",
       TORCH_FN(VariableType::renorm_)
);
m.impl("replication_pad3d_backward.grad_input",
       TORCH_FN(VariableType::replication_pad3d_backward_out_grad_input)
);
m.impl("resize", torch::autograd::autogradNotImplementedFallback());
m.impl("round_",
       TORCH_FN(VariableType::round_)
);
m.impl("round_.decimals",
       TORCH_FN(VariableType::round__decimals)
);
m.impl("row_indices_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("scatter_reduce.two_out",
       TORCH_FN(VariableType::scatter_reduce_out_two_out)
);
m.impl("searchsorted.Tensor",
       TORCH_FN(VariableType::searchsorted_Tensor)
);
m.impl("searchsorted.Scalar",
       TORCH_FN(VariableType::searchsorted_Scalar)
);
m.impl("searchsorted.Tensor_out",
       TORCH_FN(VariableType::searchsorted_out_Tensor_out)
);
m.impl("searchsorted.Scalar_out",
       TORCH_FN(VariableType::searchsorted_out_Scalar_out)
);
m.impl("select_copy.int",
       TORCH_FN(VariableType::select_copy_int)
);
m.impl("select_scatter",
       TORCH_FN(VariableType::select_scatter)
);
m.impl("signbit",
       TORCH_FN(VariableType::signbit)
);
m.impl("silu_",
       TORCH_FN(VariableType::silu_)
);
m.impl("sinc",
       TORCH_FN(VariableType::sinc)
);
m.impl("sinc_",
       TORCH_FN(VariableType::sinc_)
);
m.impl("slice.Tensor",
       TORCH_FN(VariableType::slice_Tensor)
);
m.impl("slice_copy.Tensor",
       TORCH_FN(VariableType::slice_copy_Tensor)
);
m.impl("slow_conv3d_forward",
       TORCH_FN(VariableType::slow_conv3d_forward)
);
m.impl("slow_conv_transpose3d",
       TORCH_FN(VariableType::slow_conv_transpose3d)
);
m.impl("smooth_l1_loss",
       TORCH_FN(VariableType::smooth_l1_loss)
);
m.impl("soft_margin_loss_backward.grad_input",
       TORCH_FN(VariableType::soft_margin_loss_backward_out_grad_input)
);
m.impl("softplus",
       TORCH_FN(VariableType::softplus)
);
m.impl("softplus.out",
       TORCH_FN(VariableType::softplus_out_out)
);
m.impl("softshrink_backward.grad_input",
       TORCH_FN(VariableType::softshrink_backward_out_grad_input)
);
m.impl("softshrink.out",
       TORCH_FN(VariableType::softshrink_out_out)
);
m.impl("sparse_compressed_tensor.comp_plain_value_size",
       TORCH_FN(VariableType::sparse_compressed_tensor_comp_plain_value_size)
);
m.impl("sparse_compressed_tensor.comp_plain_value", torch::autograd::autogradNotImplementedFallback());
m.impl("sparse_coo_tensor.size", torch::autograd::autogradNotImplementedFallback());
m.impl("sparse_dim",
       TORCH_FN(VariableType::sparse_dim)
);
m.impl("sparse_resize_and_clear", torch::autograd::autogradNotImplementedFallback());
m.impl("special_chebyshev_polynomial_t",
       TORCH_FN(VariableType::special_chebyshev_polynomial_t)
);
m.impl("special_chebyshev_polynomial_t.x_scalar",
       TORCH_FN(VariableType::special_chebyshev_polynomial_t_x_scalar)
);
m.impl("special_chebyshev_polynomial_t.n_scalar",
       TORCH_FN(VariableType::special_chebyshev_polynomial_t_n_scalar)
);
m.impl("special_chebyshev_polynomial_v.out",
       TORCH_FN(VariableType::special_chebyshev_polynomial_v_out_out)
);
m.impl("special_chebyshev_polynomial_v.x_scalar_out",
       TORCH_FN(VariableType::special_chebyshev_polynomial_v_out_x_scalar_out)
);
m.impl("special_chebyshev_polynomial_v.n_scalar_out",
       TORCH_FN(VariableType::special_chebyshev_polynomial_v_out_n_scalar_out)
);
m.impl("special_chebyshev_polynomial_w.out",
       TORCH_FN(VariableType::special_chebyshev_polynomial_w_out_out)
);
m.impl("special_chebyshev_polynomial_w.x_scalar_out",
       TORCH_FN(VariableType::special_chebyshev_polynomial_w_out_x_scalar_out)
);
m.impl("special_chebyshev_polynomial_w.n_scalar_out",
       TORCH_FN(VariableType::special_chebyshev_polynomial_w_out_n_scalar_out)
);
m.impl("special_erfcx",
       TORCH_FN(VariableType::special_erfcx)
);
m.impl("special_erfcx.out",
       TORCH_FN(VariableType::special_erfcx_out_out)
);
m.impl("special_i0e",
       TORCH_FN(VariableType::special_i0e)
);
m.impl("special_i1",
       TORCH_FN(VariableType::special_i1)
);
m.impl("special_legendre_polynomial_p.out",
       TORCH_FN(VariableType::special_legendre_polynomial_p_out_out)
);
m.impl("special_legendre_polynomial_p.x_scalar_out",
       TORCH_FN(VariableType::special_legendre_polynomial_p_out_x_scalar_out)
);
m.impl("special_legendre_polynomial_p.n_scalar_out",
       TORCH_FN(VariableType::special_legendre_polynomial_p_out_n_scalar_out)
);
m.impl("special_log_ndtr.out",
       TORCH_FN(VariableType::special_log_ndtr_out_out)
);
m.impl("special_modified_bessel_i0.out",
       TORCH_FN(VariableType::special_modified_bessel_i0_out_out)
);
m.impl("special_scaled_modified_bessel_k1",
       TORCH_FN(VariableType::special_scaled_modified_bessel_k1)
);
m.impl("special_shifted_chebyshev_polynomial_v.out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_v_out_out)
);
m.impl("special_shifted_chebyshev_polynomial_v.x_scalar_out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_v_out_x_scalar_out)
);
m.impl("special_shifted_chebyshev_polynomial_v.n_scalar_out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_v_out_n_scalar_out)
);
m.impl("split_with_sizes_copy",
       TORCH_FN(VariableType::split_with_sizes_copy)
);
m.impl("sspaddmm.out", torch::autograd::autogradNotImplementedFallback());
m.impl("sub.Tensor",
       TORCH_FN(VariableType::sub_Tensor)
);
m.impl("sub.Scalar",
       TORCH_FN(VariableType::sub_Scalar)
);
m.impl("t_copy",
       TORCH_FN(VariableType::t_copy)
);
m.impl("take",
       TORCH_FN(VariableType::take)
);
m.impl("tan",
       TORCH_FN(VariableType::tan)
);
m.impl("tan.out",
       TORCH_FN(VariableType::tan_out_out)
);
m.impl("trace",
       TORCH_FN(VariableType::trace)
);
m.impl("triu_indices", torch::autograd::autogradNotImplementedFallback());
m.impl("trunc.out",
       TORCH_FN(VariableType::trunc_out_out)
);
m.impl("unbind_copy.int",
       TORCH_FN(VariableType::unbind_copy_int)
);
m.impl("unfold",
       TORCH_FN(VariableType::unfold)
);
m.impl("unfold_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("uniform.out", torch::autograd::autogradNotImplementedFallback());
m.impl("unique_consecutive.out", torch::autograd::autogradNotImplementedFallback());
m.impl("unique_dim_consecutive",
       TORCH_FN(VariableType::unique_dim_consecutive)
);
m.impl("unsafe_split.Tensor_out",
       TORCH_FN(VariableType::unsafe_split_out_Tensor_out)
);
m.impl("unsafe_split_with_sizes",
       TORCH_FN(VariableType::unsafe_split_with_sizes)
);
m.impl("unsqueeze",
       TORCH_FN(VariableType::unsqueeze)
);
m.impl("unsqueeze_",
       TORCH_FN(VariableType::unsqueeze_)
);
m.impl("unsqueeze_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("upsample_linear1d_backward",
       TORCH_FN(VariableType::upsample_linear1d_backward)
);
m.impl("upsample_nearest1d_backward",
       TORCH_FN(VariableType::upsample_nearest1d_backward)
);
m.impl("upsample_nearest2d_backward",
       TORCH_FN(VariableType::upsample_nearest2d_backward)
);
m.impl("upsample_trilinear3d.out",
       TORCH_FN(VariableType::upsample_trilinear3d_out_out)
);
m.impl("vdot.out",
       TORCH_FN(VariableType::vdot_out_out)
);
m.impl("view_as_complex",
       TORCH_FN(VariableType::view_as_complex)
);
m.impl("view_as_complex_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("view_as_real",
       TORCH_FN(VariableType::view_as_real)
);
m.impl("zeros_like.out",
       TORCH_FN(VariableType::zeros_like_out_out)
);
m.impl("zeros.out", torch::autograd::autogradNotImplementedFallback());
m.impl("zeros.names_out", torch::autograd::autogradNotImplementedFallback());
}

}

} // namespace torch::autograd
