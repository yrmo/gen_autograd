#include "torch/csrc/autograd/VariableTypeUtils.h"
#include "torch/csrc/autograd/generated/VariableType.h"
#include "torch/csrc/autograd/FunctionsManual.h"

#include <ATen/RedispatchFunctions.h>
#include <c10/core/impl/TorchDispatchModeTLS.h>
#include <ATen/core/TorchDispatchUtils.h>
#include <torch/library.h>

#include <ATen/SparseCsrTensorUtils.h>


// @generated by torchgen/gen.py from VariableType.cpp

// NOTE [Sharded File]: on this file's split-into-shards state
//
// Back in the good old days, VariableType.cpp was generated as one
// file with every function in it, and everything was great and
// simple.
//
// However, this file was also very large (over 36,000 lines), and
// compiling it was very slow, and in fact was a significant
// bottleneck for incremental rebuilds. To address this, we now
// generate the file split across multiple shards, named
// VariableType_0.cpp and so on, which can be compiled in parallel.
//
// For ease of inspection and debugging, so that it's not necessary to
// go rooting around in multiple files, we also generate all the
// functions together in VariableTypeEverything.cpp. This generated
// file is only for convenience; it's not actually used in the
// build. If the file you're looking at now is one of the shards, you
// may want to switch over to the Everything variant to make you
// grepping smoother.

using namespace at;
using namespace torch::autograd::generated;
using namespace torch::autograd::generated::details;


namespace torch::autograd {

namespace VariableType {
namespace{
  C10_UNUSED void reset_grad_accumulator(Variable & self) {
    AutogradMeta* meta = torch::autograd::impl::get_autograd_meta(self);
    if (meta != nullptr) {
      meta->grad_accumulator_.reset();
    }
  }
}

namespace {


at::Tensor _test_autograd_multiple_dispatch_fullcoverage_AutogradCUDA(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<TestAutogradMultipleDispatchBackwardAutogradCUDA0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TestAutogradMultipleDispatchBackwardAutogradCUDA0>(new TestAutogradMultipleDispatchBackwardAutogradCUDA0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_test_autograd_multiple_dispatch", "fullcoverage");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_test_autograd_multiple_dispatch", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_test_autograd_multiple_dispatch(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _test_autograd_multiple_dispatch_fullcoverage");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _test_autograd_multiple_dispatch_fullcoverage");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_test_autograd_multiple_dispatch");
  return result;
}

at::Tensor _test_autograd_multiple_dispatch_fullcoverage_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<TestAutogradMultipleDispatchBackwardAutogradNestedTensor0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TestAutogradMultipleDispatchBackwardAutogradNestedTensor0>(new TestAutogradMultipleDispatchBackwardAutogradNestedTensor0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_test_autograd_multiple_dispatch", "fullcoverage");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_test_autograd_multiple_dispatch", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_test_autograd_multiple_dispatch(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _test_autograd_multiple_dispatch_fullcoverage");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _test_autograd_multiple_dispatch_fullcoverage");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_test_autograd_multiple_dispatch");
  return result;
}
at::Tensor _test_autograd_multiple_dispatch_ntonly_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, bool b) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<TestAutogradMultipleDispatchBackwardAutogradNestedTensor1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TestAutogradMultipleDispatchBackwardAutogradNestedTensor1>(new TestAutogradMultipleDispatchBackwardAutogradNestedTensor1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_test_autograd_multiple_dispatch", "ntonly");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_test_autograd_multiple_dispatch", *opt_op, ks, self, b);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_test_autograd_multiple_dispatch(ks & c10::after_autograd_keyset, self_, b);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _test_autograd_multiple_dispatch_ntonly");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _test_autograd_multiple_dispatch_ntonly");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_test_autograd_multiple_dispatch");
  return result;
}
at::Tensor squeeze_dim_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<SqueezeBackwardAutogradNestedTensor0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqueezeBackwardAutogradNestedTensor0>(new SqueezeBackwardAutogradNestedTensor0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::squeeze", "dim");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("squeeze", *opt_op, ks, self, dim);
    } else {
      at::AutoDispatchBelowAutograd guard;
      return at::redispatch::squeeze(ks & c10::after_autograd_keyset, self_, dim);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: squeeze_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  return result;
}
at::Tensor squeeze_dims_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<SqueezeBackwardAutogradNestedTensor1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqueezeBackwardAutogradNestedTensor1>(new SqueezeBackwardAutogradNestedTensor1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim.vec();
    grad_fn->self_dim = self.dim();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::squeeze", "dims");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("squeeze", *opt_op, ks, self, dim);
    } else {
      at::AutoDispatchBelowAutograd guard;
      return at::redispatch::squeeze(ks & c10::after_autograd_keyset, self_, dim);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: squeeze_dims");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  return result;
}
::std::vector<at::Tensor> unbind_int_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UnbindBackwardAutogradNestedTensor0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnbindBackwardAutogradNestedTensor0>(new UnbindBackwardAutogradNestedTensor0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->self_options = self.options();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::unbind(ks & c10::after_autograd_keyset, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<::std::vector<at::Tensor>> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::unbind(self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value()) {
    auto result_new_fw_grad = result_new_fw_grad_opt.value();
    TORCH_INTERNAL_ASSERT(result.size() == result_new_fw_grad.size());
    for (const auto i : c10::irange(result.size())) {
      if (result_new_fw_grad[i].defined() && result[i].defined()) {
        // The hardcoded 0 here will need to be updated once we support multiple levels.
        result[i]._set_fw_grad(result_new_fw_grad[i], /* level */ 0, /* is_inplace_op */ false);
      }
    }
  }
  return result;
}
void unbind_copy_out_int_out_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, at::TensorList out) {
  auto& self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::unbind_copy_outf(ks & c10::after_autograd_keyset, self_, dim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with unbind_copy_out that does not support it because it is an out= function");
}
at::Tensor values_AutogradNestedTensor(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<ValuesBackwardAutogradNestedTensor0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ValuesBackwardAutogradNestedTensor0>(new ValuesBackwardAutogradNestedTensor0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::values", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("values", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowAutograd guard;
      return at::redispatch::values(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: values");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  return result;
}

void _amp_foreach_non_finite_check_and_unscale_(c10::DispatchKeySet ks, at::TensorList self, at::Tensor & found_inf, const at::Tensor & inv_scale) {
  auto self_ = unpack(self, "self", 0);
  auto& found_inf_ = unpack(found_inf, "found_inf", 1);
  auto& inv_scale_ = unpack(inv_scale, "inv_scale", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  c10::optional<Storage> found_inf__storage_saved =
    found_inf_.has_storage() ? c10::optional<Storage>(found_inf_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> found_inf__impl_saved;
  if (found_inf_.defined()) found_inf__impl_saved = found_inf_.getIntrusivePtr();
  c10::optional<Storage> inv_scale__storage_saved =
    inv_scale_.has_storage() ? c10::optional<Storage>(inv_scale_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> inv_scale__impl_saved;
  if (inv_scale_.defined()) inv_scale__impl_saved = inv_scale_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_amp_foreach_non_finite_check_and_unscale_(ks & c10::after_autograd_keyset, self_, found_inf_, inv_scale_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  if (found_inf__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(found_inf_))
    TORCH_INTERNAL_ASSERT(found_inf__storage_saved.value().is_alias_of(found_inf_.storage()));
  if (found_inf__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(found_inf_))
    TORCH_INTERNAL_ASSERT(found_inf__impl_saved == found_inf_.getIntrusivePtr());
  if (inv_scale__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(inv_scale_))
    TORCH_INTERNAL_ASSERT(inv_scale__storage_saved.value().is_alias_of(inv_scale_.storage()));
  if (inv_scale__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(inv_scale_))
    TORCH_INTERNAL_ASSERT(inv_scale__impl_saved == inv_scale_.getIntrusivePtr());
  #endif
}
at::Tensor _conj(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ConjBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ConjBackward0>(new ConjBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::_conj(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _conj");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t.conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _conj_physical(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ConjPhysicalBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ConjPhysicalBackward0>(new ConjPhysicalBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_conj_physical(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _conj_physical");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _conj_physical");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t.conj_physical();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
const at::Tensor & _conv_depthwise2d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, const at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  auto& out_ = unpack(out, "out", 7);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, weight, bias )) {
    throw_error_out_requires_grad("_conv_depthwise2d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("_conv_depthwise2d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_conv_depthwise2d_symint_outf(ks & c10::after_autograd_keyset, self_, weight_, kernel_size, bias, stride, padding, dilation, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias) || isFwGradDefined(out))), "Trying to use forward AD with _conv_depthwise2d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor _convolution(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias));
  std::shared_ptr<ConvolutionBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ConvolutionBackward1>(new ConvolutionBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->bias_sym_sizes_opt = bias.has_value() ? c10::optional<c10::SymIntArrayRef>(bias->sym_sizes()) : c10::nullopt;
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->output_padding = output_padding.vec();
    grad_fn->padding = padding.vec();
    grad_fn->stride = stride.vec();
    grad_fn->transposed = transposed;
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_convolution_symint(ks & c10::after_autograd_keyset, input_, weight_, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _convolution");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _convolution");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_convolution");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto input_t_raw = toNonOptFwGrad(input);
      auto input_tensor = toNonOptTensor(input);
      auto input_t = (input_t_raw.defined() || !input_tensor.defined())
        ? input_t_raw : at::_efficientzerotensor(input_tensor.sizes(), input_tensor.options());
      auto input_p = toNonOptPrimal(input);
      auto weight_t_raw = toNonOptFwGrad(weight);
      auto weight_tensor = toNonOptTensor(weight);
      auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
        ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
      auto weight_p = toNonOptPrimal(weight);
      auto bias_t_raw = toNonOptFwGrad(bias);
      auto bias_tensor = toNonOptTensor(bias);
      auto bias_t = (bias_t_raw.defined() || !bias_tensor.defined())
        ? bias_t_raw : at::_efficientzerotensor(bias_tensor.sizes(), bias_tensor.options());
      auto bias_p = toNonOptPrimal(bias);
      result_new_fw_grad_opt = _convolution_jvp(input_p, input_t, weight_p, weight_t, bias_p, bias_t, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor> _ctc_loss(c10::DispatchKeySet ks, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool zero_infinity) {
  auto& log_probs_ = unpack(log_probs, "log_probs", 0);
  auto& targets_ = unpack(targets, "targets", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( log_probs );
  
  check_no_requires_grad(targets, "targets", "_ctc_loss");
  std::shared_ptr<CtcLossBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CtcLossBackward0>(new CtcLossBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( log_probs ));
    grad_fn->blank = blank;
    grad_fn->input_lengths = input_lengths.vec();
    grad_fn->log_probs_ = SavedVariable(log_probs, false);
    grad_fn->target_lengths = target_lengths.vec();
    grad_fn->targets_ = SavedVariable(targets, false);
    grad_fn->zero_infinity = zero_infinity;
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> log_probs__storage_saved =
    log_probs_.has_storage() ? c10::optional<Storage>(log_probs_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> log_probs__impl_saved;
  if (log_probs_.defined()) log_probs__impl_saved = log_probs_.getIntrusivePtr();
  c10::optional<Storage> targets__storage_saved =
    targets_.has_storage() ? c10::optional<Storage>(targets_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> targets__impl_saved;
  if (targets_.defined()) targets__impl_saved = targets_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(log_probs) || isFwGradDefined(targets))) {
      static c10::OperatorName full_name("aten::_ctc_loss", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_ctc_loss", *opt_op, ks, log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_ctc_loss(ks & c10::after_autograd_keyset, log_probs_, targets_, input_lengths, target_lengths, blank, zero_infinity);
    }
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (log_probs__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__storage_saved.value().is_alias_of(log_probs_.storage()));
  if (log_probs__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__impl_saved == log_probs_.getIntrusivePtr());
  if (targets__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__storage_saved.value().is_alias_of(targets_.storage()));
  if (targets__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__impl_saved == targets_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _ctc_loss");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _ctc_loss");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _ctc_loss");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _ctc_loss");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_ctc_loss");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
::std::tuple<at::Tensor,at::Tensor> _ctc_loss_Tensor(c10::DispatchKeySet ks, const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, bool zero_infinity) {
  auto& log_probs_ = unpack(log_probs, "log_probs", 0);
  auto& targets_ = unpack(targets, "targets", 1);
  auto& input_lengths_ = unpack(input_lengths, "input_lengths", 2);
  auto& target_lengths_ = unpack(target_lengths, "target_lengths", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( log_probs );
  
  check_no_requires_grad(targets, "targets", "_ctc_loss");
  check_no_requires_grad(input_lengths, "input_lengths", "_ctc_loss");
  check_no_requires_grad(target_lengths, "target_lengths", "_ctc_loss");
  std::shared_ptr<CtcLossBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CtcLossBackward1>(new CtcLossBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( log_probs ));
    grad_fn->blank = blank;
    grad_fn->input_lengths_ = SavedVariable(input_lengths, false);
    grad_fn->log_probs_ = SavedVariable(log_probs, false);
    grad_fn->target_lengths_ = SavedVariable(target_lengths, false);
    grad_fn->targets_ = SavedVariable(targets, false);
    grad_fn->zero_infinity = zero_infinity;
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> log_probs__storage_saved =
    log_probs_.has_storage() ? c10::optional<Storage>(log_probs_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> log_probs__impl_saved;
  if (log_probs_.defined()) log_probs__impl_saved = log_probs_.getIntrusivePtr();
  c10::optional<Storage> targets__storage_saved =
    targets_.has_storage() ? c10::optional<Storage>(targets_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> targets__impl_saved;
  if (targets_.defined()) targets__impl_saved = targets_.getIntrusivePtr();
  c10::optional<Storage> input_lengths__storage_saved =
    input_lengths_.has_storage() ? c10::optional<Storage>(input_lengths_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input_lengths__impl_saved;
  if (input_lengths_.defined()) input_lengths__impl_saved = input_lengths_.getIntrusivePtr();
  c10::optional<Storage> target_lengths__storage_saved =
    target_lengths_.has_storage() ? c10::optional<Storage>(target_lengths_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target_lengths__impl_saved;
  if (target_lengths_.defined()) target_lengths__impl_saved = target_lengths_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(log_probs) || isFwGradDefined(targets) || isFwGradDefined(input_lengths) || isFwGradDefined(target_lengths))) {
      static c10::OperatorName full_name("aten::_ctc_loss", "Tensor");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_ctc_loss", *opt_op, ks, log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_ctc_loss(ks & c10::after_autograd_keyset, log_probs_, targets_, input_lengths_, target_lengths_, blank, zero_infinity);
    }
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (log_probs__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__storage_saved.value().is_alias_of(log_probs_.storage()));
  if (log_probs__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__impl_saved == log_probs_.getIntrusivePtr());
  if (targets__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__storage_saved.value().is_alias_of(targets_.storage()));
  if (targets__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__impl_saved == targets_.getIntrusivePtr());
  if (input_lengths__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_lengths_))
    TORCH_INTERNAL_ASSERT(input_lengths__storage_saved.value().is_alias_of(input_lengths_.storage()));
  if (input_lengths__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_lengths_))
    TORCH_INTERNAL_ASSERT(input_lengths__impl_saved == input_lengths_.getIntrusivePtr());
  if (target_lengths__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_lengths_))
    TORCH_INTERNAL_ASSERT(target_lengths__storage_saved.value().is_alias_of(target_lengths_.storage()));
  if (target_lengths__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_lengths_))
    TORCH_INTERNAL_ASSERT(target_lengths__impl_saved == target_lengths_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _ctc_loss_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _ctc_loss_Tensor");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _ctc_loss_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _ctc_loss_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_ctc_loss");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
::std::tuple<at::Tensor,at::Tensor> _cudnn_ctc_loss(c10::DispatchKeySet ks, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) {
  auto& log_probs_ = unpack(log_probs, "log_probs", 0);
  auto& targets_ = unpack(targets, "targets", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( log_probs );
  
  check_no_requires_grad(targets, "targets", "_cudnn_ctc_loss");
  std::shared_ptr<CudnnCtcLossBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CudnnCtcLossBackward0>(new CudnnCtcLossBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( log_probs ));
    grad_fn->zero_infinity = zero_infinity;
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> log_probs__storage_saved =
    log_probs_.has_storage() ? c10::optional<Storage>(log_probs_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> log_probs__impl_saved;
  if (log_probs_.defined()) log_probs__impl_saved = log_probs_.getIntrusivePtr();
  c10::optional<Storage> targets__storage_saved =
    targets_.has_storage() ? c10::optional<Storage>(targets_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> targets__impl_saved;
  if (targets_.defined()) targets__impl_saved = targets_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(log_probs) || isFwGradDefined(targets))) {
      static c10::OperatorName full_name("aten::_cudnn_ctc_loss", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_cudnn_ctc_loss", *opt_op, ks, log_probs, targets, input_lengths, target_lengths, blank, deterministic, zero_infinity);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_cudnn_ctc_loss(ks & c10::after_autograd_keyset, log_probs_, targets_, input_lengths, target_lengths, blank, deterministic, zero_infinity);
    }
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (log_probs__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__storage_saved.value().is_alias_of(log_probs_.storage()));
  if (log_probs__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__impl_saved == log_probs_.getIntrusivePtr());
  if (targets__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__storage_saved.value().is_alias_of(targets_.storage()));
  if (targets__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__impl_saved == targets_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _cudnn_ctc_loss");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _cudnn_ctc_loss");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _cudnn_ctc_loss");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _cudnn_ctc_loss");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_cudnn_ctc_loss");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
::std::tuple<at::Tensor,at::Tensor> _cudnn_ctc_loss_Tensor(c10::DispatchKeySet ks, const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, bool deterministic, bool zero_infinity) {
  auto& log_probs_ = unpack(log_probs, "log_probs", 0);
  auto& targets_ = unpack(targets, "targets", 1);
  auto& input_lengths_ = unpack(input_lengths, "input_lengths", 2);
  auto& target_lengths_ = unpack(target_lengths, "target_lengths", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( log_probs );
  
  check_no_requires_grad(targets, "targets", "_cudnn_ctc_loss");
  check_no_requires_grad(input_lengths, "input_lengths", "_cudnn_ctc_loss");
  check_no_requires_grad(target_lengths, "target_lengths", "_cudnn_ctc_loss");
  std::shared_ptr<CudnnCtcLossBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CudnnCtcLossBackward1>(new CudnnCtcLossBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( log_probs ));
    grad_fn->zero_infinity = zero_infinity;
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> log_probs__storage_saved =
    log_probs_.has_storage() ? c10::optional<Storage>(log_probs_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> log_probs__impl_saved;
  if (log_probs_.defined()) log_probs__impl_saved = log_probs_.getIntrusivePtr();
  c10::optional<Storage> targets__storage_saved =
    targets_.has_storage() ? c10::optional<Storage>(targets_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> targets__impl_saved;
  if (targets_.defined()) targets__impl_saved = targets_.getIntrusivePtr();
  c10::optional<Storage> input_lengths__storage_saved =
    input_lengths_.has_storage() ? c10::optional<Storage>(input_lengths_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input_lengths__impl_saved;
  if (input_lengths_.defined()) input_lengths__impl_saved = input_lengths_.getIntrusivePtr();
  c10::optional<Storage> target_lengths__storage_saved =
    target_lengths_.has_storage() ? c10::optional<Storage>(target_lengths_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target_lengths__impl_saved;
  if (target_lengths_.defined()) target_lengths__impl_saved = target_lengths_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(log_probs) || isFwGradDefined(targets) || isFwGradDefined(input_lengths) || isFwGradDefined(target_lengths))) {
      static c10::OperatorName full_name("aten::_cudnn_ctc_loss", "Tensor");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_cudnn_ctc_loss", *opt_op, ks, log_probs, targets, input_lengths, target_lengths, blank, deterministic, zero_infinity);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_cudnn_ctc_loss(ks & c10::after_autograd_keyset, log_probs_, targets_, input_lengths_, target_lengths_, blank, deterministic, zero_infinity);
    }
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (log_probs__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__storage_saved.value().is_alias_of(log_probs_.storage()));
  if (log_probs__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__impl_saved == log_probs_.getIntrusivePtr());
  if (targets__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__storage_saved.value().is_alias_of(targets_.storage()));
  if (targets__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__impl_saved == targets_.getIntrusivePtr());
  if (input_lengths__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_lengths_))
    TORCH_INTERNAL_ASSERT(input_lengths__storage_saved.value().is_alias_of(input_lengths_.storage()));
  if (input_lengths__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_lengths_))
    TORCH_INTERNAL_ASSERT(input_lengths__impl_saved == input_lengths_.getIntrusivePtr());
  if (target_lengths__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_lengths_))
    TORCH_INTERNAL_ASSERT(target_lengths__storage_saved.value().is_alias_of(target_lengths_.storage()));
  if (target_lengths__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_lengths_))
    TORCH_INTERNAL_ASSERT(target_lengths__impl_saved == target_lengths_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _cudnn_ctc_loss_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _cudnn_ctc_loss_Tensor");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _cudnn_ctc_loss_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _cudnn_ctc_loss_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_cudnn_ctc_loss");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _cudnn_rnn(c10::DispatchKeySet ks, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const c10::optional<at::Tensor> & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, c10::SymIntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state) {
  auto& input_ = unpack(input, "input", 0);
  auto weight_ = unpack(weight, "weight", 1);
  auto& hx_ = unpack(hx, "hx", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, hx, cx );
  
  check_no_requires_grad(weight_buf, "weight_buf", "_cudnn_rnn");
  std::shared_ptr<CudnnRnnBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CudnnRnnBackward0>(new CudnnRnnBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, hx, cx ));
    grad_fn->batch_first = batch_first;
    grad_fn->batch_sizes = batch_sizes.vec();
    grad_fn->bidirectional = bidirectional;
    grad_fn->cx_ = SavedVariable(cx, false);
    grad_fn->dropout = dropout;
    grad_fn->dropout_state_ = SavedVariable(dropout_state, false);
    grad_fn->hidden_size = hidden_size;
    grad_fn->hx_ = SavedVariable(hx, false);
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->mode = mode;
    grad_fn->num_layers = num_layers;
    grad_fn->proj_size = proj_size;
    grad_fn->train = train;
    grad_fn->weight_ = make_saved_variable_list(weight, false);
    grad_fn->weight_stride0 = weight_stride0;
    grad_fn->weight_size_ = weight.size();
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  at::Tensor result3;
  at::Tensor result4;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> weight__storage_saved(weight_.size());
  for (const Tensor& tensor : weight_)
    weight__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> weight__impl_saved(weight_.size());
  for (size_t i=0; i<weight_.size(); i++)
    if (weight_[i].defined()) weight__impl_saved[i] = weight_[i].getIntrusivePtr();
  c10::optional<Storage> hx__storage_saved =
    hx_.has_storage() ? c10::optional<Storage>(hx_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> hx__impl_saved;
  if (hx_.defined()) hx__impl_saved = hx_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input) || isFwGradDefinedTensorList(weight) || isFwGradDefined(weight_buf) || isFwGradDefined(hx) || isFwGradDefined(cx))) {
      static c10::OperatorName full_name("aten::_cudnn_rnn", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>>("_cudnn_rnn", *opt_op, ks, input, weight, weight_stride0, weight_buf, hx, cx, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_cudnn_rnn_symint(ks & c10::after_autograd_keyset, input_, weight_, weight_stride0, weight_buf, hx_, cx, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state);
    }
  })();
  std::tie(result0, result1, result2, result3, result4) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  for (size_t i=0; i<weight_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weight__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(weight_))
      TORCH_INTERNAL_ASSERT(weight__storage_saved[i].value().is_alias_of(weight_[i].storage()));
  }
  for (size_t i=0; i<weight_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weight__impl_saved[i] && !at::impl::tensorlist_has_dispatch(weight_))
      TORCH_INTERNAL_ASSERT(weight__impl_saved[i] == weight_[i].getIntrusivePtr());
  }
  if (hx__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__storage_saved.value().is_alias_of(hx_.storage()));
  if (hx__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__impl_saved == hx_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_cudnn_rnn");
  throw_error_for_complex_autograd(result1, "_cudnn_rnn");
  throw_error_for_complex_autograd(result2, "_cudnn_rnn");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result3_ = SavedVariable(result3, true);
    grad_fn->result4_ = SavedVariable(result4, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3), std::move(result4));
}
void _cudnn_rnn_backward_out_out(c10::DispatchKeySet ks, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, c10::SymIntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::TensorList out3) {
  auto& input_ = unpack(input, "input", 0);
  auto weight_ = unpack(weight, "weight", 1);
  auto& weight_buf_ = unpack(weight_buf, "weight_buf", 3);
  auto& hx_ = unpack(hx, "hx", 4);
  auto& output_ = unpack(output, "output", 6);
  auto& reserve_ = unpack(reserve, "reserve", 20);
  auto& out0_ = unpack(out0, "out0", 22);
  auto& out1_ = unpack(out1, "out1", 23);
  auto& out2_ = unpack(out2, "out2", 24);
  auto out3_ = unpack(out3, "out3", 25);
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> weight__storage_saved(weight_.size());
  for (const Tensor& tensor : weight_)
    weight__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> weight__impl_saved(weight_.size());
  for (size_t i=0; i<weight_.size(); i++)
    if (weight_[i].defined()) weight__impl_saved[i] = weight_[i].getIntrusivePtr();
  c10::optional<Storage> weight_buf__storage_saved =
    weight_buf_.has_storage() ? c10::optional<Storage>(weight_buf_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight_buf__impl_saved;
  if (weight_buf_.defined()) weight_buf__impl_saved = weight_buf_.getIntrusivePtr();
  c10::optional<Storage> hx__storage_saved =
    hx_.has_storage() ? c10::optional<Storage>(hx_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> hx__impl_saved;
  if (hx_.defined()) hx__impl_saved = hx_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> reserve__storage_saved =
    reserve_.has_storage() ? c10::optional<Storage>(reserve_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> reserve__impl_saved;
  if (reserve_.defined()) reserve__impl_saved = reserve_.getIntrusivePtr();
  c10::optional<Storage> out0__storage_saved =
    out0_.has_storage() ? c10::optional<Storage>(out0_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out0__impl_saved;
  if (out0_.defined()) out0__impl_saved = out0_.getIntrusivePtr();
  c10::optional<Storage> out1__storage_saved =
    out1_.has_storage() ? c10::optional<Storage>(out1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out1__impl_saved;
  if (out1_.defined()) out1__impl_saved = out1_.getIntrusivePtr();
  c10::optional<Storage> out2__storage_saved =
    out2_.has_storage() ? c10::optional<Storage>(out2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out2__impl_saved;
  if (out2_.defined()) out2__impl_saved = out2_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out3__storage_saved(out3_.size());
  for (const Tensor& tensor : out3_)
    out3__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out3__impl_saved(out3_.size());
  for (size_t i=0; i<out3_.size(); i++)
    if (out3_[i].defined()) out3__impl_saved[i] = out3_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_cudnn_rnn_backward_symint_outf(ks & c10::after_autograd_keyset, input_, weight_, weight_stride0, weight_buf_, hx_, cx, output_, grad_output, grad_hy, grad_cy, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve_, output_mask, out0_, out1_, out2_, out3_);
  }
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  for (size_t i=0; i<weight_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weight__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(weight_))
      TORCH_INTERNAL_ASSERT(weight__storage_saved[i].value().is_alias_of(weight_[i].storage()));
  }
  for (size_t i=0; i<weight_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weight__impl_saved[i] && !at::impl::tensorlist_has_dispatch(weight_))
      TORCH_INTERNAL_ASSERT(weight__impl_saved[i] == weight_[i].getIntrusivePtr());
  }
  if (weight_buf__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_buf_))
    TORCH_INTERNAL_ASSERT(weight_buf__storage_saved.value().is_alias_of(weight_buf_.storage()));
  if (weight_buf__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_buf_))
    TORCH_INTERNAL_ASSERT(weight_buf__impl_saved == weight_buf_.getIntrusivePtr());
  if (hx__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__storage_saved.value().is_alias_of(hx_.storage()));
  if (hx__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(hx_))
    TORCH_INTERNAL_ASSERT(hx__impl_saved == hx_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (reserve__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(reserve_))
    TORCH_INTERNAL_ASSERT(reserve__storage_saved.value().is_alias_of(reserve_.storage()));
  if (reserve__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(reserve_))
    TORCH_INTERNAL_ASSERT(reserve__impl_saved == reserve_.getIntrusivePtr());
  if (out0__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out0_))
    TORCH_INTERNAL_ASSERT(out0__storage_saved.value().is_alias_of(out0_.storage()));
  if (out0__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out0_))
    TORCH_INTERNAL_ASSERT(out0__impl_saved == out0_.getIntrusivePtr());
  if (out1__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out1_))
    TORCH_INTERNAL_ASSERT(out1__storage_saved.value().is_alias_of(out1_.storage()));
  if (out1__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out1_))
    TORCH_INTERNAL_ASSERT(out1__impl_saved == out1_.getIntrusivePtr());
  if (out2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out2_))
    TORCH_INTERNAL_ASSERT(out2__storage_saved.value().is_alias_of(out2_.storage()));
  if (out2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out2_))
    TORCH_INTERNAL_ASSERT(out2__impl_saved == out2_.getIntrusivePtr());
  for (size_t i=0; i<out3_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out3__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out3_))
      TORCH_INTERNAL_ASSERT(out3__storage_saved[i].value().is_alias_of(out3_[i].storage()));
  }
  for (size_t i=0; i<out3_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out3__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out3_))
      TORCH_INTERNAL_ASSERT(out3__impl_saved[i] == out3_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(input) || isFwGradDefinedTensorList(weight) || isFwGradDefined(weight_buf) || isFwGradDefined(hx) || isFwGradDefined(cx) || isFwGradDefined(output) || isFwGradDefined(grad_output) || isFwGradDefined(grad_hy) || isFwGradDefined(grad_cy) || isFwGradDefined(dropout_state) || isFwGradDefined(reserve) || isFwGradDefined(out0) || isFwGradDefined(out1) || isFwGradDefined(out2) || isFwGradDefinedTensorList(out3))), "Trying to use forward AD with _cudnn_rnn_backward_out that does not support it because it is an out= function");
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _embedding_bag(c10::DispatchKeySet ks, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
  auto& weight_ = unpack(weight, "weight", 0);
  auto& indices_ = unpack(indices, "indices", 1);
  auto& offsets_ = unpack(offsets, "offsets", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( weight, per_sample_weights );
  
  std::shared_ptr<EmbeddingBagBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<EmbeddingBagBackward0>(new EmbeddingBagBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( weight, per_sample_weights ));
    grad_fn->indices_ = SavedVariable(indices, false);
    grad_fn->mode = mode;
    grad_fn->offsets_ = SavedVariable(offsets, false);
    grad_fn->padding_idx = padding_idx;
    grad_fn->per_sample_weights_ = SavedVariable(per_sample_weights, false);
    grad_fn->scale_grad_by_freq = scale_grad_by_freq;
    grad_fn->sparse = sparse;
    if (grad_fn->should_compute_output(1)) {
      grad_fn->weight_ = SavedVariable(weight, false);
    }
    grad_fn->weight_sym_argsize_0 = weight.sym_size(0);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  at::Tensor result3;
  #ifndef NDEBUG
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  c10::optional<Storage> offsets__storage_saved =
    offsets_.has_storage() ? c10::optional<Storage>(offsets_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> offsets__impl_saved;
  if (offsets_.defined()) offsets__impl_saved = offsets_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(weight) || isFwGradDefined(per_sample_weights))) {
      static c10::OperatorName full_name("aten::_embedding_bag", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>>("_embedding_bag", *opt_op, ks, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_embedding_bag(ks & c10::after_autograd_keyset, weight_, indices_, offsets_, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
    }
  })();
  std::tie(result0, result1, result2, result3) = std::move(_tmp);
  #ifndef NDEBUG
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (offsets__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(offsets_))
    TORCH_INTERNAL_ASSERT(offsets__storage_saved.value().is_alias_of(offsets_.storage()));
  if (offsets__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(offsets_))
    TORCH_INTERNAL_ASSERT(offsets__impl_saved == offsets_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_embedding_bag");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
    grad_fn->result3_ = SavedVariable(result3, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3));
}
at::Tensor & _fft_r2c_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("_fft_r2c");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("_fft_r2c");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_fft_r2c_outf(ks & c10::after_autograd_keyset, self_, dim, normalization, onesided, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with _fft_r2c_out that does not support it because it is an out= function");
  return out;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _flash_attention_forward(c10::DispatchKeySet ks, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & cum_seq_q, const c10::optional<at::Tensor> & cum_seq_k, c10::SymInt max_q, c10::SymInt max_k, double dropout_p, bool is_causal, bool return_debug_mask, c10::optional<double> scale) {
  auto& query_ = unpack(query, "query", 0);
  auto& key_ = unpack(key, "key", 1);
  auto& value_ = unpack(value, "value", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( query, key, value );
  
  check_no_requires_grad(cum_seq_q, "cum_seq_q", "_flash_attention_forward");
  check_no_requires_grad(cum_seq_k, "cum_seq_k", "_flash_attention_forward");
  std::shared_ptr<FlashAttentionBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FlashAttentionBackward0>(new FlashAttentionBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( query, key, value ));
    grad_fn->cum_seq_k_ = SavedVariable(cum_seq_k, false);
    grad_fn->cum_seq_q_ = SavedVariable(cum_seq_q, false);
    grad_fn->dropout_p = dropout_p;
    grad_fn->is_causal = is_causal;
    grad_fn->key_ = SavedVariable(key, false);
    grad_fn->max_k = max_k;
    grad_fn->max_q = max_q;
    grad_fn->query_ = SavedVariable(query, false);
    grad_fn->scale = scale;
    grad_fn->value_ = SavedVariable(value, false);
  }
  at::Tensor output;
  at::Tensor softmax_logsumexp;
  at::Tensor philox_seed;
  at::Tensor philox_offset;
  at::Tensor debug_attn_mask;
  #ifndef NDEBUG
  c10::optional<Storage> query__storage_saved =
    query_.has_storage() ? c10::optional<Storage>(query_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> query__impl_saved;
  if (query_.defined()) query__impl_saved = query_.getIntrusivePtr();
  c10::optional<Storage> key__storage_saved =
    key_.has_storage() ? c10::optional<Storage>(key_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> key__impl_saved;
  if (key_.defined()) key__impl_saved = key_.getIntrusivePtr();
  c10::optional<Storage> value__storage_saved =
    value_.has_storage() ? c10::optional<Storage>(value_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> value__impl_saved;
  if (value_.defined()) value__impl_saved = value_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(query) || isFwGradDefined(key) || isFwGradDefined(value) || isFwGradDefined(cum_seq_q) || isFwGradDefined(cum_seq_k))) {
      static c10::OperatorName full_name("aten::_flash_attention_forward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>>("_flash_attention_forward", *opt_op, ks, query, key, value, cum_seq_q, cum_seq_k, max_q, max_k, dropout_p, is_causal, return_debug_mask, scale);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_flash_attention_forward_symint(ks & c10::after_autograd_keyset, query_, key_, value_, cum_seq_q, cum_seq_k, max_q, max_k, dropout_p, is_causal, return_debug_mask, scale);
    }
  })();
  std::tie(output, softmax_logsumexp, philox_seed, philox_offset, debug_attn_mask) = std::move(_tmp);
  #ifndef NDEBUG
  if (query__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(query_))
    TORCH_INTERNAL_ASSERT(query__storage_saved.value().is_alias_of(query_.storage()));
  if (query__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(query_))
    TORCH_INTERNAL_ASSERT(query__impl_saved == query_.getIntrusivePtr());
  if (key__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(key_))
    TORCH_INTERNAL_ASSERT(key__storage_saved.value().is_alias_of(key_.storage()));
  if (key__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(key_))
    TORCH_INTERNAL_ASSERT(key__impl_saved == key_.getIntrusivePtr());
  if (value__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__storage_saved.value().is_alias_of(value_.storage()));
  if (value__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__impl_saved == value_.getIntrusivePtr());
  if (output.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output)) {
    TORCH_INTERNAL_ASSERT(output.storage().use_count() == 1, "function: _flash_attention_forward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output))
    TORCH_INTERNAL_ASSERT(output.use_count() <= 1, "function: _flash_attention_forward");
  if (softmax_logsumexp.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(softmax_logsumexp)) {
    TORCH_INTERNAL_ASSERT(softmax_logsumexp.storage().use_count() == 1, "function: _flash_attention_forward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(softmax_logsumexp))
    TORCH_INTERNAL_ASSERT(softmax_logsumexp.use_count() <= 1, "function: _flash_attention_forward");
  if (philox_seed.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_seed)) {
    TORCH_INTERNAL_ASSERT(philox_seed.storage().use_count() == 1, "function: _flash_attention_forward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_seed))
    TORCH_INTERNAL_ASSERT(philox_seed.use_count() <= 1, "function: _flash_attention_forward");
  if (philox_offset.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_offset)) {
    TORCH_INTERNAL_ASSERT(philox_offset.storage().use_count() == 1, "function: _flash_attention_forward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(philox_offset))
    TORCH_INTERNAL_ASSERT(philox_offset.use_count() <= 1, "function: _flash_attention_forward");
  if (debug_attn_mask.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(debug_attn_mask)) {
    TORCH_INTERNAL_ASSERT(debug_attn_mask.storage().use_count() == 1, "function: _flash_attention_forward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(debug_attn_mask))
    TORCH_INTERNAL_ASSERT(debug_attn_mask.use_count() <= 1, "function: _flash_attention_forward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( output ), grad_fn);
  }
  throw_error_for_complex_autograd(output, "_flash_attention_forward");
  if (grad_fn) {
    grad_fn->output_ = SavedVariable(output, true);
    grad_fn->philox_offset_ = SavedVariable(philox_offset, true);
    grad_fn->philox_seed_ = SavedVariable(philox_seed, true);
    grad_fn->softmax_logsumexp_ = SavedVariable(softmax_logsumexp, true);
  }
  return std::make_tuple(std::move(output), std::move(softmax_logsumexp), std::move(philox_seed), std::move(philox_offset), std::move(debug_attn_mask));
}
void _foreach_acos_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<AcosBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<AcosBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<AcosBackward0>(new AcosBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_acos_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * -((-original_self_p * original_self_p + 1).rsqrt()).conj()).conj()) : (original_self_t.conj() * -((-original_self_p * original_self_p + 1).rsqrt()).conj()).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
::std::vector<at::Tensor> _foreach_add_Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachAddBackward1Scalar> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachAddBackward1Scalar>(new ForeachAddBackward1Scalar(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_add(ks & c10::after_autograd_keyset, self_, scalar);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = self_t.clone();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_add_List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::shared_ptr<ForeachAddBackward0List> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachAddBackward0List>(new ForeachAddBackward0List(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->alpha = alpha;
    grad_fn->other_ = make_saved_variable_list(other, false);
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
    grad_fn->other_size_ = other.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_add(ks & c10::after_autograd_keyset, self_, other_, alpha);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        result_new_fw_grad_opts[i] = self_t + maybe_multiply(other_t, alpha);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_add_ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachAddBackward1ScalarList> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachAddBackward1ScalarList>(new ForeachAddBackward1ScalarList(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_add(ks & c10::after_autograd_keyset, self_, scalars);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = self_t.clone();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_add_Tensor(c10::DispatchKeySet ks, at::TensorList self, const at::Tensor & other, const at::Scalar & alpha) {
  auto self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(other);
  }
  std::shared_ptr<ForeachAddBackward0Tensor> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachAddBackward0Tensor>(new ForeachAddBackward0Tensor(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->alpha = alpha;
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_add(ks & c10::after_autograd_keyset, self_, other_, alpha);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto other_t_raw = toNonOptFwGrad(other);
        auto other_tensor = toNonOptTensor(other);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        result_new_fw_grad_opts[i] = self_t + maybe_multiply(other_t, alpha);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_add_out_Scalar_out(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_add_outf(ks & c10::after_autograd_keyset, self_, scalar, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_add_out that does not support it because it is an out= function");
}
void _foreach_add_out_List_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other, const at::Scalar & alpha, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_add_outf(ks & c10::after_autograd_keyset, self_, other_, alpha, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(other) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_add_out that does not support it because it is an out= function");
}
void _foreach_add_out_ScalarList_out(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_add_outf(ks & c10::after_autograd_keyset, self_, scalars, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_add_out that does not support it because it is an out= function");
}
void _foreach_add_out_Tensor_out(c10::DispatchKeySet ks, at::TensorList self, const at::Tensor & other, const at::Scalar & alpha, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_add_outf(ks & c10::after_autograd_keyset, self_, other_, alpha, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefined(other) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_add_out that does not support it because it is an out= function");
}
void _foreach_asin_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<AsinBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<AsinBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<AsinBackward0>(new AsinBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_asin_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * (-original_self_p * original_self_p + 1).rsqrt().conj()).conj()) : (original_self_t.conj() * (-original_self_p * original_self_p + 1).rsqrt().conj()).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_atan_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_atan_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_atan_out that does not support it because it is an out= function");
}
::std::vector<at::Tensor> _foreach_clamp_min_Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachClampMinBackward0Scalar> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachClampMinBackward0Scalar>(new ForeachClampMinBackward0Scalar(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->scalar = scalar;
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_clamp_min(ks & c10::after_autograd_keyset, self_, scalar);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (where(self_p >= scalar, self_t.conj(), at::scalar_tensor(0., self_t.conj().options()))).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_clamp_min_List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::shared_ptr<ForeachClampMinBackward1List> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachClampMinBackward1List>(new ForeachClampMinBackward1List(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = make_saved_variable_list(other, false);
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
    grad_fn->other_size_ = other.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_clamp_min(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other[i]);
        result_new_fw_grad_opts[i] = where(self_p >= other_p, self_t, other_t);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_clamp_min_ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachClampMinBackward0ScalarList> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachClampMinBackward0ScalarList>(new ForeachClampMinBackward0ScalarList(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->scalars = scalars.vec();
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_clamp_min(ks & c10::after_autograd_keyset, self_, scalars);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (where(self_p >= scalars[i], self_t.conj(), at::scalar_tensor(0., self_t.conj().options()))).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_clamp_min__Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<ClampMinBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<ClampMinBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<ClampMinBackward0>(new ClampMinBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->min = scalar;
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_clamp_min_(ks & c10::after_autograd_keyset, self_, scalar);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((where(original_self_p >= scalar, original_self_t.conj(), at::scalar_tensor(0., original_self_t.conj().options()))).conj()) : (where(original_self_p >= scalar, original_self_t.conj(), at::scalar_tensor(0., original_self_t.conj().options()))).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_clamp_min__List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<ClampMinBackward1>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<ClampMinBackward1> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<ClampMinBackward1>(new ClampMinBackward1(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->min_ = SavedVariable(other[i], false);
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_clamp_min_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto min_t_raw = toNonOptFwGrad(other[i]);
        auto min_tensor = toNonOptTensor(other[i]);
        auto min_t = (min_t_raw.defined() || !min_tensor.defined())
          ? min_t_raw : at::_efficientzerotensor(min_tensor.sizes(), min_tensor.options());
        auto min_p = toNonOptPrimal(other[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(where(original_self_p >= min_p, original_self_t, min_t)) : where(original_self_p >= min_p, original_self_t, min_t);
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_clamp_min__ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<ClampMinBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<ClampMinBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<ClampMinBackward0>(new ClampMinBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->min = scalars[i];
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_clamp_min_(ks & c10::after_autograd_keyset, self_, scalars);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((where(original_self_p >= scalars[i], original_self_t.conj(), at::scalar_tensor(0., original_self_t.conj().options()))).conj()) : (where(original_self_p >= scalars[i], original_self_t.conj(), at::scalar_tensor(0., original_self_t.conj().options()))).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_copy_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList src, bool non_blocking, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto src_ = unpack(src, "src", 1);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> src__storage_saved(src_.size());
  for (const Tensor& tensor : src_)
    src__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> src__impl_saved(src_.size());
  for (size_t i=0; i<src_.size(); i++)
    if (src_[i].defined()) src__impl_saved[i] = src_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_copy_outf(ks & c10::after_autograd_keyset, self_, src_, non_blocking, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<src_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (src__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(src_))
      TORCH_INTERNAL_ASSERT(src__storage_saved[i].value().is_alias_of(src_[i].storage()));
  }
  for (size_t i=0; i<src_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (src__impl_saved[i] && !at::impl::tensorlist_has_dispatch(src_))
      TORCH_INTERNAL_ASSERT(src__impl_saved[i] == src_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(src) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_copy_out that does not support it because it is an out= function");
}
::std::vector<at::Tensor> _foreach_cosh(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachCoshBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachCoshBackward0>(new ForeachCoshBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_cosh(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (self_t.conj() * self_p.sinh().conj()).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_cosh_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<CoshBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<CoshBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<CoshBackward0>(new CoshBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_cosh_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * original_self_p.sinh().conj()).conj()) : (original_self_t.conj() * original_self_p.sinh().conj()).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
::std::vector<at::Tensor> _foreach_erfc(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachErfcBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachErfcBackward0>(new ForeachErfcBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_erfc(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (-2.0 / sqrt(M_PI) * exp(-(self_p.pow(2))) * self_t.conj()).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_expm1_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<Expm1Backward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<Expm1Backward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<Expm1Backward0>(new Expm1Backward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_expm1_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((self_t.conj() * (self_p.conj() + 1)).conj()) : (self_t.conj() * (self_p.conj() + 1)).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
  if (!grad_fns.empty()) {
  
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              grad_fn->result_ = SavedVariable(self[i], true, self[i].is_view());
          }
      }
  }
}
::std::vector<at::Tensor> _foreach_frac(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachFracBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachFracBackward0>(new ForeachFracBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_frac(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = self_t;
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_frac_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_frac_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_frac_out that does not support it because it is an out= function");
}
::std::vector<at::Tensor> _foreach_lerp_List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensors1, at::TensorList weights) {
  auto self_ = unpack(self, "self", 0);
  auto tensors1_ = unpack(tensors1, "tensors1", 1);
  auto weights_ = unpack(weights, "weights", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, tensors1, weights );
  
  TORCH_CHECK(
      self.size() == tensors1.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      tensors1.size());
  TORCH_CHECK(
      self.size() == weights.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      weights.size());
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(tensors1[i]) || isFwGradDefined(weights[i]);
  }
  std::shared_ptr<ForeachLerpBackward1List> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachLerpBackward1List>(new ForeachLerpBackward1List(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, tensors1, weights ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->tensors1_ = make_saved_variable_list(tensors1, false);
    grad_fn->weights_ = make_saved_variable_list(weights, false);
    grad_fn->self_size_ = self.size();
    grad_fn->tensors1_size_ = tensors1.size();
    grad_fn->weights_size_ = weights.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensors1__storage_saved(tensors1_.size());
  for (const Tensor& tensor : tensors1_)
    tensors1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensors1__impl_saved(tensors1_.size());
  for (size_t i=0; i<tensors1_.size(); i++)
    if (tensors1_[i].defined()) tensors1__impl_saved[i] = tensors1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> weights__storage_saved(weights_.size());
  for (const Tensor& tensor : weights_)
    weights__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> weights__impl_saved(weights_.size());
  for (size_t i=0; i<weights_.size(); i++)
    if (weights_[i].defined()) weights__impl_saved[i] = weights_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_lerp(ks & c10::after_autograd_keyset, self_, tensors1_, weights_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__storage_saved[i].value().is_alias_of(tensors1_[i].storage()));
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__impl_saved[i] == tensors1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<weights_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weights__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(weights_))
      TORCH_INTERNAL_ASSERT(weights__storage_saved[i].value().is_alias_of(weights_[i].storage()));
  }
  for (size_t i=0; i<weights_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (weights__impl_saved[i] && !at::impl::tensorlist_has_dispatch(weights_))
      TORCH_INTERNAL_ASSERT(weights__impl_saved[i] == weights_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto tensors1_t_raw = toNonOptFwGrad(tensors1[i]);
        auto tensors1_tensor = toNonOptTensor(tensors1[i]);
        auto tensors1_t = (tensors1_t_raw.defined() || !tensors1_tensor.defined())
          ? tensors1_t_raw : at::_efficientzerotensor(tensors1_tensor.sizes(), tensors1_tensor.options());
        auto tensors1_p = toNonOptPrimal(tensors1[i]);
        auto weights_t_raw = toNonOptFwGrad(weights[i]);
        auto weights_tensor = toNonOptTensor(weights[i]);
        auto weights_t = (weights_t_raw.defined() || !weights_tensor.defined())
          ? weights_t_raw : at::_efficientzerotensor(weights_tensor.sizes(), weights_tensor.options());
        auto weights_p = toNonOptPrimal(weights[i]);
        result_new_fw_grad_opts[i] = at::lerp(self_t, tensors1_t, weights_p) + weights_t * (tensors1_p - self_p);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_lerp_Scalar(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensors1, const at::Scalar & weight) {
  auto self_ = unpack(self, "self", 0);
  auto tensors1_ = unpack(tensors1, "tensors1", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, tensors1 );
  
  TORCH_CHECK(
      self.size() == tensors1.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      tensors1.size());
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(tensors1[i]);
  }
  std::shared_ptr<ForeachLerpBackward0Scalar> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachLerpBackward0Scalar>(new ForeachLerpBackward0Scalar(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, tensors1 ));
    grad_fn->weight = weight;
    grad_fn->self_size_ = self.size();
    grad_fn->tensors1_size_ = tensors1.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensors1__storage_saved(tensors1_.size());
  for (const Tensor& tensor : tensors1_)
    tensors1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensors1__impl_saved(tensors1_.size());
  for (size_t i=0; i<tensors1_.size(); i++)
    if (tensors1_[i].defined()) tensors1__impl_saved[i] = tensors1_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_lerp(ks & c10::after_autograd_keyset, self_, tensors1_, weight);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__storage_saved[i].value().is_alias_of(tensors1_[i].storage()));
  }
  for (size_t i=0; i<tensors1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensors1_))
      TORCH_INTERNAL_ASSERT(tensors1__impl_saved[i] == tensors1_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto tensors1_t_raw = toNonOptFwGrad(tensors1[i]);
        auto tensors1_tensor = toNonOptTensor(tensors1[i]);
        auto tensors1_t = (tensors1_t_raw.defined() || !tensors1_tensor.defined())
          ? tensors1_t_raw : at::_efficientzerotensor(tensors1_tensor.sizes(), tensors1_tensor.options());
        result_new_fw_grad_opts[i] = at::lerp(self_t, tensors1_t, weight);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_lgamma(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachLgammaBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachLgammaBackward0>(new ForeachLgammaBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_lgamma(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = (self_t.conj() * digamma(self_p)).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_log_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<LogBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<LogBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<LogBackward0>(new LogBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_log_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj().div(original_self_p.conj())).conj()) : (original_self_t.conj().div(original_self_p.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_log_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_log_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_log_out that does not support it because it is an out= function");
}
void _foreach_maximum_out_Scalar_out(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_maximum_outf(ks & c10::after_autograd_keyset, self_, scalar, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_maximum_out that does not support it because it is an out= function");
}
void _foreach_maximum_out_List_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_maximum_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(other) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_maximum_out that does not support it because it is an out= function");
}
void _foreach_maximum_out_ScalarList_out(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_maximum_outf(ks & c10::after_autograd_keyset, self_, scalars, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_maximum_out that does not support it because it is an out= function");
}
::std::vector<at::Tensor> _foreach_minimum_Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachMinimumBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachMinimumBackward0>(new ForeachMinimumBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->scalar = scalar;
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_minimum(ks & c10::after_autograd_keyset, self_, scalar);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = scalar + at::where(self_p == scalar, at::scalar_tensor(0.5, result[i].options()), (self_p < scalar).to(result[i].scalar_type())) * (self_t - scalar);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_minimum_List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::shared_ptr<ForeachMinimumBackward0List> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachMinimumBackward0List>(new ForeachMinimumBackward0List(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = make_saved_variable_list(other, false);
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
    grad_fn->other_size_ = other.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_minimum(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other[i]);
        result_new_fw_grad_opts[i] = other_t + at::where(self_p == other_p, at::scalar_tensor(0.5, result[i].options()), (self_p < other_p).to(result[i].scalar_type())) * (self_t - other_t);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
::std::vector<at::Tensor> _foreach_minimum_ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachMinimumBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachMinimumBackward1>(new ForeachMinimumBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->scalars = scalars.vec();
    grad_fn->self_ = make_saved_variable_list(self, false);
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_minimum(ks & c10::after_autograd_keyset, self_, scalars);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        result_new_fw_grad_opts[i] = scalars[i] + at::where(self_p == scalars[i], at::scalar_tensor(0.5, result[i].options()), (self_p < scalars[i]).to(result[i].scalar_type())) * (self_t - scalars[i]);
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_minimum__Scalar(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<NotImplemented>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<NotImplemented> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("_foreach_minimum_"), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_minimum_(ks & c10::after_autograd_keyset, self_, scalar);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self))), "Trying to use forward AD with _foreach_minimum_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
}
void _foreach_minimum__List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  TORCH_CHECK(
      self.size() == other.size(),
        "Tensor lists must have the same number of tensors, got ",
      self.size(),
        " and ",
      other.size());
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]) || isFwGradDefined(other[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  for (const auto& i : c10::irange(self.size())) {
    if (_any_has_forward_grad_self[i]) {
      original_selfs[i] = self[i].clone();
    }
  }
  std::vector<std::shared_ptr<MinimumBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<MinimumBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<MinimumBackward0>(new MinimumBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {
    
        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->other_ = SavedVariable(other[i], false);
                if (!original_selfs[i].has_value()) original_selfs[i] = self[i].clone();
                grad_fn->self_ = SavedVariable(original_selfs[i].value(), false);
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_minimum_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        auto other_t_raw = toNonOptFwGrad(other[i]);
        auto other_tensor = toNonOptTensor(other[i]);
        auto other_t = (other_t_raw.defined() || !other_tensor.defined())
          ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
        auto other_p = toNonOptPrimal(other[i]);
        auto original_self_t_raw = toNonOptFwGrad(original_selfs[i]);
        auto original_self_tensor = toNonOptTensor(original_selfs[i]);
        auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
          ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
        auto original_self_p = toNonOptPrimal(original_selfs[i]);
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_(other_t + at::where(original_self_p == other_p, at::scalar_tensor(0.5, self_p.options()), (original_self_p < other_p).to(self_p.scalar_type())) * (original_self_t - other_t)) : other_t + at::where(original_self_p == other_p, at::scalar_tensor(0.5, self_p.options()), (original_self_p < other_p).to(self_p.scalar_type())) * (original_self_t - other_t);
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_minimum__ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<NotImplemented>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<NotImplemented> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("_foreach_minimum_"), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_minimum_(ks & c10::after_autograd_keyset, self_, scalars);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self))), "Trying to use forward AD with _foreach_minimum_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
}
::std::vector<at::Tensor> _foreach_round(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachRoundBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachRoundBackward0>(new ForeachRoundBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_round(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = (zeros_like(self_t.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_sign_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sign_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_sign_out that does not support it because it is an out= function");
}
void _foreach_sub_out_Scalar_out(c10::DispatchKeySet ks, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sub_outf(ks & c10::after_autograd_keyset, self_, scalar, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_sub_out that does not support it because it is an out= function");
}
void _foreach_sub_out_List_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other, const at::Scalar & alpha, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto other_ = unpack(other, "other", 1);
  auto out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sub_outf(ks & c10::after_autograd_keyset, self_, other_, alpha, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(other) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_sub_out that does not support it because it is an out= function");
}
void _foreach_sub_out_ScalarList_out(c10::DispatchKeySet ks, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_sub_outf(ks & c10::after_autograd_keyset, self_, scalars, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_sub_out that does not support it because it is an out= function");
}
void _foreach_tan_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<TanBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<TanBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<TanBackward0>(new TanBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_tan_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        auto self_p = toNonOptPrimal(self[i]);
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((self_t.conj() * (1 + self_p.pow(2)).conj()).conj()) : (self_t.conj() * (1 + self_p.pow(2)).conj()).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
  if (!grad_fns.empty()) {
  
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              grad_fn->result_ = SavedVariable(self[i], true, self[i].is_view());
          }
      }
  }
}
::std::vector<at::Tensor> _foreach_trunc(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_result(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_result[i] = isFwGradDefined(self[i]);
  }
  std::shared_ptr<ForeachTruncBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ForeachTruncBackward0>(new ForeachTruncBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_size_ = self.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_foreach_trunc(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  std::vector<c10::optional<at::Tensor>> result_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_result[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
        result_new_fw_grad_opts[i] = (zeros_like(self_t.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(result_new_fw_grad_opts.size())) {
    auto& result_new_fw_grad_opt = result_new_fw_grad_opts[i];
    if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      result[i]._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
    }
  }
  return result;
}
void _foreach_trunc_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::vector<bool> _any_has_forward_grad_self(self.size());
  for (const auto& i : c10::irange(self.size())) {
    _any_has_forward_grad_self[i] = isFwGradDefined(self[i]);
  }
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<TruncBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<TruncBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<TruncBackward0>(new TruncBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_trunc_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  std::vector<c10::optional<at::Tensor>> self_new_fw_grad_opts(self.size(), c10::nullopt);
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    if (_any_has_forward_grad_self[i]) {
        auto self_t_raw = toNonOptFwGrad(self[i]);
        auto self_tensor = toNonOptTensor(self[i]);
        auto self_t = (self_t_raw.defined() || !self_tensor.defined())
          ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
        self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
        self_new_fw_grad_opts[i] = self_t_raw.defined() ? self_t_raw.copy_((zeros_like(self_t.conj())).conj()) : (zeros_like(self_t.conj())).conj();
    }
  }
  for (const auto& i : c10::irange(self_new_fw_grad_opts.size())) {
    auto& self_new_fw_grad_opt = self_new_fw_grad_opts[i];
    if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self[i].defined()) {
      // The hardcoded 0 here will need to be updated once we support multiple levels.
      self[i]._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
    }
  }
}
void _foreach_trunc_out_out(c10::DispatchKeySet ks, at::TensorList self, at::TensorList out) {
  auto self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_trunc_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with _foreach_trunc_out that does not support it because it is an out= function");
}
::std::tuple<at::Tensor,at::Tensor> _fused_dropout(c10::DispatchKeySet ks, const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<FusedDropoutBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FusedDropoutBackward0>(new FusedDropoutBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->p = p;
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_fused_dropout", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_fused_dropout", *opt_op, ks, self, p, generator);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_fused_dropout(ks & c10::after_autograd_keyset, self_, p, generator);
    }
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _fused_dropout");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _fused_dropout");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _fused_dropout");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _fused_dropout");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_fused_dropout");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
::std::tuple<at::Tensor,at::Tensor> _fused_moving_avg_obs_fq_helper(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
  auto& self_ = unpack(self, "self", 0);
  auto& observer_on_ = unpack(observer_on, "observer_on", 1);
  auto& fake_quant_on_ = unpack(fake_quant_on, "fake_quant_on", 2);
  auto& running_min_ = unpack(running_min, "running_min", 3);
  auto& running_max_ = unpack(running_max, "running_max", 4);
  auto& scale_ = unpack(scale, "scale", 5);
  auto& zero_point_ = unpack(zero_point, "zero_point", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  check_no_requires_grad(observer_on, "observer_on", "_fused_moving_avg_obs_fq_helper");
  check_no_requires_grad(fake_quant_on, "fake_quant_on", "_fused_moving_avg_obs_fq_helper");
  check_no_requires_grad(running_min, "running_min", "_fused_moving_avg_obs_fq_helper");
  check_no_requires_grad(running_max, "running_max", "_fused_moving_avg_obs_fq_helper");
  check_no_requires_grad(scale, "scale", "_fused_moving_avg_obs_fq_helper");
  check_no_requires_grad(zero_point, "zero_point", "_fused_moving_avg_obs_fq_helper");
  std::shared_ptr<FusedMovingAvgObsFqHelperBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FusedMovingAvgObsFqHelperBackward0>(new FusedMovingAvgObsFqHelperBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  at::Tensor output;
  at::Tensor mask;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> observer_on__storage_saved =
    observer_on_.has_storage() ? c10::optional<Storage>(observer_on_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> observer_on__impl_saved;
  if (observer_on_.defined()) observer_on__impl_saved = observer_on_.getIntrusivePtr();
  c10::optional<Storage> fake_quant_on__storage_saved =
    fake_quant_on_.has_storage() ? c10::optional<Storage>(fake_quant_on_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> fake_quant_on__impl_saved;
  if (fake_quant_on_.defined()) fake_quant_on__impl_saved = fake_quant_on_.getIntrusivePtr();
  c10::optional<Storage> running_min__storage_saved =
    running_min_.has_storage() ? c10::optional<Storage>(running_min_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> running_min__impl_saved;
  if (running_min_.defined()) running_min__impl_saved = running_min_.getIntrusivePtr();
  c10::optional<Storage> running_max__storage_saved =
    running_max_.has_storage() ? c10::optional<Storage>(running_max_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> running_max__impl_saved;
  if (running_max_.defined()) running_max__impl_saved = running_max_.getIntrusivePtr();
  c10::optional<Storage> scale__storage_saved =
    scale_.has_storage() ? c10::optional<Storage>(scale_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> scale__impl_saved;
  if (scale_.defined()) scale__impl_saved = scale_.getIntrusivePtr();
  c10::optional<Storage> zero_point__storage_saved =
    zero_point_.has_storage() ? c10::optional<Storage>(zero_point_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> zero_point__impl_saved;
  if (zero_point_.defined()) zero_point__impl_saved = zero_point_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(observer_on) || isFwGradDefined(fake_quant_on) || isFwGradDefined(running_min) || isFwGradDefined(running_max) || isFwGradDefined(scale) || isFwGradDefined(zero_point))) {
      static c10::OperatorName full_name("aten::_fused_moving_avg_obs_fq_helper", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_fused_moving_avg_obs_fq_helper", *opt_op, ks, self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_fused_moving_avg_obs_fq_helper(ks & c10::after_autograd_keyset, self_, observer_on_, fake_quant_on_, running_min_, running_max_, scale_, zero_point_, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
    }
  })();
  std::tie(output, mask) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (observer_on__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(observer_on_))
    TORCH_INTERNAL_ASSERT(observer_on__storage_saved.value().is_alias_of(observer_on_.storage()));
  if (observer_on__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(observer_on_))
    TORCH_INTERNAL_ASSERT(observer_on__impl_saved == observer_on_.getIntrusivePtr());
  if (fake_quant_on__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(fake_quant_on_))
    TORCH_INTERNAL_ASSERT(fake_quant_on__storage_saved.value().is_alias_of(fake_quant_on_.storage()));
  if (fake_quant_on__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(fake_quant_on_))
    TORCH_INTERNAL_ASSERT(fake_quant_on__impl_saved == fake_quant_on_.getIntrusivePtr());
  if (running_min__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(running_min_))
    TORCH_INTERNAL_ASSERT(running_min__storage_saved.value().is_alias_of(running_min_.storage()));
  if (running_min__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(running_min_))
    TORCH_INTERNAL_ASSERT(running_min__impl_saved == running_min_.getIntrusivePtr());
  if (running_max__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(running_max_))
    TORCH_INTERNAL_ASSERT(running_max__storage_saved.value().is_alias_of(running_max_.storage()));
  if (running_max__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(running_max_))
    TORCH_INTERNAL_ASSERT(running_max__impl_saved == running_max_.getIntrusivePtr());
  if (scale__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(scale_))
    TORCH_INTERNAL_ASSERT(scale__storage_saved.value().is_alias_of(scale_.storage()));
  if (scale__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(scale_))
    TORCH_INTERNAL_ASSERT(scale__impl_saved == scale_.getIntrusivePtr());
  if (zero_point__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(zero_point_))
    TORCH_INTERNAL_ASSERT(zero_point__storage_saved.value().is_alias_of(zero_point_.storage()));
  if (zero_point__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(zero_point_))
    TORCH_INTERNAL_ASSERT(zero_point__impl_saved == zero_point_.getIntrusivePtr());
  if (output.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output)) {
    TORCH_INTERNAL_ASSERT(output.storage().use_count() == 1, "function: _fused_moving_avg_obs_fq_helper");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output))
    TORCH_INTERNAL_ASSERT(output.use_count() <= 1, "function: _fused_moving_avg_obs_fq_helper");
  if (mask.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask)) {
    TORCH_INTERNAL_ASSERT(mask.storage().use_count() == 1, "function: _fused_moving_avg_obs_fq_helper");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask))
    TORCH_INTERNAL_ASSERT(mask.use_count() <= 1, "function: _fused_moving_avg_obs_fq_helper");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( output ), grad_fn);
  }
  throw_error_for_complex_autograd(output, "_fused_moving_avg_obs_fq_helper");
  if (grad_fn) {
    grad_fn->mask_ = SavedVariable(mask, true);
  }
  return std::make_tuple(std::move(output), std::move(mask));
}
bool _has_same_storage_numel(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_has_same_storage_numel(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor _index_put_impl(c10::DispatchKeySet ks, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, bool unsafe) {
  auto& self_ = unpack(self, "self", 0);
  auto& values_ = unpack(values, "values", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, values );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(values));
  check_no_requires_grad(indices, "indices", "_index_put_impl");
  std::shared_ptr<IndexPutImplBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<IndexPutImplBackward0>(new IndexPutImplBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, values ));
    grad_fn->accumulate = accumulate;
    grad_fn->indices_ = make_saved_variable_list(indices, false);
    grad_fn->values_info = values;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> indices_storage_saved(indices.size());
  for (const c10::optional<Tensor>& tensor : indices)
    indices_storage_saved.push_back(
      tensor.has_value() && tensor->has_storage() ? c10::optional<Storage>(tensor->storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> indices_impl_saved(indices.size());
  for (size_t i=0; i<indices.size(); i++) {
    c10::optional<Tensor> t = indices[i];
    if (t.has_value() && t->defined()) indices_impl_saved[i] = t->getIntrusivePtr();
  }
  c10::optional<Storage> values__storage_saved =
    values_.has_storage() ? c10::optional<Storage>(values_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> values__impl_saved;
  if (values_.defined()) values__impl_saved = values_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_index_put_impl(ks & c10::after_autograd_keyset, self_, indices, values_, accumulate, unsafe);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<indices.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (indices_storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(indices))
      TORCH_INTERNAL_ASSERT(indices_storage_saved[i].value().is_alias_of(
          static_cast<c10::optional<Tensor>>(indices[i])->storage()));
  }
  for (size_t i=0; i<indices.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (indices_impl_saved[i])
      TORCH_INTERNAL_ASSERT(
        indices_impl_saved[i] == static_cast<c10::optional<Tensor>>(indices[i])->getIntrusivePtr());
  }
  if (values__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__storage_saved.value().is_alias_of(values_.storage()));
  if (values__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__impl_saved == values_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _index_put_impl");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _index_put_impl");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_index_put_impl");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto values_t_raw = toNonOptFwGrad(values);
      auto values_tensor = toNonOptTensor(values);
      auto values_t = (values_t_raw.defined() || !values_tensor.defined())
        ? values_t_raw : at::_efficientzerotensor(values_tensor.sizes(), values_tensor.options());
      result_new_fw_grad_opt = at::_index_put_impl_(self_t, indices, values_t, accumulate, unsafe);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _is_all_true(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_is_all_true(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _is_all_true");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _is_all_true");
  #endif
  return result;
}
::std::tuple<at::Tensor,at::Tensor> _linalg_eigh(c10::DispatchKeySet ks, const at::Tensor & A, c10::string_view UPLO, bool compute_v) {
  auto& A_ = unpack(A, "A", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_eigenvalues_eigenvectors = (isFwGradDefined(A));
  std::shared_ptr<LinalgEighBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgEighBackward0>(new LinalgEighBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( A ));
  }
  at::Tensor eigenvalues;
  at::Tensor eigenvectors;
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_linalg_eigh(ks & c10::after_autograd_keyset, A_, UPLO, compute_v);
  })();
  std::tie(eigenvalues, eigenvectors) = std::move(_tmp);
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (eigenvalues.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(eigenvalues)) {
    TORCH_INTERNAL_ASSERT(eigenvalues.storage().use_count() == 1, "function: _linalg_eigh");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(eigenvalues))
    TORCH_INTERNAL_ASSERT(eigenvalues.use_count() <= 1, "function: _linalg_eigh");
  if (eigenvectors.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(eigenvectors)) {
    TORCH_INTERNAL_ASSERT(eigenvectors.storage().use_count() == 1, "function: _linalg_eigh");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(eigenvectors))
    TORCH_INTERNAL_ASSERT(eigenvectors.use_count() <= 1, "function: _linalg_eigh");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( eigenvalues, eigenvectors ), grad_fn);
  }
  c10::optional<::std::tuple<at::Tensor,at::Tensor>> eigenvalues_eigenvectors_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_eigenvalues_eigenvectors) {
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      eigenvalues_eigenvectors_new_fw_grad_opt = linalg_eig_jvp(A_t, eigenvalues, eigenvectors, /*is_hermitian=*/true);
  }
  if (eigenvalues_eigenvectors_new_fw_grad_opt.has_value() && std::get<0>(eigenvalues_eigenvectors_new_fw_grad_opt.value()).defined()
      && eigenvalues.defined()) {
    eigenvalues._set_fw_grad(std::get<0>(eigenvalues_eigenvectors_new_fw_grad_opt.value()), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (eigenvalues_eigenvectors_new_fw_grad_opt.has_value() && std::get<1>(eigenvalues_eigenvectors_new_fw_grad_opt.value()).defined()
      && eigenvectors.defined()) {
    eigenvectors._set_fw_grad(std::get<1>(eigenvalues_eigenvectors_new_fw_grad_opt.value()), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->eigenvalues_ = SavedVariable(eigenvalues, true);
    grad_fn->eigenvectors_ = SavedVariable(eigenvectors, true);
  }
  return std::make_tuple(std::move(eigenvalues), std::move(eigenvectors));
}
at::Tensor _log_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<LogSoftmaxBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LogSoftmaxBackward0>(new LogSoftmaxBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_log_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _log_softmax");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _log_softmax");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_log_softmax");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = self_t - logsumexp_jvp(self_p, self_t, {dim}, true);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & _log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("_log_softmax");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("_log_softmax");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_log_softmax_outf(ks & c10::after_autograd_keyset, self_, dim, half_to_float, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with _log_softmax_out that does not support it because it is an out= function");
  return out;
}
at::Tensor _neg_view(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<NegViewBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NegViewBackward0>(new NegViewBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::_neg_view(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _neg_view");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t._neg_view();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _neg_view_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<NegViewBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NegViewBackward0_copy>(new NegViewBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_neg_view_copy(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _neg_view_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _neg_view_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_neg_view_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t._neg_view();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _pin_memory(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::Device> device) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<PinMemoryBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PinMemoryBackward0>(new PinMemoryBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_pin_memory", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_pin_memory", *opt_op, ks, self, device);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_pin_memory(ks & c10::after_autograd_keyset, self_, device);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _pin_memory");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _pin_memory");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_pin_memory");
  return result;
}
::std::tuple<at::Tensor,at::Tensor> _prelu_kernel_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& weight_ = unpack(weight, "weight", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self, weight );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(grad_output) || isFwGradDefined(weight));
  [[maybe_unused]] auto _any_has_forward_grad_result1 = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<PreluKernelBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PreluKernelBackwardBackward0>(new PreluKernelBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self, weight ));
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->grad_output_options = grad_output.options();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->self_info = self;
    grad_fn->self_options = self.options();
    grad_fn->weight_ = SavedVariable(weight, false);
    grad_fn->weight_options = weight.options();
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_prelu_kernel_backward(ks & c10::after_autograd_keyset, grad_output_, self_, weight_);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _prelu_kernel_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _prelu_kernel_backward");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _prelu_kernel_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _prelu_kernel_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_prelu_kernel_backward");
  throw_error_for_complex_autograd(result1, "_prelu_kernel_backward");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto self_p = toNonOptPrimal(self);
      auto weight_t_raw = toNonOptFwGrad(weight);
      auto weight_tensor = toNonOptTensor(weight);
      auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
        ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
      auto weight_p = toNonOptPrimal(weight);
      result0_new_fw_grad_opt = at::where(self_p >= 0, grad_output_t, grad_output_t * weight_p + grad_output_p * weight_t);
  }
  c10::optional<at::Tensor> result1_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result1 && (result1.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result1_new_fw_grad_opt = at::where(self_p >= 0, at::zeros({}, self_p.options()), grad_output_p * self_t + grad_output_t * self_p);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (result1_new_fw_grad_opt.has_value() && result1_new_fw_grad_opt.value().defined() && result1.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result1._set_fw_grad(result1_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor _slow_conv2d_forward(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<SlowConv2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SlowConv2DBackward0>(new SlowConv2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight, bias ));
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::_slow_conv2d_forward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_slow_conv2d_forward", *opt_op, ks, self, weight, kernel_size, bias, stride, padding);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_slow_conv2d_forward_symint(ks & c10::after_autograd_keyset, self_, weight_, kernel_size, bias, stride, padding);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _slow_conv2d_forward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_slow_conv2d_forward");
  return result;
}
at::Tensor & _softmax_backward_data_out_out(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& output_ = unpack(output, "output", 1);
  auto& grad_input_ = unpack(grad_input, "grad_input", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, output );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, output )) {
    throw_error_out_requires_grad("_softmax_backward_data");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("_softmax_backward_data");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_softmax_backward_data_outf(ks & c10::after_autograd_keyset, grad_output_, output_, dim, input_dtype, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(output) || isFwGradDefined(grad_input))), "Trying to use forward AD with _softmax_backward_data_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor _sparse_addmm(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  auto& mat1_ = unpack(mat1, "mat1", 1);
  auto& mat2_ = unpack(mat2, "mat2", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, mat1, mat2 );
  
  std::shared_ptr<SparseAddmmBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SparseAddmmBackward0>(new SparseAddmmBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, mat1, mat2 ));
    grad_fn->alpha = alpha;
    grad_fn->beta = beta;
    grad_fn->mat1_ = SavedVariable(mat1, false);
    if (grad_fn->should_compute_output(1)) {
      grad_fn->mat2_ = SavedVariable(mat2, false);
    }
    grad_fn->mat2_layout = mat2.layout();
    grad_fn->mat2_sym_sizes = mat2.sym_sizes().vec();
    grad_fn->mat2_sym_strides = strides_or_error(mat2, "mat2").vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mat1__storage_saved =
    mat1_.has_storage() ? c10::optional<Storage>(mat1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mat1__impl_saved;
  if (mat1_.defined()) mat1__impl_saved = mat1_.getIntrusivePtr();
  c10::optional<Storage> mat2__storage_saved =
    mat2_.has_storage() ? c10::optional<Storage>(mat2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mat2__impl_saved;
  if (mat2_.defined()) mat2__impl_saved = mat2_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(mat1) || isFwGradDefined(mat2))) {
      static c10::OperatorName full_name("aten::_sparse_addmm", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_sparse_addmm", *opt_op, ks, self, mat1, mat2, beta, alpha);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_sparse_addmm(ks & c10::after_autograd_keyset, self_, mat1_, mat2_, beta, alpha);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mat1__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mat1_))
    TORCH_INTERNAL_ASSERT(mat1__storage_saved.value().is_alias_of(mat1_.storage()));
  if (mat1__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mat1_))
    TORCH_INTERNAL_ASSERT(mat1__impl_saved == mat1_.getIntrusivePtr());
  if (mat2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mat2_))
    TORCH_INTERNAL_ASSERT(mat2__storage_saved.value().is_alias_of(mat2_.storage()));
  if (mat2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mat2_))
    TORCH_INTERNAL_ASSERT(mat2__impl_saved == mat2_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _sparse_addmm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _sparse_addmm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  return result;
}
at::Tensor _sparse_sum_dim(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<SparseSumBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SparseSumBackward0>(new SparseSumBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim.vec();
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_sparse_sum", "dim");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_sparse_sum", *opt_op, ks, self, dim);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_sparse_sum(ks & c10::after_autograd_keyset, self_, dim);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _sparse_sum_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _sparse_sum_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_sparse_sum");
  return result;
}
at::Tensor _standard_gamma(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<StandardGammaBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<StandardGammaBackward0>(new StandardGammaBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_standard_gamma", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_standard_gamma", *opt_op, ks, self, generator);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_standard_gamma(ks & c10::after_autograd_keyset, self_, generator);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _standard_gamma");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _standard_gamma");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_standard_gamma");
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor _test_autograd_multiple_dispatch_fullcoverage(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<TestAutogradMultipleDispatchBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TestAutogradMultipleDispatchBackward0>(new TestAutogradMultipleDispatchBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_test_autograd_multiple_dispatch(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _test_autograd_multiple_dispatch_fullcoverage");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _test_autograd_multiple_dispatch_fullcoverage");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_test_autograd_multiple_dispatch");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_test_autograd_multiple_dispatch(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _to_copy(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  _any_requires_grad &= (!dtype || isDifferentiableType(*dtype));
  [[maybe_unused]] auto _any_has_forward_grad_result = (!dtype || isDifferentiableType(*dtype)) && (isFwGradDefined(self));
  std::shared_ptr<ToCopyBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ToCopyBackward0>(new ToCopyBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_options = self.options();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_to_copy(ks & c10::after_autograd_keyset, self_, dtype, layout, device, pin_memory, non_blocking, memory_format);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _to_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _to_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = _to_copy(self_t, dtype, layout, device, pin_memory, non_blocking, memory_format);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor> _unique(c10::DispatchKeySet ks, const at::Tensor & self, bool sorted, bool return_inverse) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<UniqueBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UniqueBackward0>(new UniqueBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::_unique", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_unique", *opt_op, ks, self, sorted, return_inverse);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_unique(ks & c10::after_autograd_keyset, self_, sorted, return_inverse);
    }
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _unique");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _unique");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _unique");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _unique");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_unique");
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor _unsafe_index_put(c10::DispatchKeySet ks, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
  auto& self_ = unpack(self, "self", 0);
  auto& values_ = unpack(values, "values", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, values );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(values));
  check_no_requires_grad(indices, "indices", "_unsafe_index_put");
  std::shared_ptr<UnsafeIndexPutBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnsafeIndexPutBackward0>(new UnsafeIndexPutBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, values ));
    grad_fn->accumulate = accumulate;
    grad_fn->indices_ = make_saved_variable_list(indices, false);
    grad_fn->values_info = values;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> indices_storage_saved(indices.size());
  for (const c10::optional<Tensor>& tensor : indices)
    indices_storage_saved.push_back(
      tensor.has_value() && tensor->has_storage() ? c10::optional<Storage>(tensor->storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> indices_impl_saved(indices.size());
  for (size_t i=0; i<indices.size(); i++) {
    c10::optional<Tensor> t = indices[i];
    if (t.has_value() && t->defined()) indices_impl_saved[i] = t->getIntrusivePtr();
  }
  c10::optional<Storage> values__storage_saved =
    values_.has_storage() ? c10::optional<Storage>(values_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> values__impl_saved;
  if (values_.defined()) values__impl_saved = values_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_unsafe_index_put(ks & c10::after_autograd_keyset, self_, indices, values_, accumulate);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<indices.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (indices_storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(indices))
      TORCH_INTERNAL_ASSERT(indices_storage_saved[i].value().is_alias_of(
          static_cast<c10::optional<Tensor>>(indices[i])->storage()));
  }
  for (size_t i=0; i<indices.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (indices_impl_saved[i])
      TORCH_INTERNAL_ASSERT(
        indices_impl_saved[i] == static_cast<c10::optional<Tensor>>(indices[i])->getIntrusivePtr());
  }
  if (values__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__storage_saved.value().is_alias_of(values_.storage()));
  if (values__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__impl_saved == values_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _unsafe_index_put");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _unsafe_index_put");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_unsafe_index_put");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto values_t_raw = toNonOptFwGrad(values);
      auto values_tensor = toNonOptTensor(values);
      auto values_t = (values_t_raw.defined() || !values_tensor.defined())
        ? values_t_raw : at::_efficientzerotensor(values_tensor.sizes(), values_tensor.options());
      result_new_fw_grad_opt = at::_unsafe_index_put(self_t, indices, values_t, accumulate);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor _upsample_bicubic2d_aa(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UpsampleBicubic2DAaBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleBicubic2DAaBackward0>(new UpsampleBicubic2DAaBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->align_corners = align_corners;
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_upsample_bicubic2d_aa_symint(ks & c10::after_autograd_keyset, self_, output_size, align_corners, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _upsample_bicubic2d_aa");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _upsample_bicubic2d_aa");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_upsample_bicubic2d_aa");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::_upsample_bicubic2d_aa_symint(self_t, output_size, align_corners, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & _upsample_bicubic2d_aa_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& grad_input_ = unpack(grad_input, "grad_input", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output )) {
    throw_error_out_requires_grad("_upsample_bicubic2d_aa_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("_upsample_bicubic2d_aa_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_upsample_bicubic2d_aa_backward_symint_outf(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, align_corners, scales_h, scales_w, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(grad_input))), "Trying to use forward AD with _upsample_bicubic2d_aa_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor _upsample_bilinear2d_aa_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<UpsampleBilinear2DAaBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleBilinear2DAaBackwardBackward0>(new UpsampleBilinear2DAaBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output ));
    grad_fn->align_corners = align_corners;
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_upsample_bilinear2d_aa_backward_symint(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, align_corners, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: _upsample_bilinear2d_aa_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: _upsample_bilinear2d_aa_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_upsample_bilinear2d_aa_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result_new_fw_grad_opt = at::_upsample_bilinear2d_aa_backward_symint(grad_output_t, output_size, input_size, align_corners, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & _upsample_nearest_exact1d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("_upsample_nearest_exact1d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("_upsample_nearest_exact1d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_upsample_nearest_exact1d_symint_outf(ks & c10::after_autograd_keyset, self_, output_size, scales, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with _upsample_nearest_exact1d_out that does not support it because it is an out= function");
  return out;
}
bool _use_cudnn_ctc_loss(c10::DispatchKeySet ks, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank) {
  auto& log_probs_ = unpack(log_probs, "log_probs", 0);
  auto& targets_ = unpack(targets, "targets", 1);
  #ifndef NDEBUG
  c10::optional<Storage> log_probs__storage_saved =
    log_probs_.has_storage() ? c10::optional<Storage>(log_probs_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> log_probs__impl_saved;
  if (log_probs_.defined()) log_probs__impl_saved = log_probs_.getIntrusivePtr();
  c10::optional<Storage> targets__storage_saved =
    targets_.has_storage() ? c10::optional<Storage>(targets_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> targets__impl_saved;
  if (targets_.defined()) targets__impl_saved = targets_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_use_cudnn_ctc_loss(ks & c10::after_autograd_keyset, log_probs_, targets_, input_lengths, target_lengths, blank);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (log_probs__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__storage_saved.value().is_alias_of(log_probs_.storage()));
  if (log_probs__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__impl_saved == log_probs_.getIntrusivePtr());
  if (targets__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__storage_saved.value().is_alias_of(targets_.storage()));
  if (targets__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__impl_saved == targets_.getIntrusivePtr());
  #endif
  return result;
}
bool _use_cudnn_ctc_loss_Tensor(c10::DispatchKeySet ks, const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank) {
  auto& log_probs_ = unpack(log_probs, "log_probs", 0);
  auto& targets_ = unpack(targets, "targets", 1);
  auto& input_lengths_ = unpack(input_lengths, "input_lengths", 2);
  auto& target_lengths_ = unpack(target_lengths, "target_lengths", 3);
  #ifndef NDEBUG
  c10::optional<Storage> log_probs__storage_saved =
    log_probs_.has_storage() ? c10::optional<Storage>(log_probs_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> log_probs__impl_saved;
  if (log_probs_.defined()) log_probs__impl_saved = log_probs_.getIntrusivePtr();
  c10::optional<Storage> targets__storage_saved =
    targets_.has_storage() ? c10::optional<Storage>(targets_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> targets__impl_saved;
  if (targets_.defined()) targets__impl_saved = targets_.getIntrusivePtr();
  c10::optional<Storage> input_lengths__storage_saved =
    input_lengths_.has_storage() ? c10::optional<Storage>(input_lengths_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input_lengths__impl_saved;
  if (input_lengths_.defined()) input_lengths__impl_saved = input_lengths_.getIntrusivePtr();
  c10::optional<Storage> target_lengths__storage_saved =
    target_lengths_.has_storage() ? c10::optional<Storage>(target_lengths_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target_lengths__impl_saved;
  if (target_lengths_.defined()) target_lengths__impl_saved = target_lengths_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::_use_cudnn_ctc_loss(ks & c10::after_autograd_keyset, log_probs_, targets_, input_lengths_, target_lengths_, blank);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (log_probs__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__storage_saved.value().is_alias_of(log_probs_.storage()));
  if (log_probs__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(log_probs_))
    TORCH_INTERNAL_ASSERT(log_probs__impl_saved == log_probs_.getIntrusivePtr());
  if (targets__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__storage_saved.value().is_alias_of(targets_.storage()));
  if (targets__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(targets_))
    TORCH_INTERNAL_ASSERT(targets__impl_saved == targets_.getIntrusivePtr());
  if (input_lengths__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_lengths_))
    TORCH_INTERNAL_ASSERT(input_lengths__storage_saved.value().is_alias_of(input_lengths_.storage()));
  if (input_lengths__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_lengths_))
    TORCH_INTERNAL_ASSERT(input_lengths__impl_saved == input_lengths_.getIntrusivePtr());
  if (target_lengths__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_lengths_))
    TORCH_INTERNAL_ASSERT(target_lengths__storage_saved.value().is_alias_of(target_lengths_.storage()));
  if (target_lengths__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_lengths_))
    TORCH_INTERNAL_ASSERT(target_lengths__impl_saved == target_lengths_.getIntrusivePtr());
  #endif
  return result;
}
::std::tuple<at::Tensor,at::Tensor> _weight_norm_interface(c10::DispatchKeySet ks, const at::Tensor & v, const at::Tensor & g, int64_t dim) {
  auto& v_ = unpack(v, "v", 0);
  auto& g_ = unpack(g, "g", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( v, g );
  
  std::shared_ptr<WeightNormInterfaceBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<WeightNormInterfaceBackward0>(new WeightNormInterfaceBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( v, g ));
    grad_fn->dim = dim;
    grad_fn->g_ = SavedVariable(g, false);
    grad_fn->v_ = SavedVariable(v, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> v__storage_saved =
    v_.has_storage() ? c10::optional<Storage>(v_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> v__impl_saved;
  if (v_.defined()) v__impl_saved = v_.getIntrusivePtr();
  c10::optional<Storage> g__storage_saved =
    g_.has_storage() ? c10::optional<Storage>(g_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> g__impl_saved;
  if (g_.defined()) g__impl_saved = g_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(v) || isFwGradDefined(g))) {
      static c10::OperatorName full_name("aten::_weight_norm_interface", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("_weight_norm_interface", *opt_op, ks, v, g, dim);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::_weight_norm_interface(ks & c10::after_autograd_keyset, v_, g_, dim);
    }
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (v__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(v_))
    TORCH_INTERNAL_ASSERT(v__storage_saved.value().is_alias_of(v_.storage()));
  if (v__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(v_))
    TORCH_INTERNAL_ASSERT(v__impl_saved == v_.getIntrusivePtr());
  if (g__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(g_))
    TORCH_INTERNAL_ASSERT(g__storage_saved.value().is_alias_of(g_.storage()));
  if (g__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(g_))
    TORCH_INTERNAL_ASSERT(g__impl_saved == g_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: _weight_norm_interface");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: _weight_norm_interface");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: _weight_norm_interface");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: _weight_norm_interface");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_weight_norm_interface");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor & abs_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("abs");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("abs");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::abs_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with abs_out that does not support it because it is an out= function");
  return out;
}
at::Tensor acosh(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<AcoshBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AcoshBackward0>(new AcoshBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::acosh(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: acosh");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: acosh");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_p.is_complex() ? self_t.conj() * ((self_p + 1).rsqrt() * (self_p - 1).rsqrt()).conj() : self_t.conj() * (self_p * self_p - 1).rsqrt()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor addcmul(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  auto& self_ = unpack(self, "self", 0);
  auto& tensor1_ = unpack(tensor1, "tensor1", 1);
  auto& tensor2_ = unpack(tensor2, "tensor2", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, tensor1, tensor2 );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(tensor1) || isFwGradDefined(tensor2));
  std::shared_ptr<AddcmulBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AddcmulBackward0>(new AddcmulBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, tensor1, tensor2 ));
    grad_fn->self_scalar_type = self.scalar_type();
    if (grad_fn->should_compute_output(2)) {
      grad_fn->tensor1_ = SavedVariable(tensor1, false);
    }
    grad_fn->tensor1_scalar_type = tensor1.scalar_type();
    if (grad_fn->should_compute_output(1)) {
      grad_fn->tensor2_ = SavedVariable(tensor2, false);
    }
    grad_fn->tensor2_scalar_type = tensor2.scalar_type();
    grad_fn->value = value;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> tensor1__storage_saved =
    tensor1_.has_storage() ? c10::optional<Storage>(tensor1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> tensor1__impl_saved;
  if (tensor1_.defined()) tensor1__impl_saved = tensor1_.getIntrusivePtr();
  c10::optional<Storage> tensor2__storage_saved =
    tensor2_.has_storage() ? c10::optional<Storage>(tensor2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> tensor2__impl_saved;
  if (tensor2_.defined()) tensor2__impl_saved = tensor2_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::addcmul(ks & c10::after_autograd_keyset, self_, tensor1_, tensor2_, value);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (tensor1__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(tensor1_))
    TORCH_INTERNAL_ASSERT(tensor1__storage_saved.value().is_alias_of(tensor1_.storage()));
  if (tensor1__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(tensor1_))
    TORCH_INTERNAL_ASSERT(tensor1__impl_saved == tensor1_.getIntrusivePtr());
  if (tensor2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(tensor2_))
    TORCH_INTERNAL_ASSERT(tensor2__storage_saved.value().is_alias_of(tensor2_.storage()));
  if (tensor2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(tensor2_))
    TORCH_INTERNAL_ASSERT(tensor2__impl_saved == tensor2_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: addcmul");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: addcmul");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto tensor1_t_raw = toNonOptFwGrad(tensor1);
      auto tensor1_tensor = toNonOptTensor(tensor1);
      auto tensor1_t = (tensor1_t_raw.defined() || !tensor1_tensor.defined())
        ? tensor1_t_raw : at::_efficientzerotensor(tensor1_tensor.sizes(), tensor1_tensor.options());
      auto tensor1_p = toNonOptPrimal(tensor1);
      auto tensor2_t_raw = toNonOptFwGrad(tensor2);
      auto tensor2_tensor = toNonOptTensor(tensor2);
      auto tensor2_t = (tensor2_t_raw.defined() || !tensor2_tensor.defined())
        ? tensor2_t_raw : at::_efficientzerotensor(tensor2_tensor.sizes(), tensor2_tensor.options());
      auto tensor2_p = toNonOptPrimal(tensor2);
      result_new_fw_grad_opt = self_t + maybe_multiply(tensor1_t * tensor2_p, value) + maybe_multiply(tensor2_t * tensor1_p, value);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & addcmul_(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  auto& self_ = unpack(self, "self", 0);
  auto& tensor1_ = unpack(tensor1, "tensor1", 1);
  auto& tensor2_ = unpack(tensor2, "tensor2", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, tensor1, tensor2 );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(tensor1) || isFwGradDefined(tensor2));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<AddcmulBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AddcmulBackward0>(new AddcmulBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, tensor1, tensor2 ));
    grad_fn->self_scalar_type = self.scalar_type();
    if (grad_fn->should_compute_output(2)) {
      grad_fn->tensor1_ = SavedVariable(tensor1, false);
    }
    grad_fn->tensor1_scalar_type = tensor1.scalar_type();
    if (grad_fn->should_compute_output(1)) {
      grad_fn->tensor2_ = SavedVariable(tensor2, false);
    }
    grad_fn->tensor2_scalar_type = tensor2.scalar_type();
    grad_fn->value = value;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> tensor1__storage_saved =
    tensor1_.has_storage() ? c10::optional<Storage>(tensor1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> tensor1__impl_saved;
  if (tensor1_.defined()) tensor1__impl_saved = tensor1_.getIntrusivePtr();
  c10::optional<Storage> tensor2__storage_saved =
    tensor2_.has_storage() ? c10::optional<Storage>(tensor2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> tensor2__impl_saved;
  if (tensor2_.defined()) tensor2__impl_saved = tensor2_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::addcmul_(ks & c10::after_autograd_keyset, self_, tensor1_, tensor2_, value);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (tensor1__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(tensor1_))
    TORCH_INTERNAL_ASSERT(tensor1__storage_saved.value().is_alias_of(tensor1_.storage()));
  if (tensor1__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(tensor1_))
    TORCH_INTERNAL_ASSERT(tensor1__impl_saved == tensor1_.getIntrusivePtr());
  if (tensor2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(tensor2_))
    TORCH_INTERNAL_ASSERT(tensor2__storage_saved.value().is_alias_of(tensor2_.storage()));
  if (tensor2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(tensor2_))
    TORCH_INTERNAL_ASSERT(tensor2__impl_saved == tensor2_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto tensor1_t_raw = toNonOptFwGrad(tensor1);
      auto tensor1_tensor = toNonOptTensor(tensor1);
      auto tensor1_t = (tensor1_t_raw.defined() || !tensor1_tensor.defined())
        ? tensor1_t_raw : at::_efficientzerotensor(tensor1_tensor.sizes(), tensor1_tensor.options());
      auto tensor1_p = toNonOptPrimal(tensor1);
      auto tensor2_t_raw = toNonOptFwGrad(tensor2);
      auto tensor2_tensor = toNonOptTensor(tensor2);
      auto tensor2_t = (tensor2_t_raw.defined() || !tensor2_tensor.defined())
        ? tensor2_t_raw : at::_efficientzerotensor(tensor2_tensor.sizes(), tensor2_tensor.options());
      auto tensor2_p = toNonOptPrimal(tensor2);
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(self_t + maybe_multiply(tensor1_t * tensor2_p, value) + maybe_multiply(tensor2_t * tensor1_p, value)) : self_t + maybe_multiply(tensor1_t * tensor2_p, value) + maybe_multiply(tensor2_t * tensor1_p, value);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor addmm(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  auto& mat1_ = unpack(mat1, "mat1", 1);
  auto& mat2_ = unpack(mat2, "mat2", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, mat1, mat2 );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(mat1) || isFwGradDefined(mat2));
  std::shared_ptr<AddmmBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AddmmBackward0>(new AddmmBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, mat1, mat2 ));
    grad_fn->alpha = alpha;
    grad_fn->beta = beta;
    if (grad_fn->should_compute_output(2)) {
      grad_fn->mat1_ = SavedVariable(mat1, false);
    }
    grad_fn->mat1_layout = mat1.layout();
    grad_fn->mat1_sym_sizes = mat1.sym_sizes().vec();
    grad_fn->mat1_sym_strides = strides_or_error(mat1, "mat1").vec();
    if (grad_fn->should_compute_output(1)) {
      grad_fn->mat2_ = SavedVariable(mat2, false);
    }
    grad_fn->mat2_layout = mat2.layout();
    grad_fn->mat2_sym_sizes = mat2.sym_sizes().vec();
    grad_fn->mat2_sym_strides = strides_or_error(mat2, "mat2").vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mat1__storage_saved =
    mat1_.has_storage() ? c10::optional<Storage>(mat1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mat1__impl_saved;
  if (mat1_.defined()) mat1__impl_saved = mat1_.getIntrusivePtr();
  c10::optional<Storage> mat2__storage_saved =
    mat2_.has_storage() ? c10::optional<Storage>(mat2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mat2__impl_saved;
  if (mat2_.defined()) mat2__impl_saved = mat2_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::addmm(ks & c10::after_autograd_keyset, self_, mat1_, mat2_, beta, alpha);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mat1__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mat1_))
    TORCH_INTERNAL_ASSERT(mat1__storage_saved.value().is_alias_of(mat1_.storage()));
  if (mat1__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mat1_))
    TORCH_INTERNAL_ASSERT(mat1__impl_saved == mat1_.getIntrusivePtr());
  if (mat2__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mat2_))
    TORCH_INTERNAL_ASSERT(mat2__storage_saved.value().is_alias_of(mat2_.storage()));
  if (mat2__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mat2_))
    TORCH_INTERNAL_ASSERT(mat2__impl_saved == mat2_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: addmm");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: addmm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto mat1_t_raw = toNonOptFwGrad(mat1);
      auto mat1_tensor = toNonOptTensor(mat1);
      auto mat1_t = (mat1_t_raw.defined() || !mat1_tensor.defined())
        ? mat1_t_raw : at::_efficientzerotensor(mat1_tensor.sizes(), mat1_tensor.options());
      auto mat1_p = toNonOptPrimal(mat1);
      auto mat2_t_raw = toNonOptFwGrad(mat2);
      auto mat2_tensor = toNonOptTensor(mat2);
      auto mat2_t = (mat2_t_raw.defined() || !mat2_tensor.defined())
        ? mat2_t_raw : at::_efficientzerotensor(mat2_tensor.sizes(), mat2_tensor.options());
      auto mat2_p = toNonOptPrimal(mat2);
      result_new_fw_grad_opt = maybe_multiply(self_t, beta) + maybe_multiply(mat1_t.mm(mat2_p), alpha) + maybe_multiply(mat1_p.mm(mat2_t), alpha);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & addmv_(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  auto& mat_ = unpack(mat, "mat", 1);
  auto& vec_ = unpack(vec, "vec", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, mat, vec );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(mat) || isFwGradDefined(vec));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<AddmvBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AddmvBackward0>(new AddmvBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, mat, vec ));
    grad_fn->alpha = alpha;
    grad_fn->beta = beta;
    if (grad_fn->should_compute_output(2)) {
      grad_fn->mat_ = SavedVariable(mat, false);
    }
    if (grad_fn->should_compute_output(1)) {
      grad_fn->vec_ = SavedVariable(vec, false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mat__storage_saved =
    mat_.has_storage() ? c10::optional<Storage>(mat_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mat__impl_saved;
  if (mat_.defined()) mat__impl_saved = mat_.getIntrusivePtr();
  c10::optional<Storage> vec__storage_saved =
    vec_.has_storage() ? c10::optional<Storage>(vec_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> vec__impl_saved;
  if (vec_.defined()) vec__impl_saved = vec_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::addmv_(ks & c10::after_autograd_keyset, self_, mat_, vec_, beta, alpha);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mat__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mat_))
    TORCH_INTERNAL_ASSERT(mat__storage_saved.value().is_alias_of(mat_.storage()));
  if (mat__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mat_))
    TORCH_INTERNAL_ASSERT(mat__impl_saved == mat_.getIntrusivePtr());
  if (vec__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(vec_))
    TORCH_INTERNAL_ASSERT(vec__storage_saved.value().is_alias_of(vec_.storage()));
  if (vec__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(vec_))
    TORCH_INTERNAL_ASSERT(vec__impl_saved == vec_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto mat_t_raw = toNonOptFwGrad(mat);
      auto mat_tensor = toNonOptTensor(mat);
      auto mat_t = (mat_t_raw.defined() || !mat_tensor.defined())
        ? mat_t_raw : at::_efficientzerotensor(mat_tensor.sizes(), mat_tensor.options());
      auto mat_p = toNonOptPrimal(mat);
      auto vec_t_raw = toNonOptFwGrad(vec);
      auto vec_tensor = toNonOptTensor(vec);
      auto vec_t = (vec_t_raw.defined() || !vec_tensor.defined())
        ? vec_t_raw : at::_efficientzerotensor(vec_tensor.sizes(), vec_tensor.options());
      auto vec_p = toNonOptPrimal(vec);
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(maybe_multiply(self_t, beta) + maybe_multiply(mat_t.mv(vec_p), alpha) + maybe_multiply(mat_p.mv(vec_t), alpha)) : maybe_multiply(self_t, beta) + maybe_multiply(mat_t.mv(vec_p), alpha) + maybe_multiply(mat_p.mv(vec_t), alpha);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
bool allclose(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::allclose(ks & c10::after_autograd_keyset, self_, other_, rtol, atol, equal_nan);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor & amin_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("amin");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("amin");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::amin_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with amin_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & angle_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("angle");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("angle");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::angle_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with angle_out that does not support it because it is an out= function");
  return out;
}
at::Tensor argmax(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::argmax(ks & c10::after_autograd_keyset, self_, dim, keepdim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: argmax");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: argmax");
  #endif
  return result;
}
at::Tensor & argsort_out_stable_out(c10::DispatchKeySet ks, const at::Tensor & self, bool stable, int64_t dim, bool descending, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::argsort_outf(ks & c10::after_autograd_keyset, self_, stable, dim, descending, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with argsort_out that does not support it because it is an out= function");
  return out;
}
at::Tensor as_strided(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<AsStridedBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AsStridedBackward0>(new AsStridedBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_geometry = TensorGeometry(self);
    grad_fn->size = size.vec();
    grad_fn->storage_offset = storage_offset;
    grad_fn->stride = stride.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::as_strided_symint(ks & c10::after_autograd_keyset, self_, size, stride, storage_offset);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: as_strided");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::as_strided_symint(self_t, size, stride, storage_offset);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor as_strided_scatter(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & src, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset) {
  auto& self_ = unpack(self, "self", 0);
  auto& src_ = unpack(src, "src", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, src );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(src));
  std::shared_ptr<AsStridedScatterBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AsStridedScatterBackward0>(new AsStridedScatterBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, src ));
    grad_fn->self_geometry = TensorGeometry(self);
    grad_fn->size = size.vec();
    grad_fn->src_geometry = TensorGeometry(src);
    grad_fn->storage_offset = storage_offset;
    grad_fn->stride = stride.vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> src__storage_saved =
    src_.has_storage() ? c10::optional<Storage>(src_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> src__impl_saved;
  if (src_.defined()) src__impl_saved = src_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::as_strided_scatter_symint(ks & c10::after_autograd_keyset, self_, src_, size, stride, storage_offset);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (src__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__storage_saved.value().is_alias_of(src_.storage()));
  if (src__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__impl_saved == src_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: as_strided_scatter");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: as_strided_scatter");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto src_t_raw = toNonOptFwGrad(src);
      auto src_tensor = toNonOptTensor(src);
      auto src_t = (src_t_raw.defined() || !src_tensor.defined())
        ? src_t_raw : at::_efficientzerotensor(src_tensor.sizes(), src_tensor.options());
      result_new_fw_grad_opt = at::as_strided_scatter_symint(self_t, src_t, size, stride, storage_offset);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & atanh_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<AtanhBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AtanhBackward1>(new AtanhBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::atanh_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self))), "Trying to use forward AD with atanh_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
  return self;
}
at::Tensor & avg_pool2d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 7);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("avg_pool2d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("avg_pool2d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::avg_pool2d_outf(ks & c10::after_autograd_keyset, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with avg_pool2d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & binary_cross_entropy_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& target_ = unpack(target, "target", 1);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, target );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(target));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, target, weight )) {
    throw_error_out_requires_grad("binary_cross_entropy");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("binary_cross_entropy");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::binary_cross_entropy_outf(ks & c10::after_autograd_keyset, self_, target_, weight, reduction, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(target) || isFwGradDefined(weight) || isFwGradDefined(out))), "Trying to use forward AD with binary_cross_entropy_out that does not support it because it is an out= function");
  return out;
}
at::Tensor cat(c10::DispatchKeySet ks, const at::ITensorListRef & tensors, int64_t dim) {
  auto tensors_ = unpack(tensors, "tensors", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( tensors );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = true;
  std::shared_ptr<CatBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CatBackward0>(new CatBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( tensors ));
    grad_fn->dim = dim;
    grad_fn->tensors_args_scalartypes = to_args_scalartypes(tensors);
    grad_fn->tensors_args_sizes_symint = to_args_sizes_symint(tensors);
    grad_fn->tensors_size_ = tensors.size();
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> tensors__storage_saved(tensors_.size());
  for (const Tensor& tensor : tensors_)
    tensors__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensors__impl_saved(tensors_.size());
  for (size_t i=0; i<tensors_.size(); i++)
    if (tensors_[i].defined()) tensors__impl_saved[i] = tensors_[i].getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::cat(ks & c10::after_autograd_keyset, tensors_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  for (size_t i=0; i<tensors_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensors_))
      TORCH_INTERNAL_ASSERT(tensors__storage_saved[i].value().is_alias_of(tensors_[i].storage()));
  }
  for (size_t i=0; i<tensors_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensors_))
      TORCH_INTERNAL_ASSERT(tensors__impl_saved[i] == tensors_[i].getIntrusivePtr());
  }
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: cat");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: cat");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
  
      result_new_fw_grad_opt = cat_jvp(tensors, dim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor cholesky(c10::DispatchKeySet ks, const at::Tensor & self, bool upper) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<CholeskyBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CholeskyBackward0>(new CholeskyBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->upper = upper;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::cholesky", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("cholesky", *opt_op, ks, self, upper);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::cholesky(ks & c10::after_autograd_keyset, self_, upper);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: cholesky");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: cholesky");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & clamp_(c10::DispatchKeySet ks, at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<ClampBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ClampBackward1>(new ClampBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->max = max;
    grad_fn->min = min;
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::clamp_(ks & c10::after_autograd_keyset, self_, min, max);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((clamp_backward(original_self_t.conj(), original_self_p, min, max)).conj()) : (clamp_backward(original_self_t.conj(), original_self_p, min, max)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & clamp__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, min, max );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(min) || isFwGradDefined(max));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<ClampBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ClampBackward0>(new ClampBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, min, max ));
    grad_fn->max_ = SavedVariable(max, false);
    grad_fn->min_ = SavedVariable(min, false);
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::clamp_(ks & c10::after_autograd_keyset, self_, min, max);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto min_t_raw = toNonOptFwGrad(min);
      auto min_tensor = toNonOptTensor(min);
      auto min_t = (min_t_raw.defined() || !min_tensor.defined())
        ? min_t_raw : at::_efficientzerotensor(min_tensor.sizes(), min_tensor.options());
      auto min_p = toNonOptPrimal(min);
      auto max_t_raw = toNonOptFwGrad(max);
      auto max_tensor = toNonOptTensor(max);
      auto max_t = (max_t_raw.defined() || !max_tensor.defined())
        ? max_t_raw : at::_efficientzerotensor(max_tensor.sizes(), max_tensor.options());
      auto max_p = toNonOptPrimal(max);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(clamp_jvp(original_self_p, original_self_t, min_p, min_t, max_p, max_t)) : clamp_jvp(original_self_p, original_self_t, min_p, min_t, max_p, max_t);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & clamp_min_(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & min) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<ClampMinBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ClampMinBackward0>(new ClampMinBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->min = min;
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::clamp_min_(ks & c10::after_autograd_keyset, self_, min);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((where(original_self_p >= min, original_self_t.conj(), at::scalar_tensor(0., original_self_t.conj().options()))).conj()) : (where(original_self_p >= min, original_self_t.conj(), at::scalar_tensor(0., original_self_t.conj().options()))).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & clamp_min__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & min) {
  auto& self_ = unpack(self, "self", 0);
  auto& min_ = unpack(min, "min", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, min );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(min));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<ClampMinBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ClampMinBackward1>(new ClampMinBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, min ));
    grad_fn->min_ = SavedVariable(min, false);
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> min__storage_saved =
    min_.has_storage() ? c10::optional<Storage>(min_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> min__impl_saved;
  if (min_.defined()) min__impl_saved = min_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::clamp_min_(ks & c10::after_autograd_keyset, self_, min_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (min__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(min_))
    TORCH_INTERNAL_ASSERT(min__storage_saved.value().is_alias_of(min_.storage()));
  if (min__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(min_))
    TORCH_INTERNAL_ASSERT(min__impl_saved == min_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto min_t_raw = toNonOptFwGrad(min);
      auto min_tensor = toNonOptTensor(min);
      auto min_t = (min_t_raw.defined() || !min_tensor.defined())
        ? min_t_raw : at::_efficientzerotensor(min_tensor.sizes(), min_tensor.options());
      auto min_p = toNonOptPrimal(min);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::_efficientzerotensor(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_(where(original_self_p >= min_p, original_self_t, min_t)) : where(original_self_p >= min_p, original_self_t, min_t);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor convolution(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias));
  std::shared_ptr<ConvolutionBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ConvolutionBackward0>(new ConvolutionBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->bias_sym_sizes_opt = bias.has_value() ? c10::optional<c10::SymIntArrayRef>(bias->sym_sizes()) : c10::nullopt;
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->output_padding = output_padding.vec();
    grad_fn->padding = padding.vec();
    grad_fn->stride = stride.vec();
    grad_fn->transposed = transposed;
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::convolution_symint(ks & c10::after_autograd_keyset, input_, weight_, bias, stride, padding, dilation, transposed, output_padding, groups);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: convolution");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: convolution");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "convolution");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto input_t_raw = toNonOptFwGrad(input);
      auto input_tensor = toNonOptTensor(input);
      auto input_t = (input_t_raw.defined() || !input_tensor.defined())
        ? input_t_raw : at::_efficientzerotensor(input_tensor.sizes(), input_tensor.options());
      auto input_p = toNonOptPrimal(input);
      auto weight_t_raw = toNonOptFwGrad(weight);
      auto weight_tensor = toNonOptTensor(weight);
      auto weight_t = (weight_t_raw.defined() || !weight_tensor.defined())
        ? weight_t_raw : at::_efficientzerotensor(weight_tensor.sizes(), weight_tensor.options());
      auto weight_p = toNonOptPrimal(weight);
      auto bias_t_raw = toNonOptFwGrad(bias);
      auto bias_tensor = toNonOptTensor(bias);
      auto bias_t = (bias_t_raw.defined() || !bias_tensor.defined())
        ? bias_t_raw : at::_efficientzerotensor(bias_tensor.sizes(), bias_tensor.options());
      auto bias_p = toNonOptPrimal(bias);
      result_new_fw_grad_opt = convolution_jvp(input_p, input_t, weight_p, weight_t, bias_p, bias_t, stride, padding, dilation, transposed, output_padding, groups);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> convolution_backward_overrideable(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array<bool,3> output_mask) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& input_ = unpack(input, "input", 1);
  auto& weight_ = unpack(weight, "weight", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, input, weight );
  
  std::shared_ptr<ConvolutionBackwardOverrideableBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ConvolutionBackwardOverrideableBackward0>(new ConvolutionBackwardOverrideableBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, input, weight ));
    grad_fn->dilation = dilation.vec();
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->groups = groups;
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->output_padding = output_padding.vec();
    grad_fn->padding = padding.vec();
    grad_fn->stride = stride.vec();
    grad_fn->transposed = transposed;
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor grad_input;
  at::Tensor grad_weight;
  at::Tensor grad_bias;
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(grad_output) || isFwGradDefined(input) || isFwGradDefined(weight))) {
      static c10::OperatorName full_name("aten::convolution_backward_overrideable", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor>>("convolution_backward_overrideable", *opt_op, ks, grad_output, input, weight, stride, padding, dilation, transposed, output_padding, groups, output_mask);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::convolution_backward_overrideable_symint(ks & c10::after_autograd_keyset, grad_output_, input_, weight_, stride, padding, dilation, transposed, output_padding, groups, output_mask);
    }
  })();
  std::tie(grad_input, grad_weight, grad_bias) = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (grad_input.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input)) {
    TORCH_INTERNAL_ASSERT(grad_input.storage().use_count() == 1, "function: convolution_backward_overrideable");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input))
    TORCH_INTERNAL_ASSERT(grad_input.use_count() <= 1, "function: convolution_backward_overrideable");
  if (grad_weight.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_weight)) {
    TORCH_INTERNAL_ASSERT(grad_weight.storage().use_count() == 1, "function: convolution_backward_overrideable");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_weight))
    TORCH_INTERNAL_ASSERT(grad_weight.use_count() <= 1, "function: convolution_backward_overrideable");
  if (grad_bias.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_bias)) {
    TORCH_INTERNAL_ASSERT(grad_bias.storage().use_count() == 1, "function: convolution_backward_overrideable");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_bias))
    TORCH_INTERNAL_ASSERT(grad_bias.use_count() <= 1, "function: convolution_backward_overrideable");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( grad_input, grad_weight, grad_bias ), grad_fn);
  }
  throw_error_for_complex_autograd(grad_input, "convolution_backward_overrideable");
  throw_error_for_complex_autograd(grad_weight, "convolution_backward_overrideable");
  throw_error_for_complex_autograd(grad_bias, "convolution_backward_overrideable");
  return std::make_tuple(std::move(grad_input), std::move(grad_weight), std::move(grad_bias));
}
at::Tensor cos(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<CosBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CosBackward0>(new CosBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::cos(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: cos");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: cos");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() * -self_p.sin().conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & cos_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("cos");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("cos");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::cos_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with cos_out that does not support it because it is an out= function");
  return out;
}
at::Tensor crow_indices_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::crow_indices_copy(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: crow_indices_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: crow_indices_copy");
  #endif
  return result;
}
at::Tensor cudnn_affine_grid_generator(c10::DispatchKeySet ks, const at::Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) {
  auto& theta_ = unpack(theta, "theta", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( theta );
  
  std::shared_ptr<CudnnAffineGridGeneratorBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CudnnAffineGridGeneratorBackward0>(new CudnnAffineGridGeneratorBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( theta ));
    grad_fn->C = C;
    grad_fn->H = H;
    grad_fn->N = N;
    grad_fn->W = W;
  }
  #ifndef NDEBUG
  c10::optional<Storage> theta__storage_saved =
    theta_.has_storage() ? c10::optional<Storage>(theta_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> theta__impl_saved;
  if (theta_.defined()) theta__impl_saved = theta_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(theta))) {
      static c10::OperatorName full_name("aten::cudnn_affine_grid_generator", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("cudnn_affine_grid_generator", *opt_op, ks, theta, N, C, H, W);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::cudnn_affine_grid_generator(ks & c10::after_autograd_keyset, theta_, N, C, H, W);
    }
  })();
  auto grid = std::move(_tmp);
  #ifndef NDEBUG
  if (theta__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(theta_))
    TORCH_INTERNAL_ASSERT(theta__storage_saved.value().is_alias_of(theta_.storage()));
  if (theta__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(theta_))
    TORCH_INTERNAL_ASSERT(theta__impl_saved == theta_.getIntrusivePtr());
  if (grid.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grid)) {
    TORCH_INTERNAL_ASSERT(grid.storage().use_count() == 1, "function: cudnn_affine_grid_generator");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grid))
    TORCH_INTERNAL_ASSERT(grid.use_count() <= 1, "function: cudnn_affine_grid_generator");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( grid ), grad_fn);
  }
  throw_error_for_complex_autograd(grid, "cudnn_affine_grid_generator");
  return grid;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> cudnn_batch_norm_backward(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon, const at::Tensor & reserveSpace) {
  auto& input_ = unpack(input, "input", 0);
  auto& grad_output_ = unpack(grad_output, "grad_output", 1);
  auto& weight_ = unpack(weight, "weight", 2);
  auto& reserveSpace_ = unpack(reserveSpace, "reserveSpace", 8);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, grad_output, weight, save_mean, save_var, reserveSpace );
  
  check_no_requires_grad(running_mean, "running_mean", "cudnn_batch_norm_backward");
  check_no_requires_grad(running_var, "running_var", "cudnn_batch_norm_backward");
  std::shared_ptr<CudnnBatchNormBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CudnnBatchNormBackwardBackward0>(new CudnnBatchNormBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, grad_output, weight, save_mean, save_var, reserveSpace ));
    grad_fn->epsilon = epsilon;
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->reserveSpace_ = SavedVariable(reserveSpace, false);
    grad_fn->running_mean_ = SavedVariable(running_mean, false);
    grad_fn->running_var_ = SavedVariable(running_var, false);
    grad_fn->save_mean_ = SavedVariable(save_mean, false);
    grad_fn->save_var_ = SavedVariable(save_var, false);
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  c10::optional<Storage> reserveSpace__storage_saved =
    reserveSpace_.has_storage() ? c10::optional<Storage>(reserveSpace_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> reserveSpace__impl_saved;
  if (reserveSpace_.defined()) reserveSpace__impl_saved = reserveSpace_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input) || isFwGradDefined(grad_output) || isFwGradDefined(weight) || isFwGradDefined(running_mean) || isFwGradDefined(running_var) || isFwGradDefined(save_mean) || isFwGradDefined(save_var) || isFwGradDefined(reserveSpace))) {
      static c10::OperatorName full_name("aten::cudnn_batch_norm_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor>>("cudnn_batch_norm_backward", *opt_op, ks, input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon, reserveSpace);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::cudnn_batch_norm_backward(ks & c10::after_autograd_keyset, input_, grad_output_, weight_, running_mean, running_var, save_mean, save_var, epsilon, reserveSpace_);
    }
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (reserveSpace__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(reserveSpace_))
    TORCH_INTERNAL_ASSERT(reserveSpace__storage_saved.value().is_alias_of(reserveSpace_.storage()));
  if (reserveSpace__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(reserveSpace_))
    TORCH_INTERNAL_ASSERT(reserveSpace__impl_saved == reserveSpace_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: cudnn_batch_norm_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: cudnn_batch_norm_backward");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: cudnn_batch_norm_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: cudnn_batch_norm_backward");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: cudnn_batch_norm_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: cudnn_batch_norm_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "cudnn_batch_norm_backward");
  throw_error_for_complex_autograd(result1, "cudnn_batch_norm_backward");
  throw_error_for_complex_autograd(result2, "cudnn_batch_norm_backward");
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
at::Tensor cudnn_convolution_transpose(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight );
  
  std::shared_ptr<CudnnConvolutionTransposeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CudnnConvolutionTransposeBackward0>(new CudnnConvolutionTransposeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight ));
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
    grad_fn->output_padding = output_padding.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight))) {
      static c10::OperatorName full_name("aten::cudnn_convolution_transpose", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("cudnn_convolution_transpose", *opt_op, ks, self, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::cudnn_convolution_transpose_symint(ks & c10::after_autograd_keyset, self_, weight_, padding, output_padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: cudnn_convolution_transpose");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: cudnn_convolution_transpose");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "cudnn_convolution_transpose");
  return result;
}
at::Tensor cumsum(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<CumsumBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<CumsumBackward0>(new CumsumBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::cumsum(ks & c10::after_autograd_keyset, self_, dim, dtype);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: cumsum");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: cumsum");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::cumsum(self_t, dim, dtype);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor diagonal_scatter(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & src, int64_t offset, int64_t dim1, int64_t dim2) {
  auto& self_ = unpack(self, "self", 0);
  auto& src_ = unpack(src, "src", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, src );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(src));
  std::shared_ptr<DiagonalScatterBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DiagonalScatterBackward0>(new DiagonalScatterBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, src ));
    grad_fn->dim1 = dim1;
    grad_fn->dim2 = dim2;
    grad_fn->offset = offset;
    grad_fn->src_info = src;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> src__storage_saved =
    src_.has_storage() ? c10::optional<Storage>(src_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> src__impl_saved;
  if (src_.defined()) src__impl_saved = src_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::diagonal_scatter(ks & c10::after_autograd_keyset, self_, src_, offset, dim1, dim2);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (src__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__storage_saved.value().is_alias_of(src_.storage()));
  if (src__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(src_))
    TORCH_INTERNAL_ASSERT(src__impl_saved == src_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: diagonal_scatter");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: diagonal_scatter");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto src_t_raw = toNonOptFwGrad(src);
      auto src_tensor = toNonOptTensor(src);
      auto src_t = (src_t_raw.defined() || !src_tensor.defined())
        ? src_t_raw : at::_efficientzerotensor(src_tensor.sizes(), src_tensor.options());
      result_new_fw_grad_opt = at::diagonal_scatter(self_t, src_t, offset, dim1, dim2);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & digamma_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<DigammaBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DigammaBackward0>(new DigammaBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::digamma_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * polygamma(1, original_self_p)).conj()) : (original_self_t.conj() * polygamma(1, original_self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & div_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("div");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("div");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::div_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with div_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & div_out_out_mode(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("div");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("div");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::div_outf(ks & c10::after_autograd_keyset, self_, other_, rounding_mode, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with div_out that does not support it because it is an out= function");
  return out;
}
at::Tensor elu_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_or_result_ = unpack(self_or_result, "self_or_result", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self_or_result );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self_or_result));
  std::shared_ptr<EluBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<EluBackwardBackward0>(new EluBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self_or_result ));
    grad_fn->alpha = alpha;
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->input_scale = input_scale;
    grad_fn->is_result = is_result;
    grad_fn->scale = scale;
    grad_fn->self_or_result_ = SavedVariable(self_or_result, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self_or_result__storage_saved =
    self_or_result_.has_storage() ? c10::optional<Storage>(self_or_result_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self_or_result__impl_saved;
  if (self_or_result_.defined()) self_or_result__impl_saved = self_or_result_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::elu_backward(ks & c10::after_autograd_keyset, grad_output_, alpha, scale, input_scale, is_result, self_or_result_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self_or_result__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_or_result_))
    TORCH_INTERNAL_ASSERT(self_or_result__storage_saved.value().is_alias_of(self_or_result_.storage()));
  if (self_or_result__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_or_result_))
    TORCH_INTERNAL_ASSERT(self_or_result__impl_saved == self_or_result_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: elu_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: elu_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "elu_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto self_or_result_t_raw = toNonOptFwGrad(self_or_result);
      auto self_or_result_tensor = toNonOptTensor(self_or_result);
      auto self_or_result_t = (self_or_result_t_raw.defined() || !self_or_result_tensor.defined())
        ? self_or_result_t_raw : at::_efficientzerotensor(self_or_result_tensor.sizes(), self_or_result_tensor.options());
      auto self_or_result_p = toNonOptPrimal(self_or_result);
      result_new_fw_grad_opt = elu_backward(grad_output_t, alpha, scale, input_scale, is_result, self_or_result_p) + elu_double_backward(self_or_result_t, grad_output_p, alpha, scale, input_scale, is_result, self_or_result_p);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor embedding_dense_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & indices, c10::SymInt num_weights, c10::SymInt padding_idx, bool scale_grad_by_freq) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& indices_ = unpack(indices, "indices", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<EmbeddingDenseBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<EmbeddingDenseBackwardBackward0>(new EmbeddingDenseBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output ));
    grad_fn->indices_ = SavedVariable(indices, false);
    grad_fn->padding_idx = padding_idx;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::embedding_dense_backward_symint(ks & c10::after_autograd_keyset, grad_output_, indices_, num_weights, padding_idx, scale_grad_by_freq);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: embedding_dense_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: embedding_dense_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "embedding_dense_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      result_new_fw_grad_opt = at::embedding_dense_backward_symint(grad_output_t, indices, num_weights, padding_idx, scale_grad_by_freq);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor eq_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::eq(ks & c10::after_autograd_keyset, self_, other);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: eq_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: eq_Scalar");
  #endif
  return result;
}
at::Tensor eq_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::eq(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: eq_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: eq_Tensor");
  #endif
  return result;
}
at::Tensor & erfinv_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("erfinv");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("erfinv");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::erfinv_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with erfinv_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & exp_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<ExpBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ExpBackward0>(new ExpBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::exp_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((self_t.conj() * self_p.conj()).conj()) : (self_t.conj() * self_p.conj()).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor expand_copy(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, bool implicit) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ExpandBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ExpandBackward0_copy>(new ExpandBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::expand_copy_symint(ks & c10::after_autograd_keyset, self_, size, implicit);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: expand_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: expand_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "expand_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t.expand_symint(size, implicit);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor expm1(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Expm1Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Expm1Backward0>(new Expm1Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::expm1(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: expm1");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: expm1");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (self_t.conj() * (result.conj() + 1)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor exponential(c10::DispatchKeySet ks, const at::Tensor & self, double lambd, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ExponentialBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ExponentialBackward0>(new ExponentialBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::exponential(ks & c10::after_autograd_keyset, self_, lambd, generator);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: exponential");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: exponential");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "exponential");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t.zero_();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & exponential_(c10::DispatchKeySet ks, at::Tensor & self, double lambd, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<ExponentialBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ExponentialBackward0>(new ExponentialBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::exponential_(ks & c10::after_autograd_keyset, self_, lambd, generator);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
::std::tuple<at::Tensor,at::Tensor> fake_quantize_per_channel_affine_cachemask(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
  auto& self_ = unpack(self, "self", 0);
  auto& scale_ = unpack(scale, "scale", 1);
  auto& zero_point_ = unpack(zero_point, "zero_point", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  check_no_requires_grad(scale, "scale", "fake_quantize_per_channel_affine_cachemask");
  check_no_requires_grad(zero_point, "zero_point", "fake_quantize_per_channel_affine_cachemask");
  std::shared_ptr<FakeQuantizePerChannelAffineCachemaskBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FakeQuantizePerChannelAffineCachemaskBackward0>(new FakeQuantizePerChannelAffineCachemaskBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  at::Tensor output;
  at::Tensor mask;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> scale__storage_saved =
    scale_.has_storage() ? c10::optional<Storage>(scale_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> scale__impl_saved;
  if (scale_.defined()) scale__impl_saved = scale_.getIntrusivePtr();
  c10::optional<Storage> zero_point__storage_saved =
    zero_point_.has_storage() ? c10::optional<Storage>(zero_point_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> zero_point__impl_saved;
  if (zero_point_.defined()) zero_point__impl_saved = zero_point_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(scale) || isFwGradDefined(zero_point))) {
      static c10::OperatorName full_name("aten::fake_quantize_per_channel_affine_cachemask", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor>>("fake_quantize_per_channel_affine_cachemask", *opt_op, ks, self, scale, zero_point, axis, quant_min, quant_max);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::fake_quantize_per_channel_affine_cachemask(ks & c10::after_autograd_keyset, self_, scale_, zero_point_, axis, quant_min, quant_max);
    }
  })();
  std::tie(output, mask) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (scale__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(scale_))
    TORCH_INTERNAL_ASSERT(scale__storage_saved.value().is_alias_of(scale_.storage()));
  if (scale__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(scale_))
    TORCH_INTERNAL_ASSERT(scale__impl_saved == scale_.getIntrusivePtr());
  if (zero_point__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(zero_point_))
    TORCH_INTERNAL_ASSERT(zero_point__storage_saved.value().is_alias_of(zero_point_.storage()));
  if (zero_point__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(zero_point_))
    TORCH_INTERNAL_ASSERT(zero_point__impl_saved == zero_point_.getIntrusivePtr());
  if (output.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output)) {
    TORCH_INTERNAL_ASSERT(output.storage().use_count() == 1, "function: fake_quantize_per_channel_affine_cachemask");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output))
    TORCH_INTERNAL_ASSERT(output.use_count() <= 1, "function: fake_quantize_per_channel_affine_cachemask");
  if (mask.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask)) {
    TORCH_INTERNAL_ASSERT(mask.storage().use_count() == 1, "function: fake_quantize_per_channel_affine_cachemask");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask))
    TORCH_INTERNAL_ASSERT(mask.use_count() <= 1, "function: fake_quantize_per_channel_affine_cachemask");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( output ), grad_fn);
  }
  throw_error_for_complex_autograd(output, "fake_quantize_per_channel_affine_cachemask");
  if (grad_fn) {
    grad_fn->mask_ = SavedVariable(mask, true);
  }
  return std::make_tuple(std::move(output), std::move(mask));
}
at::Tensor fill_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & value) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<FillBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FillBackward0>(new FillBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::fill(ks & c10::after_autograd_keyset, self_, value);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: fill_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: fill_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::fill(self_t, 0);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor fill_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & value) {
  auto& self_ = unpack(self, "self", 0);
  auto& value_ = unpack(value, "value", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, value );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(value));
  std::shared_ptr<FillBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FillBackward1>(new FillBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, value ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> value__storage_saved =
    value_.has_storage() ? c10::optional<Storage>(value_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> value__impl_saved;
  if (value_.defined()) value__impl_saved = value_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::fill(ks & c10::after_autograd_keyset, self_, value_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (value__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__storage_saved.value().is_alias_of(value_.storage()));
  if (value__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(value_))
    TORCH_INTERNAL_ASSERT(value__impl_saved == value_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: fill_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: fill_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto value_t_raw = toNonOptFwGrad(value);
      auto value_tensor = toNonOptTensor(value);
      auto value_t = (value_t_raw.defined() || !value_tensor.defined())
        ? value_t_raw : at::_efficientzerotensor(value_tensor.sizes(), value_tensor.options());
      result_new_fw_grad_opt = at::fill(self_t, value_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & floor_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<FloorBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FloorBackward0>(new FloorBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::floor_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((zeros_like(self_t.conj())).conj()) : (zeros_like(self_t.conj())).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & fmod_out_Scalar_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("fmod");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("fmod");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::fmod_outf(ks & c10::after_autograd_keyset, self_, other, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with fmod_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & fmod_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("fmod");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("fmod");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::fmod_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with fmod_out that does not support it because it is an out= function");
  return out;
}
at::Tensor fractional_max_pool3d_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& indices_ = unpack(indices, "indices", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  check_no_requires_grad(indices, "indices", "fractional_max_pool3d_backward");
  std::shared_ptr<FractionalMaxPool3DBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FractionalMaxPool3DBackwardBackward0>(new FractionalMaxPool3DBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->indices_ = SavedVariable(indices, false);
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::fractional_max_pool3d_backward(ks & c10::after_autograd_keyset, grad_output_, self_, kernel_size, output_size, indices_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: fractional_max_pool3d_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: fractional_max_pool3d_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "fractional_max_pool3d_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::fractional_max_pool3d_backward(grad_output_t, self_t, kernel_size, output_size, indices);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & full_like_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & fill_value, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 3);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::full_like_outf(ks & c10::after_autograd_keyset, self_, fill_value, memory_format, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with full_like_out that does not support it because it is an out= function");
  return out;
}
at::Tensor gelu_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<GeluBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GeluBackwardBackward0>(new GeluBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->approximate = std::string(approximate);
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::gelu_backward(ks & c10::after_autograd_keyset, grad_output_, self_, approximate);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: gelu_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: gelu_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "gelu_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = gelu_backward(grad_output_t, self_p, approximate) + gelu_double_backward(self_t, grad_output_p, self_p, approximate);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & gelu_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& grad_input_ = unpack(grad_input, "grad_input", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self )) {
    throw_error_out_requires_grad("gelu_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("gelu_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::gelu_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, approximate, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with gelu_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor geometric(c10::DispatchKeySet ks, const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<GeometricBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GeometricBackward0>(new GeometricBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::geometric(ks & c10::after_autograd_keyset, self_, p, generator);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: geometric");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: geometric");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "geometric");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t.zero_();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & geometric_(c10::DispatchKeySet ks, at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<GeometricBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<GeometricBackward0>(new GeometricBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::geometric_(ks & c10::after_autograd_keyset, self_, p, generator);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & glu_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("glu");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("glu");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::glu_outf(ks & c10::after_autograd_keyset, self_, dim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with glu_out that does not support it because it is an out= function");
  return out;
}
at::Tensor igamma(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  std::shared_ptr<IgammaBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<IgammaBackward0>(new IgammaBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    if (grad_fn->should_compute_output(1)) {
      grad_fn->other_ = SavedVariable(other, false);
    }
    if (grad_fn->should_compute_output(1)) {
      grad_fn->self_ = SavedVariable(self, false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(other))) {
      static c10::OperatorName full_name("aten::igamma", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("igamma", *opt_op, ks, self, other);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::igamma(ks & c10::after_autograd_keyset, self_, other_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: igamma");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: igamma");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "igamma");
  return result;
}
at::Tensor index_copy(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  auto& source_ = unpack(source, "source", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, source );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(source));
  std::shared_ptr<IndexCopyBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<IndexCopyBackward0>(new IndexCopyBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, source ));
    grad_fn->dim = dim;
    grad_fn->index_ = SavedVariable(index, false);
    if (grad_fn->should_compute_output(1)) {
      grad_fn->source_ = SavedVariable(source, false);
    }
    grad_fn->source_dim = source.dim();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> source__storage_saved =
    source_.has_storage() ? c10::optional<Storage>(source_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> source__impl_saved;
  if (source_.defined()) source__impl_saved = source_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::index_copy(ks & c10::after_autograd_keyset, self_, dim, index_, source_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (source__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__storage_saved.value().is_alias_of(source_.storage()));
  if (source__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__impl_saved == source_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: index_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: index_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto source_t_raw = toNonOptFwGrad(source);
      auto source_tensor = toNonOptTensor(source);
      auto source_t = (source_t_raw.defined() || !source_tensor.defined())
        ? source_t_raw : at::_efficientzerotensor(source_tensor.sizes(), source_tensor.options());
      result_new_fw_grad_opt = self_t.index_copy(dim, index, source_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & index_copy_(c10::DispatchKeySet ks, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  auto& source_ = unpack(source, "source", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, source );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self) || isFwGradDefined(source));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<IndexCopyBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<IndexCopyBackward0>(new IndexCopyBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, source ));
    grad_fn->dim = dim;
    grad_fn->index_ = SavedVariable(index, false);
    if (grad_fn->should_compute_output(1)) {
      grad_fn->source_ = SavedVariable(source, false);
    }
    grad_fn->source_dim = source.dim();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> source__storage_saved =
    source_.has_storage() ? c10::optional<Storage>(source_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> source__impl_saved;
  if (source_.defined()) source__impl_saved = source_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::index_copy_(ks & c10::after_autograd_keyset, self_, dim, index_, source_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (source__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__storage_saved.value().is_alias_of(source_.storage()));
  if (source__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(source_))
    TORCH_INTERNAL_ASSERT(source__impl_saved == source_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto source_t_raw = toNonOptFwGrad(source);
      auto source_tensor = toNonOptTensor(source);
      auto source_t = (source_t_raw.defined() || !source_tensor.defined())
        ? source_t_raw : at::_efficientzerotensor(source_tensor.sizes(), source_tensor.options());
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.index_copy_(dim, index, source_t) : self_t.index_copy(dim, index, source_t);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor index_select(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, const at::Tensor & index) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<IndexSelectBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<IndexSelectBackward0>(new IndexSelectBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->index_ = SavedVariable(index, false);
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::index_select(ks & c10::after_autograd_keyset, self_, dim, index_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: index_select");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: index_select");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::index_select(self_t, dim, index);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
bool is_pinned(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::Device> device) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::is_pinned(ks & c10::after_autograd_keyset, self_, device);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor & isnan_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::isnan_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with isnan_out that does not support it because it is an out= function");
  return out;
}
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_out_values(c10::DispatchKeySet ks, const at::Tensor & self, int64_t k, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  auto& self_ = unpack(self, "self", 0);
  auto& values_ = unpack(values, "values", 4);
  auto& indices_ = unpack(indices, "indices", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("kthvalue");
  }
  if (compute_requires_grad( values )) {
    throw_error_out_requires_grad("kthvalue");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> values__storage_saved =
    values_.has_storage() ? c10::optional<Storage>(values_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> values__impl_saved;
  if (values_.defined()) values__impl_saved = values_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::kthvalue_outf(ks & c10::after_autograd_keyset, self_, k, dim, keepdim, values_, indices_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__storage_saved.value().is_alias_of(values_.storage()));
  if (values__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values_))
    TORCH_INTERNAL_ASSERT(values__impl_saved == values_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( values ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(values) || isFwGradDefined(indices))), "Trying to use forward AD with kthvalue_out that does not support it because it is an out= function");
  return std::forward_as_tuple(values, indices);
}
at::Tensor le_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::le(ks & c10::after_autograd_keyset, self_, other);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: le_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: le_Scalar");
  #endif
  return result;
}
at::Tensor le_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::le(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: le_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: le_Tensor");
  #endif
  return result;
}
at::Tensor & le__Scalar(c10::DispatchKeySet ks, at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<LeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LeBackward0>(new LeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::le_(ks & c10::after_autograd_keyset, self_, other);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & le__Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<LeBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LeBackward1>(new LeBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_info = other;
    grad_fn->self_info = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::le_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & leaky_relu_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& grad_input_ = unpack(grad_input, "grad_input", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self )) {
    throw_error_out_requires_grad("leaky_relu_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("leaky_relu_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::leaky_relu_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, negative_slope, self_is_result, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with leaky_relu_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & lerp_out_Scalar_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& end_ = unpack(end, "end", 1);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, end );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(end));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, end )) {
    throw_error_out_requires_grad("lerp");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("lerp");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> end__storage_saved =
    end_.has_storage() ? c10::optional<Storage>(end_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> end__impl_saved;
  if (end_.defined()) end__impl_saved = end_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::lerp_outf(ks & c10::after_autograd_keyset, self_, end_, weight, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (end__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(end_))
    TORCH_INTERNAL_ASSERT(end__storage_saved.value().is_alias_of(end_.storage()));
  if (end__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(end_))
    TORCH_INTERNAL_ASSERT(end__impl_saved == end_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(end) || isFwGradDefined(out))), "Trying to use forward AD with lerp_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & lerp_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& end_ = unpack(end, "end", 1);
  auto& weight_ = unpack(weight, "weight", 2);
  auto& out_ = unpack(out, "out", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, end, weight );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(end) || isFwGradDefined(weight));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, end, weight )) {
    throw_error_out_requires_grad("lerp");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("lerp");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> end__storage_saved =
    end_.has_storage() ? c10::optional<Storage>(end_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> end__impl_saved;
  if (end_.defined()) end__impl_saved = end_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::lerp_outf(ks & c10::after_autograd_keyset, self_, end_, weight_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (end__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(end_))
    TORCH_INTERNAL_ASSERT(end__storage_saved.value().is_alias_of(end_.storage()));
  if (end__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(end_))
    TORCH_INTERNAL_ASSERT(end__impl_saved == end_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(end) || isFwGradDefined(weight) || isFwGradDefined(out))), "Trying to use forward AD with lerp_out that does not support it because it is an out= function");
  return out;
}
at::Tensor lgamma(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<LgammaBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LgammaBackward0>(new LgammaBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::lgamma(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: lgamma");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: lgamma");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "lgamma");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() * digamma(self_p)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & lgamma_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<LgammaBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LgammaBackward0>(new LgammaBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::lgamma_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() * digamma(original_self_p)).conj()) : (original_self_t.conj() * digamma(original_self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor & lgamma_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("lgamma");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("lgamma");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::lgamma_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with lgamma_out that does not support it because it is an out= function");
  return out;
}
at::Tensor lift(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<LiftBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LiftBackward0>(new LiftBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::lift(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "lift");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::lift(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor lift_fresh(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<LiftFreshBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LiftFreshBackward0>(new LiftFreshBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::lift_fresh(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "lift_fresh");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::lift_fresh(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_inv_ex_out_inverse(c10::DispatchKeySet ks, const at::Tensor & A, bool check_errors, at::Tensor & inverse, at::Tensor & info) {
  auto& A_ = unpack(A, "A", 0);
  auto& inverse_ = unpack(inverse, "inverse", 2);
  auto& info_ = unpack(info, "info", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_inverse = (isFwGradDefined(A));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( A )) {
    throw_error_out_requires_grad("linalg_inv_ex");
  }
  if (compute_requires_grad( inverse )) {
    throw_error_out_requires_grad("linalg_inv_ex");
  }
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  c10::optional<Storage> inverse__storage_saved =
    inverse_.has_storage() ? c10::optional<Storage>(inverse_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> inverse__impl_saved;
  if (inverse_.defined()) inverse__impl_saved = inverse_.getIntrusivePtr();
  c10::optional<Storage> info__storage_saved =
    info_.has_storage() ? c10::optional<Storage>(info_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> info__impl_saved;
  if (info_.defined()) info__impl_saved = info_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::linalg_inv_ex_outf(ks & c10::after_autograd_keyset, A_, check_errors, inverse_, info_);
  }
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (inverse__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(inverse_))
    TORCH_INTERNAL_ASSERT(inverse__storage_saved.value().is_alias_of(inverse_.storage()));
  if (inverse__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(inverse_))
    TORCH_INTERNAL_ASSERT(inverse__impl_saved == inverse_.getIntrusivePtr());
  if (info__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(info_))
    TORCH_INTERNAL_ASSERT(info__storage_saved.value().is_alias_of(info_.storage()));
  if (info__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(info_))
    TORCH_INTERNAL_ASSERT(info__impl_saved == info_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( inverse ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(A) || isFwGradDefined(inverse) || isFwGradDefined(info))), "Trying to use forward AD with linalg_inv_ex_out that does not support it because it is an out= function");
  return std::forward_as_tuple(inverse, info);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> linalg_lstsq(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver) {
  auto& self_ = unpack(self, "self", 0);
  auto& b_ = unpack(b, "b", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, b );
  
  [[maybe_unused]] auto _any_has_forward_grad_solution = (isFwGradDefined(self) || isFwGradDefined(b));
  std::shared_ptr<LinalgLstsqBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgLstsqBackward0>(new LinalgLstsqBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, b ));
    grad_fn->b_ = SavedVariable(b, false);
    grad_fn->self_ = SavedVariable(self, false);
  }
  at::Tensor solution;
  at::Tensor residuals;
  at::Tensor rank;
  at::Tensor singular_values;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> b__storage_saved =
    b_.has_storage() ? c10::optional<Storage>(b_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> b__impl_saved;
  if (b_.defined()) b__impl_saved = b_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::linalg_lstsq(ks & c10::after_autograd_keyset, self_, b_, rcond, driver);
  })();
  std::tie(solution, residuals, rank, singular_values) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (b__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(b_))
    TORCH_INTERNAL_ASSERT(b__storage_saved.value().is_alias_of(b_.storage()));
  if (b__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(b_))
    TORCH_INTERNAL_ASSERT(b__impl_saved == b_.getIntrusivePtr());
  if (solution.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(solution)) {
    TORCH_INTERNAL_ASSERT(solution.storage().use_count() == 1, "function: linalg_lstsq");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(solution))
    TORCH_INTERNAL_ASSERT(solution.use_count() <= 1, "function: linalg_lstsq");
  if (residuals.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(residuals)) {
    TORCH_INTERNAL_ASSERT(residuals.storage().use_count() == 1, "function: linalg_lstsq");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(residuals))
    TORCH_INTERNAL_ASSERT(residuals.use_count() <= 1, "function: linalg_lstsq");
  if (rank.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(rank)) {
    TORCH_INTERNAL_ASSERT(rank.storage().use_count() == 1, "function: linalg_lstsq");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(rank))
    TORCH_INTERNAL_ASSERT(rank.use_count() <= 1, "function: linalg_lstsq");
  if (singular_values.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(singular_values)) {
    TORCH_INTERNAL_ASSERT(singular_values.storage().use_count() == 1, "function: linalg_lstsq");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(singular_values))
    TORCH_INTERNAL_ASSERT(singular_values.use_count() <= 1, "function: linalg_lstsq");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( solution ), grad_fn);
  }
  c10::optional<at::Tensor> solution_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_solution && (solution.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto b_t_raw = toNonOptFwGrad(b);
      auto b_tensor = toNonOptTensor(b);
      auto b_t = (b_t_raw.defined() || !b_tensor.defined())
        ? b_t_raw : at::_efficientzerotensor(b_tensor.sizes(), b_tensor.options());
      auto b_p = toNonOptPrimal(b);
      solution_new_fw_grad_opt = linalg_lstsq_jvp(self_p, b_p, self_t, b_t);
  }
  if (solution_new_fw_grad_opt.has_value() && solution_new_fw_grad_opt.value().defined() && solution.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    solution._set_fw_grad(solution_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return std::make_tuple(std::move(solution), std::move(residuals), std::move(rank), std::move(singular_values));
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> linalg_lstsq_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver, at::Tensor & solution, at::Tensor & residuals, at::Tensor & rank, at::Tensor & singular_values) {
  auto& self_ = unpack(self, "self", 0);
  auto& b_ = unpack(b, "b", 1);
  auto& solution_ = unpack(solution, "solution", 4);
  auto& residuals_ = unpack(residuals, "residuals", 5);
  auto& rank_ = unpack(rank, "rank", 6);
  auto& singular_values_ = unpack(singular_values, "singular_values", 7);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, b );
  
  [[maybe_unused]] auto _any_has_forward_grad_solution = (isFwGradDefined(self) || isFwGradDefined(b));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, b )) {
    throw_error_out_requires_grad("linalg_lstsq");
  }
  if (compute_requires_grad( solution )) {
    throw_error_out_requires_grad("linalg_lstsq");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> b__storage_saved =
    b_.has_storage() ? c10::optional<Storage>(b_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> b__impl_saved;
  if (b_.defined()) b__impl_saved = b_.getIntrusivePtr();
  c10::optional<Storage> solution__storage_saved =
    solution_.has_storage() ? c10::optional<Storage>(solution_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> solution__impl_saved;
  if (solution_.defined()) solution__impl_saved = solution_.getIntrusivePtr();
  c10::optional<Storage> residuals__storage_saved =
    residuals_.has_storage() ? c10::optional<Storage>(residuals_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> residuals__impl_saved;
  if (residuals_.defined()) residuals__impl_saved = residuals_.getIntrusivePtr();
  c10::optional<Storage> rank__storage_saved =
    rank_.has_storage() ? c10::optional<Storage>(rank_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> rank__impl_saved;
  if (rank_.defined()) rank__impl_saved = rank_.getIntrusivePtr();
  c10::optional<Storage> singular_values__storage_saved =
    singular_values_.has_storage() ? c10::optional<Storage>(singular_values_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> singular_values__impl_saved;
  if (singular_values_.defined()) singular_values__impl_saved = singular_values_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::linalg_lstsq_outf(ks & c10::after_autograd_keyset, self_, b_, rcond, driver, solution_, residuals_, rank_, singular_values_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (b__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(b_))
    TORCH_INTERNAL_ASSERT(b__storage_saved.value().is_alias_of(b_.storage()));
  if (b__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(b_))
    TORCH_INTERNAL_ASSERT(b__impl_saved == b_.getIntrusivePtr());
  if (solution__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(solution_))
    TORCH_INTERNAL_ASSERT(solution__storage_saved.value().is_alias_of(solution_.storage()));
  if (solution__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(solution_))
    TORCH_INTERNAL_ASSERT(solution__impl_saved == solution_.getIntrusivePtr());
  if (residuals__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(residuals_))
    TORCH_INTERNAL_ASSERT(residuals__storage_saved.value().is_alias_of(residuals_.storage()));
  if (residuals__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(residuals_))
    TORCH_INTERNAL_ASSERT(residuals__impl_saved == residuals_.getIntrusivePtr());
  if (rank__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(rank_))
    TORCH_INTERNAL_ASSERT(rank__storage_saved.value().is_alias_of(rank_.storage()));
  if (rank__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(rank_))
    TORCH_INTERNAL_ASSERT(rank__impl_saved == rank_.getIntrusivePtr());
  if (singular_values__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(singular_values_))
    TORCH_INTERNAL_ASSERT(singular_values__storage_saved.value().is_alias_of(singular_values_.storage()));
  if (singular_values__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(singular_values_))
    TORCH_INTERNAL_ASSERT(singular_values__impl_saved == singular_values_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( solution ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(b) || isFwGradDefined(solution) || isFwGradDefined(residuals) || isFwGradDefined(rank) || isFwGradDefined(singular_values))), "Trying to use forward AD with linalg_lstsq_out that does not support it because it is an out= function");
  return std::forward_as_tuple(solution, residuals, rank, singular_values);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> linalg_lu(c10::DispatchKeySet ks, const at::Tensor & A, bool pivot) {
  auto& A_ = unpack(A, "A", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( A );
  
  [[maybe_unused]] auto _any_has_forward_grad_L = (isFwGradDefined(A));
  [[maybe_unused]] auto _any_has_forward_grad_U = (isFwGradDefined(A));
  std::shared_ptr<LinalgLuBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgLuBackward0>(new LinalgLuBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( A ));
    grad_fn->pivot = pivot;
  }
  at::Tensor P;
  at::Tensor L;
  at::Tensor U;
  #ifndef NDEBUG
  c10::optional<Storage> A__storage_saved =
    A_.has_storage() ? c10::optional<Storage>(A_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> A__impl_saved;
  if (A_.defined()) A__impl_saved = A_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::linalg_lu(ks & c10::after_autograd_keyset, A_, pivot);
  })();
  std::tie(P, L, U) = std::move(_tmp);
  #ifndef NDEBUG
  if (A__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__storage_saved.value().is_alias_of(A_.storage()));
  if (A__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(A_))
    TORCH_INTERNAL_ASSERT(A__impl_saved == A_.getIntrusivePtr());
  if (P.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(P)) {
    TORCH_INTERNAL_ASSERT(P.storage().use_count() == 1, "function: linalg_lu");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(P))
    TORCH_INTERNAL_ASSERT(P.use_count() <= 1, "function: linalg_lu");
  if (L.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(L)) {
    TORCH_INTERNAL_ASSERT(L.storage().use_count() == 1, "function: linalg_lu");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(L))
    TORCH_INTERNAL_ASSERT(L.use_count() <= 1, "function: linalg_lu");
  if (U.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(U)) {
    TORCH_INTERNAL_ASSERT(U.storage().use_count() == 1, "function: linalg_lu");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(U))
    TORCH_INTERNAL_ASSERT(U.use_count() <= 1, "function: linalg_lu");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( L, U ), grad_fn);
  }
  c10::optional<at::Tensor> L_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_L && (L.defined())) {
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      L_new_fw_grad_opt = std::get<0>(linalg_lu_jvp(A_t, P, L, U, pivot));
  }
  c10::optional<at::Tensor> U_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_U && (U.defined())) {
      auto A_t_raw = toNonOptFwGrad(A);
      auto A_tensor = toNonOptTensor(A);
      auto A_t = (A_t_raw.defined() || !A_tensor.defined())
        ? A_t_raw : at::_efficientzerotensor(A_tensor.sizes(), A_tensor.options());
      U_new_fw_grad_opt = std::get<1>(linalg_lu_jvp(A_t, P, L, U, pivot));
  }
  if (L_new_fw_grad_opt.has_value() && L_new_fw_grad_opt.value().defined() && L.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    L._set_fw_grad(L_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (U_new_fw_grad_opt.has_value() && U_new_fw_grad_opt.value().defined() && U.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    U._set_fw_grad(U_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->L_ = SavedVariable(L, true);
    grad_fn->P_ = SavedVariable(P, true);
    grad_fn->U_ = SavedVariable(U, true);
  }
  return std::make_tuple(std::move(P), std::move(L), std::move(U));
}
at::Tensor linalg_matrix_exp(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<LinalgMatrixExpBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinalgMatrixExpBackward0>(new LinalgMatrixExpBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::linalg_matrix_exp(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: linalg_matrix_exp");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: linalg_matrix_exp");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = linalg_matrix_exp_differential(self_p, self_t, /*adjoint*/ false);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor linear(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  std::shared_ptr<LinearBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LinearBackward0>(new LinearBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::linear", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("linear", *opt_op, ks, input, weight, bias);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::linear(ks & c10::after_autograd_keyset, input_, weight_, bias);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: linear");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: linear");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "linear");
  return result;
}
at::Tensor log10(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Log10Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Log10Backward0>(new Log10Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::log10(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: log10");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: log10");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() / (self_p.conj() * 2.3025850929940456)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & log10_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<Log10Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Log10Backward0>(new Log10Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::log10_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj() / (original_self_p.conj() * 2.3025850929940456)).conj()) : (original_self_t.conj() / (original_self_p.conj() * 2.3025850929940456)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor log1p(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Log1PBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Log1PBackward0>(new Log1PBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::log1p(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: log1p");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: log1p");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (log1p_backward(self_t.conj(), self_p)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & log1p_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("log1p");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("log1p");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::log1p_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with log1p_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & log_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  if (_any_has_forward_grad_self) {
    original_self = self.clone();
  }
  std::shared_ptr<LogBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LogBackward0>(new LogBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::log_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto original_self_t_raw = toNonOptFwGrad(original_self);
      auto original_self_tensor = toNonOptTensor(original_self);
      auto original_self_t = (original_self_t_raw.defined() || !original_self_tensor.defined())
        ? original_self_t_raw : at::zeros(original_self_tensor.sizes(), original_self_tensor.options());
      auto original_self_p = toNonOptPrimal(original_self);
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((original_self_t.conj().div(original_self_p.conj())).conj()) : (original_self_t.conj().div(original_self_p.conj())).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor logaddexp2(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Logaddexp2Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<Logaddexp2Backward0>(new Logaddexp2Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->other_ = SavedVariable(other, false);
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::logaddexp2(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: logaddexp2");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: logaddexp2");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "logaddexp2");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      result_new_fw_grad_opt = self_t / (1 + pow(2, other_p - self_p)) + other_t / (1 + pow(2, self_p - other_p));
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & logaddexp2_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("logaddexp2");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("logaddexp2");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::logaddexp2_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with logaddexp2_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & logical_and_(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::logical_and_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  return self;
}
at::Tensor logical_or(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::logical_or(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: logical_or");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: logical_or");
  #endif
  return result;
}
at::Tensor & logical_or_(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::logical_or_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  return self;
}
at::Tensor logsumexp(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<LogsumexpBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LogsumexpBackward0>(new LogsumexpBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim.vec();
    grad_fn->keepdim = keepdim;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::logsumexp(ks & c10::after_autograd_keyset, self_, dim, keepdim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: logsumexp");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: logsumexp");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "logsumexp");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = logsumexp_jvp(self_p, self_t, dim, keepdim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lu_unpack(c10::DispatchKeySet ks, const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots) {
  auto& LU_data_ = unpack(LU_data, "LU_data", 0);
  auto& LU_pivots_ = unpack(LU_pivots, "LU_pivots", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( LU_data );
  
  [[maybe_unused]] auto _any_has_forward_grad_L = (isFwGradDefined(LU_data));
  [[maybe_unused]] auto _any_has_forward_grad_U = (isFwGradDefined(LU_data));
  std::shared_ptr<LuUnpackBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<LuUnpackBackward0>(new LuUnpackBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( LU_data ));
    grad_fn->LU_data_sym_argsize_minus_1 = LU_data.sym_size(-1);
    grad_fn->LU_data_sym_argsize_minus_2 = LU_data.sym_size(-2);
  }
  at::Tensor P;
  at::Tensor L;
  at::Tensor U;
  #ifndef NDEBUG
  c10::optional<Storage> LU_data__storage_saved =
    LU_data_.has_storage() ? c10::optional<Storage>(LU_data_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> LU_data__impl_saved;
  if (LU_data_.defined()) LU_data__impl_saved = LU_data_.getIntrusivePtr();
  c10::optional<Storage> LU_pivots__storage_saved =
    LU_pivots_.has_storage() ? c10::optional<Storage>(LU_pivots_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> LU_pivots__impl_saved;
  if (LU_pivots_.defined()) LU_pivots__impl_saved = LU_pivots_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::lu_unpack(ks & c10::after_autograd_keyset, LU_data_, LU_pivots_, unpack_data, unpack_pivots);
  })();
  std::tie(P, L, U) = std::move(_tmp);
  #ifndef NDEBUG
  if (LU_data__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(LU_data_))
    TORCH_INTERNAL_ASSERT(LU_data__storage_saved.value().is_alias_of(LU_data_.storage()));
  if (LU_data__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU_data_))
    TORCH_INTERNAL_ASSERT(LU_data__impl_saved == LU_data_.getIntrusivePtr());
  if (LU_pivots__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(LU_pivots_))
    TORCH_INTERNAL_ASSERT(LU_pivots__storage_saved.value().is_alias_of(LU_pivots_.storage()));
  if (LU_pivots__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(LU_pivots_))
    TORCH_INTERNAL_ASSERT(LU_pivots__impl_saved == LU_pivots_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( L, U ), grad_fn);
  }
  c10::optional<at::Tensor> L_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_L && (L.defined())) {
      auto LU_data_t_raw = toNonOptFwGrad(LU_data);
      auto LU_data_tensor = toNonOptTensor(LU_data);
      auto LU_data_t = (LU_data_t_raw.defined() || !LU_data_tensor.defined())
        ? LU_data_t_raw : at::_efficientzerotensor(LU_data_tensor.sizes(), LU_data_tensor.options());
      L_new_fw_grad_opt = LU_data_t.sym_size(-2) >= LU_data_t.sym_size(-1) ? LU_data_t.tril(-1) : LU_data_t.narrow_symint(-1, 0, LU_data_t.sym_size(-2)).tril(-1);
  }
  c10::optional<at::Tensor> U_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_U && (U.defined())) {
      auto LU_data_t_raw = toNonOptFwGrad(LU_data);
      auto LU_data_tensor = toNonOptTensor(LU_data);
      auto LU_data_t = (LU_data_t_raw.defined() || !LU_data_tensor.defined())
        ? LU_data_t_raw : at::_efficientzerotensor(LU_data_tensor.sizes(), LU_data_tensor.options());
      U_new_fw_grad_opt = LU_data_t.sym_size(-1) >= LU_data_t.sym_size(-2) ? LU_data_t.triu() : LU_data_t.narrow_symint(-2, 0, LU_data_t.sym_size(-1)).triu();
  }
  if (L_new_fw_grad_opt.has_value() && L_new_fw_grad_opt.value().defined() && L.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    L._set_fw_grad(L_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  
  if (U_new_fw_grad_opt.has_value() && U_new_fw_grad_opt.value().defined() && U.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    U._set_fw_grad(U_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return std::make_tuple(std::move(P), std::move(L), std::move(U));
}
::std::tuple<at::Tensor,at::Tensor> max_dim(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<MaxBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaxBackward0>(new MaxBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  at::Tensor values;
  at::Tensor indices;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::max(ks & c10::after_autograd_keyset, self_, dim, keepdim);
  })();
  std::tie(values, indices) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values)) {
    TORCH_INTERNAL_ASSERT(values.storage().use_count() == 1, "function: max_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values))
    TORCH_INTERNAL_ASSERT(values.use_count() <= 1, "function: max_dim");
  if (indices.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices)) {
    TORCH_INTERNAL_ASSERT(indices.storage().use_count() == 1, "function: max_dim");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices))
    TORCH_INTERNAL_ASSERT(indices.use_count() <= 1, "function: max_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( values ), grad_fn);
  }
  throw_error_for_complex_autograd(values, "max");
  c10::optional<at::Tensor> values_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_values && (values.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      values_new_fw_grad_opt = gather_with_keepdimed_indices(self_t, dim, indices, keepdim);
  }
  if (values_new_fw_grad_opt.has_value() && values_new_fw_grad_opt.value().defined() && values.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    values._set_fw_grad(values_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->indices_ = SavedVariable(indices, true);
  }
  return std::make_tuple(std::move(values), std::move(indices));
}
at::Tensor max(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<MaxBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaxBackward1>(new MaxBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::max(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: max");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: max");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "max");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = evenly_read_jvp(self_t, self_p, result);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor> max_pool2d_with_indices(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result0 = (isFwGradDefined(self));
  std::shared_ptr<MaxPool2DWithIndicesBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MaxPool2DWithIndicesBackward0>(new MaxPool2DWithIndicesBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->ceil_mode = ceil_mode;
    grad_fn->dilation = dilation.vec();
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::max_pool2d_with_indices(ks & c10::after_autograd_keyset, self_, kernel_size, stride, padding, dilation, ceil_mode);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: max_pool2d_with_indices");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: max_pool2d_with_indices");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: max_pool2d_with_indices");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: max_pool2d_with_indices");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "max_pool2d_with_indices");
  c10::optional<at::Tensor> result0_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result0 && (result0.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result0_new_fw_grad_opt = gather(self_t.flatten(-2), -1, result1.flatten(-2)).view_as(result1);
  }
  if (result0_new_fw_grad_opt.has_value() && result0_new_fw_grad_opt.value().defined() && result0.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result0._set_fw_grad(result0_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor & max_unpool3d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& indices_ = unpack(indices, "indices", 1);
  auto& out_ = unpack(out, "out", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("max_unpool3d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("max_unpool3d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> indices__storage_saved =
    indices_.has_storage() ? c10::optional<Storage>(indices_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> indices__impl_saved;
  if (indices_.defined()) indices__impl_saved = indices_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::max_unpool3d_symint_outf(ks & c10::after_autograd_keyset, self_, indices_, output_size, stride, padding, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (indices__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__storage_saved.value().is_alias_of(indices_.storage()));
  if (indices__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices_))
    TORCH_INTERNAL_ASSERT(indices__impl_saved == indices_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with max_unpool3d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & maximum_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("maximum");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("maximum");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::maximum_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with maximum_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & minimum_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("minimum");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("minimum");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::minimum_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with minimum_out that does not support it because it is an out= function");
  return out;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_batch_norm_backward(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon) {
  auto& input_ = unpack(input, "input", 0);
  auto& grad_output_ = unpack(grad_output, "grad_output", 1);
  auto& weight_ = unpack(weight, "weight", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( input, grad_output, weight, save_mean, save_var );
  
  check_no_requires_grad(running_mean, "running_mean", "miopen_batch_norm_backward");
  check_no_requires_grad(running_var, "running_var", "miopen_batch_norm_backward");
  std::shared_ptr<MiopenBatchNormBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MiopenBatchNormBackwardBackward0>(new MiopenBatchNormBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, grad_output, weight, save_mean, save_var ));
    grad_fn->epsilon = epsilon;
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->running_mean_ = SavedVariable(running_mean, false);
    grad_fn->running_var_ = SavedVariable(running_var, false);
    grad_fn->save_mean_ = SavedVariable(save_mean, false);
    grad_fn->save_var_ = SavedVariable(save_var, false);
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(input) || isFwGradDefined(grad_output) || isFwGradDefined(weight) || isFwGradDefined(running_mean) || isFwGradDefined(running_var) || isFwGradDefined(save_mean) || isFwGradDefined(save_var))) {
      static c10::OperatorName full_name("aten::miopen_batch_norm_backward", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<std::tuple<at::Tensor, at::Tensor, at::Tensor>>("miopen_batch_norm_backward", *opt_op, ks, input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::miopen_batch_norm_backward(ks & c10::after_autograd_keyset, input_, grad_output_, weight_, running_mean, running_var, save_mean, save_var, epsilon);
    }
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))
    TORCH_INTERNAL_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {
    TORCH_INTERNAL_ASSERT(result0.storage().use_count() == 1, "function: miopen_batch_norm_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))
    TORCH_INTERNAL_ASSERT(result0.use_count() <= 1, "function: miopen_batch_norm_backward");
  if (result1.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1)) {
    TORCH_INTERNAL_ASSERT(result1.storage().use_count() == 1, "function: miopen_batch_norm_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result1))
    TORCH_INTERNAL_ASSERT(result1.use_count() <= 1, "function: miopen_batch_norm_backward");
  if (result2.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2)) {
    TORCH_INTERNAL_ASSERT(result2.storage().use_count() == 1, "function: miopen_batch_norm_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result2))
    TORCH_INTERNAL_ASSERT(result2.use_count() <= 1, "function: miopen_batch_norm_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "miopen_batch_norm_backward");
  throw_error_for_complex_autograd(result1, "miopen_batch_norm_backward");
  throw_error_for_complex_autograd(result2, "miopen_batch_norm_backward");
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
at::Tensor mkldnn_convolution(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<MkldnnConvolutionBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MkldnnConvolutionBackward0>(new MkldnnConvolutionBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight, bias ));
    grad_fn->bias_sym_sizes_opt = bias.has_value() ? c10::optional<c10::SymIntArrayRef>(bias->sym_sizes()) : c10::nullopt;
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->stride = stride.vec();
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::mkldnn_convolution", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("mkldnn_convolution", *opt_op, ks, self, weight, bias, padding, stride, dilation, groups);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::mkldnn_convolution_symint(ks & c10::after_autograd_keyset, self_, weight_, bias, padding, stride, dilation, groups);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mkldnn_convolution");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mkldnn_convolution");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "mkldnn_convolution");
  return result;
}
at::Tensor mkldnn_linear(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
  auto& self_ = unpack(self, "self", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, weight, bias );
  
  std::shared_ptr<MkldnnLinearBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MkldnnLinearBackward0>(new MkldnnLinearBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, weight, bias ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(bias))) {
      static c10::OperatorName full_name("aten::mkldnn_linear", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("mkldnn_linear", *opt_op, ks, self, weight, bias);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::mkldnn_linear(ks & c10::after_autograd_keyset, self_, weight_, bias);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (weight__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(weight_))
    TORCH_INTERNAL_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mkldnn_linear");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mkldnn_linear");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "mkldnn_linear");
  return result;
}
::std::tuple<at::Tensor,at::Tensor> mode(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_values = (isFwGradDefined(self));
  std::shared_ptr<ModeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ModeBackward0>(new ModeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  at::Tensor values;
  at::Tensor indices;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::mode(ks & c10::after_autograd_keyset, self_, dim, keepdim);
  })();
  std::tie(values, indices) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values)) {
    TORCH_INTERNAL_ASSERT(values.storage().use_count() == 1, "function: mode");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(values))
    TORCH_INTERNAL_ASSERT(values.use_count() <= 1, "function: mode");
  if (indices.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices)) {
    TORCH_INTERNAL_ASSERT(indices.storage().use_count() == 1, "function: mode");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(indices))
    TORCH_INTERNAL_ASSERT(indices.use_count() <= 1, "function: mode");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( values ), grad_fn);
  }
  throw_error_for_complex_autograd(values, "mode");
  c10::optional<at::Tensor> values_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_values && (values.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      values_new_fw_grad_opt = gather_with_keepdimed_indices(self_t, dim, indices, keepdim);
  }
  if (values_new_fw_grad_opt.has_value() && values_new_fw_grad_opt.value().defined() && values.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    values._set_fw_grad(values_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->indices_ = SavedVariable(indices, true);
  }
  return std::make_tuple(std::move(values), std::move(indices));
}
at::Tensor mse_loss_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& target_ = unpack(target, "target", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self, target );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target));
  std::shared_ptr<MseLossBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MseLossBackwardBackward0>(new MseLossBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self, target ));
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->reduction = reduction;
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->target_ = SavedVariable(target, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::mse_loss_backward(ks & c10::after_autograd_keyset, grad_output_, self_, target_, reduction);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mse_loss_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mse_loss_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "mse_loss_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto target_t_raw = toNonOptFwGrad(target);
      auto target_tensor = toNonOptTensor(target);
      auto target_t = (target_t_raw.defined() || !target_tensor.defined())
        ? target_t_raw : at::_efficientzerotensor(target_tensor.sizes(), target_tensor.options());
      auto target_p = toNonOptPrimal(target);
      result_new_fw_grad_opt =   mse_loss_double_backward(self_t * grad_output_p, self_p, reduction) - mse_loss_double_backward(target_t * grad_output_p, target_p, reduction) + mse_loss_backward(grad_output_t, self_p, target_p, reduction) ;
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & mse_loss_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& target_ = unpack(target, "target", 2);
  auto& grad_input_ = unpack(grad_input, "grad_input", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self, target );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self, target )) {
    throw_error_out_requires_grad("mse_loss_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("mse_loss_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::mse_loss_backward_outf(ks & c10::after_autograd_keyset, grad_output_, self_, target_, reduction, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(target) || isFwGradDefined(grad_input))), "Trying to use forward AD with mse_loss_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor mul_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<MulBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MulBackward0>(new MulBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    if (grad_fn->should_compute_output(0)) {
      grad_fn->other_ = SavedVariable(other, false);
    }
    grad_fn->other_scalar_type = other.scalar_type();
    if (grad_fn->should_compute_output(1)) {
      grad_fn->self_ = SavedVariable(self, false);
    }
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::mul(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mul_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mul_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      result_new_fw_grad_opt = other_t * self_p + self_t * other_p;
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor mul_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<MulBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MulBackward1>(new MulBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->other = other;
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::mul(ks & c10::after_autograd_keyset, self_, other);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mul_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mul_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = self_t * other;
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & multi_margin_loss_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& target_ = unpack(target, "target", 1);
  auto& out_ = unpack(out, "out", 6);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, weight )) {
    throw_error_out_requires_grad("multi_margin_loss");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("multi_margin_loss");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> target__storage_saved =
    target_.has_storage() ? c10::optional<Storage>(target_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> target__impl_saved;
  if (target_.defined()) target__impl_saved = target_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::multi_margin_loss_outf(ks & c10::after_autograd_keyset, self_, target_, p, margin, weight, reduction, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (target__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__storage_saved.value().is_alias_of(target_.storage()));
  if (target__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(target_))
    TORCH_INTERNAL_ASSERT(target__impl_saved == target_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(weight) || isFwGradDefined(out))), "Trying to use forward AD with multi_margin_loss_out that does not support it because it is an out= function");
  return out;
}
at::Tensor multinomial(c10::DispatchKeySet ks, const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::multinomial(ks & c10::after_autograd_keyset, self_, num_samples, replacement, generator);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: multinomial");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: multinomial");
  #endif
  return result;
}
at::Tensor & multinomial_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::multinomial_outf(ks & c10::after_autograd_keyset, self_, num_samples, replacement, generator, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with multinomial_out that does not support it because it is an out= function");
  return out;
}
at::Tensor mvlgamma(c10::DispatchKeySet ks, const at::Tensor & self, int64_t p) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<MvlgammaBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<MvlgammaBackward0>(new MvlgammaBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->p = p;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::mvlgamma(ks & c10::after_autograd_keyset, self_, p);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: mvlgamma");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: mvlgamma");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "mvlgamma");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (mvlgamma_backward(self_t.conj(), self_p, p)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & mvlgamma_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t p, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("mvlgamma");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("mvlgamma");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::mvlgamma_outf(ks & c10::after_autograd_keyset, self_, p, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with mvlgamma_out that does not support it because it is an out= function");
  return out;
}
at::Tensor neg(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<NegBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NegBackward0>(new NegBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::neg(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: neg");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: neg");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (self_t.conj().neg()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor new_empty(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::new_empty_symint(ks & c10::after_autograd_keyset, self_, size, dtype, layout, device, pin_memory);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: new_empty");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: new_empty");
  #endif
  return result;
}
at::Tensor & new_zeros_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::new_zeros_symint_outf(ks & c10::after_autograd_keyset, self_, size, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with new_zeros_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & nextafter_(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<NextafterBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NextafterBackward0>(new NextafterBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::nextafter_(ks & c10::after_autograd_keyset, self_, other_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other))), "Trying to use forward AD with nextafter_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
  return self;
}
at::Tensor nonzero(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::nonzero(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: nonzero");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: nonzero");
  #endif
  return result;
}
at::Tensor & normal_(c10::DispatchKeySet ks, at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<NormalBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NormalBackward0>(new NormalBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::normal_(ks & c10::after_autograd_keyset, self_, mean, std, generator);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = self_t.zero_();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
at::Tensor pixel_unshuffle(c10::DispatchKeySet ks, const at::Tensor & self, int64_t downscale_factor) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<PixelUnshuffleBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PixelUnshuffleBackward0>(new PixelUnshuffleBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->downscale_factor = downscale_factor;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::pixel_unshuffle(ks & c10::after_autograd_keyset, self_, downscale_factor);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: pixel_unshuffle");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: pixel_unshuffle");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::pixel_unshuffle(self_t, downscale_factor);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor pow_Tensor_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & exponent) {
  auto& self_ = unpack(self, "self", 0);
  auto& exponent_ = unpack(exponent, "exponent", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, exponent );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(exponent));
  std::shared_ptr<PowBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PowBackward1>(new PowBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, exponent ));
    grad_fn->exponent_ = SavedVariable(exponent, false);
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> exponent__storage_saved =
    exponent_.has_storage() ? c10::optional<Storage>(exponent_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> exponent__impl_saved;
  if (exponent_.defined()) exponent__impl_saved = exponent_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::pow(ks & c10::after_autograd_keyset, self_, exponent_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (exponent__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(exponent_))
    TORCH_INTERNAL_ASSERT(exponent__storage_saved.value().is_alias_of(exponent_.storage()));
  if (exponent__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(exponent_))
    TORCH_INTERNAL_ASSERT(exponent__impl_saved == exponent_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: pow_Tensor_Tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: pow_Tensor_Tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto exponent_t_raw = toNonOptFwGrad(exponent);
      auto exponent_tensor = toNonOptTensor(exponent);
      auto exponent_t = (exponent_t_raw.defined() || !exponent_tensor.defined())
        ? exponent_t_raw : at::_efficientzerotensor(exponent_tensor.sizes(), exponent_tensor.options());
      auto exponent_p = toNonOptPrimal(exponent);
      result_new_fw_grad_opt = (pow_backward_self(self_t.conj(), self_p, exponent_p) + pow_backward_exponent(exponent_t.conj(), self_p, exponent_p, result)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor pow_Scalar(c10::DispatchKeySet ks, const at::Scalar & self, const at::Tensor & exponent) {
  auto& exponent_ = unpack(exponent, "exponent", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( exponent );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(exponent));
  std::shared_ptr<PowBackward2> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PowBackward2>(new PowBackward2(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( exponent ));
    grad_fn->exponent_ = SavedVariable(exponent, false);
    grad_fn->self = self;
  }
  #ifndef NDEBUG
  c10::optional<Storage> exponent__storage_saved =
    exponent_.has_storage() ? c10::optional<Storage>(exponent_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> exponent__impl_saved;
  if (exponent_.defined()) exponent__impl_saved = exponent_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::pow(ks & c10::after_autograd_keyset, self, exponent_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (exponent__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(exponent_))
    TORCH_INTERNAL_ASSERT(exponent__storage_saved.value().is_alias_of(exponent_.storage()));
  if (exponent__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(exponent_))
    TORCH_INTERNAL_ASSERT(exponent__impl_saved == exponent_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: pow_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: pow_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto exponent_t_raw = toNonOptFwGrad(exponent);
      auto exponent_tensor = toNonOptTensor(exponent);
      auto exponent_t = (exponent_t_raw.defined() || !exponent_tensor.defined())
        ? exponent_t_raw : at::_efficientzerotensor(exponent_tensor.sizes(), exponent_tensor.options());
      auto exponent_p = toNonOptPrimal(exponent);
      result_new_fw_grad_opt = (pow_backward_exponent(exponent_t.conj(), self, exponent_p, result)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor pow_Tensor_Scalar(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & exponent) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<PowBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<PowBackward0>(new PowBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->exponent = exponent;
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::pow(ks & c10::after_autograd_keyset, self_, exponent);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: pow_Tensor_Scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: pow_Tensor_Scalar");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (pow_backward(self_t.conj(), self_p, exponent)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & prod_out_int_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("prod");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("prod");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::prod_outf(ks & c10::after_autograd_keyset, self_, dim, keepdim, dtype, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with prod_out that does not support it because it is an out= function");
  return out;
}
at::QScheme qscheme(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::qscheme(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = _tmp;
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  return result;
}
at::Tensor & quantize_per_tensor_out_out(c10::DispatchKeySet ks, const at::Tensor & self, double scale, int64_t zero_point, at::ScalarType dtype, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::quantize_per_tensor_outf(ks & c10::after_autograd_keyset, self_, scale, zero_point, dtype, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with quantize_per_tensor_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & quantize_per_tensor_out_tensor_qparams_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, at::ScalarType dtype, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& scale_ = unpack(scale, "scale", 1);
  auto& zero_point_ = unpack(zero_point, "zero_point", 2);
  auto& out_ = unpack(out, "out", 4);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> scale__storage_saved =
    scale_.has_storage() ? c10::optional<Storage>(scale_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> scale__impl_saved;
  if (scale_.defined()) scale__impl_saved = scale_.getIntrusivePtr();
  c10::optional<Storage> zero_point__storage_saved =
    zero_point_.has_storage() ? c10::optional<Storage>(zero_point_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> zero_point__impl_saved;
  if (zero_point_.defined()) zero_point__impl_saved = zero_point_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::quantize_per_tensor_outf(ks & c10::after_autograd_keyset, self_, scale_, zero_point_, dtype, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (scale__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(scale_))
    TORCH_INTERNAL_ASSERT(scale__storage_saved.value().is_alias_of(scale_.storage()));
  if (scale__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(scale_))
    TORCH_INTERNAL_ASSERT(scale__impl_saved == scale_.getIntrusivePtr());
  if (zero_point__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(zero_point_))
    TORCH_INTERNAL_ASSERT(zero_point__storage_saved.value().is_alias_of(zero_point_.storage()));
  if (zero_point__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(zero_point_))
    TORCH_INTERNAL_ASSERT(zero_point__impl_saved == zero_point_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(scale) || isFwGradDefined(zero_point) || isFwGradDefined(out))), "Trying to use forward AD with quantize_per_tensor_out that does not support it because it is an out= function");
  return out;
}
void quantize_per_tensor_out_tensors_out(c10::DispatchKeySet ks, at::TensorList tensors, const at::Tensor & scales, const at::Tensor & zero_points, at::ScalarType dtype, at::TensorList out) {
  auto tensors_ = unpack(tensors, "tensors", 0);
  auto& scales_ = unpack(scales, "scales", 1);
  auto& zero_points_ = unpack(zero_points, "zero_points", 2);
  auto out_ = unpack(out, "out", 4);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> tensors__storage_saved(tensors_.size());
  for (const Tensor& tensor : tensors_)
    tensors__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensors__impl_saved(tensors_.size());
  for (size_t i=0; i<tensors_.size(); i++)
    if (tensors_[i].defined()) tensors__impl_saved[i] = tensors_[i].getIntrusivePtr();
  c10::optional<Storage> scales__storage_saved =
    scales_.has_storage() ? c10::optional<Storage>(scales_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> scales__impl_saved;
  if (scales_.defined()) scales__impl_saved = scales_.getIntrusivePtr();
  c10::optional<Storage> zero_points__storage_saved =
    zero_points_.has_storage() ? c10::optional<Storage>(zero_points_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> zero_points__impl_saved;
  if (zero_points_.defined()) zero_points__impl_saved = zero_points_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::quantize_per_tensor_outf(ks & c10::after_autograd_keyset, tensors_, scales_, zero_points_, dtype, out_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<tensors_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensors_))
      TORCH_INTERNAL_ASSERT(tensors__storage_saved[i].value().is_alias_of(tensors_[i].storage()));
  }
  for (size_t i=0; i<tensors_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensors__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensors_))
      TORCH_INTERNAL_ASSERT(tensors__impl_saved[i] == tensors_[i].getIntrusivePtr());
  }
  if (scales__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(scales_))
    TORCH_INTERNAL_ASSERT(scales__storage_saved.value().is_alias_of(scales_.storage()));
  if (scales__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(scales_))
    TORCH_INTERNAL_ASSERT(scales__impl_saved == scales_.getIntrusivePtr());
  if (zero_points__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(zero_points_))
    TORCH_INTERNAL_ASSERT(zero_points__storage_saved.value().is_alias_of(zero_points_.storage()));
  if (zero_points__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(zero_points_))
    TORCH_INTERNAL_ASSERT(zero_points__impl_saved == zero_points_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefinedTensorList(tensors) || isFwGradDefined(scales) || isFwGradDefined(zero_points) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with quantize_per_tensor_out that does not support it because it is an out= function");
}
at::Tensor & rad2deg_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("rad2deg");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("rad2deg");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::rad2deg_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with rad2deg_out that does not support it because it is an out= function");
  return out;
}
at::Tensor reciprocal(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ReciprocalBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReciprocalBackward0>(new ReciprocalBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::reciprocal(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: reciprocal");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: reciprocal");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (-self_t.conj() * (result * result).conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor reflection_pad1d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef padding) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ReflectionPad1DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ReflectionPad1DBackward0>(new ReflectionPad1DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->padding = padding.vec();
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::reflection_pad1d_symint(ks & c10::after_autograd_keyset, self_, padding);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: reflection_pad1d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: reflection_pad1d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::reflection_pad1d_symint(self_t, padding);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & reflection_pad2d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("reflection_pad2d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("reflection_pad2d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::reflection_pad2d_symint_outf(ks & c10::after_autograd_keyset, self_, padding, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with reflection_pad2d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & reflection_pad3d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& grad_input_ = unpack(grad_input, "grad_input", 3);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output, self )) {
    throw_error_out_requires_grad("reflection_pad3d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("reflection_pad3d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::reflection_pad3d_backward_symint_outf(ks & c10::after_autograd_keyset, grad_output_, self_, padding, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(self) || isFwGradDefined(grad_input))), "Trying to use forward AD with reflection_pad3d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor & remainder_out_Scalar_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("remainder");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("remainder");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::remainder_outf(ks & c10::after_autograd_keyset, self_, other, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with remainder_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & remainder_out_Tensor_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("remainder");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("remainder");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::remainder_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with remainder_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & replication_pad2d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("replication_pad2d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("replication_pad2d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::replication_pad2d_symint_outf(ks & c10::after_autograd_keyset, self_, padding, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with replication_pad2d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor row_indices(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::row_indices(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: row_indices");
  #endif
  return result;
}
at::Tensor rrelu_with_noise_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, bool self_is_result) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& noise_ = unpack(noise, "noise", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  check_no_requires_grad(noise, "noise", "rrelu_with_noise_backward");
  std::shared_ptr<RreluWithNoiseBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<RreluWithNoiseBackwardBackward0>(new RreluWithNoiseBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->lower = lower;
    grad_fn->noise_ = SavedVariable(noise, false);
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->training = training;
    grad_fn->upper = upper;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> noise__storage_saved =
    noise_.has_storage() ? c10::optional<Storage>(noise_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> noise__impl_saved;
  if (noise_.defined()) noise__impl_saved = noise_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::rrelu_with_noise_backward(ks & c10::after_autograd_keyset, grad_output_, self_, noise_, lower, upper, training, self_is_result);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (noise__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(noise_))
    TORCH_INTERNAL_ASSERT(noise__storage_saved.value().is_alias_of(noise_.storage()));
  if (noise__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(noise_))
    TORCH_INTERNAL_ASSERT(noise__impl_saved == noise_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: rrelu_with_noise_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: rrelu_with_noise_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "rrelu_with_noise_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = rrelu_with_noise_backward(grad_output_t, self_p, noise, lower, upper, training, false);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor segment_reduce(c10::DispatchKeySet ks, const at::Tensor & data, c10::string_view reduce, const c10::optional<at::Tensor> & lengths, const c10::optional<at::Tensor> & indices, const c10::optional<at::Tensor> & offsets, int64_t axis, bool unsafe, const c10::optional<at::Scalar> & initial) {
  auto& data_ = unpack(data, "data", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( data );
  
  check_no_requires_grad(lengths, "lengths", "segment_reduce");
  check_no_requires_grad(indices, "indices", "segment_reduce");
  check_no_requires_grad(offsets, "offsets", "segment_reduce");
  std::shared_ptr<SegmentReduceBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SegmentReduceBackward0>(new SegmentReduceBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( data ));
    grad_fn->axis = axis;
    grad_fn->data_ = SavedVariable(data, false);
    grad_fn->initial = initial;
    grad_fn->lengths_ = SavedVariable(lengths, false);
    grad_fn->offsets_ = SavedVariable(offsets, false);
    grad_fn->reduce = std::string(reduce);
  }
  #ifndef NDEBUG
  c10::optional<Storage> data__storage_saved =
    data_.has_storage() ? c10::optional<Storage>(data_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> data__impl_saved;
  if (data_.defined()) data__impl_saved = data_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(data) || isFwGradDefined(lengths) || isFwGradDefined(indices) || isFwGradDefined(offsets))) {
      static c10::OperatorName full_name("aten::segment_reduce", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("segment_reduce", *opt_op, ks, data, reduce, lengths, indices, offsets, axis, unsafe, initial);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::segment_reduce(ks & c10::after_autograd_keyset, data_, reduce, lengths, indices, offsets, axis, unsafe, initial);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (data__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(data_))
    TORCH_INTERNAL_ASSERT(data__storage_saved.value().is_alias_of(data_.storage()));
  if (data__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(data_))
    TORCH_INTERNAL_ASSERT(data__impl_saved == data_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: segment_reduce");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: segment_reduce");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "segment_reduce");
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & set__source_Storage(c10::DispatchKeySet ks, at::Tensor & self, at::Storage source) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<NotImplemented> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("set_"), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::set_(ks & c10::after_autograd_keyset, self_, source);
  }
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self))), "Trying to use forward AD with set_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
  reset_grad_accumulator(self);
  return self;
}
at::Tensor & set__source_Storage_storage_offset(c10::DispatchKeySet ks, at::Tensor & self, at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<NotImplemented> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("set_"), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::set__symint(ks & c10::after_autograd_keyset, self_, source, storage_offset, size, stride);
  }
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self))), "Trying to use forward AD with set_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
  reset_grad_accumulator(self);
  return self;
}
at::Tensor & set__source_Tensor(c10::DispatchKeySet ks, at::Tensor & self, const at::Tensor & source) {
  auto& self_ = unpack(self, "self", 0);
  auto& source_ = unpack(source, "source", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, source );
  
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<NotImplemented> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("set_"), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, source ));
  }
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::set_(ks & c10::after_autograd_keyset, self_, source_);
  }
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(source))), "Trying to use forward AD with set_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
  reset_grad_accumulator(self);
  return self;
}
at::Tensor & set_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<NotImplemented> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NotImplemented>(new NotImplemented("set_"), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::set_(ks & c10::after_autograd_keyset, self_);
  }
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self))), "Trying to use forward AD with set_ that does not support it because it has not been implemented yet.\nPlease file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.");
  reset_grad_accumulator(self);
  return self;
}
at::Tensor sgn(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SgnBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SgnBackward0>(new SgnBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::sgn(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sgn");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sgn");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = sgn_backward(self_p, self_t, result);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & sigmoid_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<SigmoidBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SigmoidBackward0>(new SigmoidBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::sigmoid_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((sigmoid_backward(self_t.conj(), self_p)).conj()) : (sigmoid_backward(self_t.conj(), self_p)).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor sigmoid_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & output) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& output_ = unpack(output, "output", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(output));
  std::shared_ptr<SigmoidBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SigmoidBackwardBackward0>(new SigmoidBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, output ));
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->output_ = SavedVariable(output, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::sigmoid_backward(ks & c10::after_autograd_keyset, grad_output_, output_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sigmoid_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sigmoid_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto output_t_raw = toNonOptFwGrad(output);
      auto output_tensor = toNonOptTensor(output);
      auto output_t = (output_t_raw.defined() || !output_tensor.defined())
        ? output_t_raw : at::_efficientzerotensor(output_tensor.sizes(), output_tensor.options());
      auto output_p = toNonOptPrimal(output);
      result_new_fw_grad_opt = sigmoid_backward(grad_output_t, output_p) + output_t.conj() * grad_output_p * (-2 * output_p.conj() + 1);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & signbit_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::signbit_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with signbit_out that does not support it because it is an out= function");
  return out;
}
at::Tensor sin(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SinBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SinBackward0>(new SinBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::sin(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sin");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sin");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() * self_p.cos().conj()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & sin_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("sin");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("sin");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::sin_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with sin_out that does not support it because it is an out= function");
  return out;
}
at::Tensor softplus_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<SoftplusBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SoftplusBackwardBackward0>(new SoftplusBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->beta = beta;
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->threshold = threshold;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::softplus_backward(ks & c10::after_autograd_keyset, grad_output_, self_, beta, threshold);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: softplus_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: softplus_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "softplus_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = softplus_backward(grad_output_t, self_p, beta, threshold) + softplus_double_backward(self_t * grad_output_p, self_p, beta, threshold);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor sparse_mask(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mask) {
  auto& self_ = unpack(self, "self", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<SparseMaskBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SparseMaskBackward0>(new SparseMaskBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->mask_ = SavedVariable(mask, false);
    grad_fn->self_layout = self.layout();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::sparse_mask", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("sparse_mask", *opt_op, ks, self, mask);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::sparse_mask(ks & c10::after_autograd_keyset, self_, mask_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mask__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(mask_))
    TORCH_INTERNAL_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: sparse_mask");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: sparse_mask");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  return result;
}
at::Tensor special_bessel_j0(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_bessel_j0(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_bessel_j0");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_bessel_j0");
  #endif
  return result;
}
at::Tensor special_bessel_y0(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_bessel_y0(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_bessel_y0");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_bessel_y0");
  #endif
  return result;
}
at::Tensor special_chebyshev_polynomial_u(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_chebyshev_polynomial_u(ks & c10::after_autograd_keyset, x_, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_chebyshev_polynomial_u");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_chebyshev_polynomial_u");
  #endif
  return result;
}
at::Tensor special_chebyshev_polynomial_u_x_scalar(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n) {
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_chebyshev_polynomial_u(ks & c10::after_autograd_keyset, x, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_chebyshev_polynomial_u_x_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_chebyshev_polynomial_u_x_scalar");
  #endif
  return result;
}
at::Tensor special_chebyshev_polynomial_u_n_scalar(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n) {
  auto& x_ = unpack(x, "x", 0);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_chebyshev_polynomial_u(ks & c10::after_autograd_keyset, x_, n);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_chebyshev_polynomial_u_n_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_chebyshev_polynomial_u_n_scalar");
  #endif
  return result;
}
at::Tensor special_entr(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SpecialEntrBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SpecialEntrBackward0>(new SpecialEntrBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_entr(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_entr");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_entr");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "special_entr");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (self_t.conj() * (-(1 + self_p.log()))).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor special_hermite_polynomial_he(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_hermite_polynomial_he(ks & c10::after_autograd_keyset, x_, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_hermite_polynomial_he");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_hermite_polynomial_he");
  #endif
  return result;
}
at::Tensor special_hermite_polynomial_he_x_scalar(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n) {
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_hermite_polynomial_he(ks & c10::after_autograd_keyset, x, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_hermite_polynomial_he_x_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_hermite_polynomial_he_x_scalar");
  #endif
  return result;
}
at::Tensor special_hermite_polynomial_he_n_scalar(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n) {
  auto& x_ = unpack(x, "x", 0);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_hermite_polynomial_he(ks & c10::after_autograd_keyset, x_, n);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_hermite_polynomial_he_n_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_hermite_polynomial_he_n_scalar");
  #endif
  return result;
}
at::Tensor & special_i0e_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("special_i0e");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("special_i0e");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_i0e_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with special_i0e_out that does not support it because it is an out= function");
  return out;
}
at::Tensor special_i1e(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SpecialI1EBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SpecialI1EBackward0>(new SpecialI1EBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_i1e(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_i1e");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_i1e");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "special_i1e");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = (i1e_backward(self_t.conj(), self_p, result)).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor special_modified_bessel_i1(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_modified_bessel_i1(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_modified_bessel_i1");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_modified_bessel_i1");
  #endif
  return result;
}
at::Tensor & special_modified_bessel_k0_out_out(c10::DispatchKeySet ks, const at::Tensor & self, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_modified_bessel_k0_outf(ks & c10::after_autograd_keyset, self_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_modified_bessel_k0_out that does not support it because it is an out= function");
  return out;
}
at::Tensor special_ndtri(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SpecialNdtriBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SpecialNdtriBackward0>(new SpecialNdtriBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_ndtri(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_ndtri");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_ndtri");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "special_ndtri");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = (self_t.conj() * std::sqrt(2 * M_PI) * (result.square() / 2).exp()).conj();
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & special_scaled_modified_bessel_k0_out_out(c10::DispatchKeySet ks, const at::Tensor & x, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& out_ = unpack(out, "out", 1);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_scaled_modified_bessel_k0_outf(ks & c10::after_autograd_keyset, x_, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_scaled_modified_bessel_k0_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_t_out_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_t_outf(ks & c10::after_autograd_keyset, x_, n_, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_t_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_t_out_x_scalar_out(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_t_outf(ks & c10::after_autograd_keyset, x, n_, out_);
  }
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_t_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_t_out_n_scalar_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_t_outf(ks & c10::after_autograd_keyset, x_, n, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_t_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_u_out_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_u_outf(ks & c10::after_autograd_keyset, x_, n_, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_u_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_u_out_x_scalar_out(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
  auto& n_ = unpack(n, "n", 1);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_u_outf(ks & c10::after_autograd_keyset, x, n_, out_);
  }
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_u_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & special_shifted_chebyshev_polynomial_u_out_n_scalar_out(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
  auto& x_ = unpack(x, "x", 0);
  auto& out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::special_shifted_chebyshev_polynomial_u_outf(ks & c10::after_autograd_keyset, x_, n, out_);
  }
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(out))), "Trying to use forward AD with special_shifted_chebyshev_polynomial_u_out that does not support it because it is an out= function");
  return out;
}
at::Tensor special_shifted_chebyshev_polynomial_v(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_shifted_chebyshev_polynomial_v(ks & c10::after_autograd_keyset, x_, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_shifted_chebyshev_polynomial_v");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_shifted_chebyshev_polynomial_v");
  #endif
  return result;
}
at::Tensor special_shifted_chebyshev_polynomial_v_x_scalar(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n) {
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_shifted_chebyshev_polynomial_v(ks & c10::after_autograd_keyset, x, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_shifted_chebyshev_polynomial_v_x_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_shifted_chebyshev_polynomial_v_x_scalar");
  #endif
  return result;
}
at::Tensor special_shifted_chebyshev_polynomial_v_n_scalar(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n) {
  auto& x_ = unpack(x, "x", 0);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_shifted_chebyshev_polynomial_v(ks & c10::after_autograd_keyset, x_, n);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_shifted_chebyshev_polynomial_v_n_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_shifted_chebyshev_polynomial_v_n_scalar");
  #endif
  return result;
}
at::Tensor special_shifted_chebyshev_polynomial_w(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & n) {
  auto& x_ = unpack(x, "x", 0);
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_shifted_chebyshev_polynomial_w(ks & c10::after_autograd_keyset, x_, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_shifted_chebyshev_polynomial_w");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_shifted_chebyshev_polynomial_w");
  #endif
  return result;
}
at::Tensor special_shifted_chebyshev_polynomial_w_x_scalar(c10::DispatchKeySet ks, const at::Scalar & x, const at::Tensor & n) {
  auto& n_ = unpack(n, "n", 1);
  #ifndef NDEBUG
  c10::optional<Storage> n__storage_saved =
    n_.has_storage() ? c10::optional<Storage>(n_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> n__impl_saved;
  if (n_.defined()) n__impl_saved = n_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_shifted_chebyshev_polynomial_w(ks & c10::after_autograd_keyset, x, n_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (n__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__storage_saved.value().is_alias_of(n_.storage()));
  if (n__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(n_))
    TORCH_INTERNAL_ASSERT(n__impl_saved == n_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_shifted_chebyshev_polynomial_w_x_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_shifted_chebyshev_polynomial_w_x_scalar");
  #endif
  return result;
}
at::Tensor special_shifted_chebyshev_polynomial_w_n_scalar(c10::DispatchKeySet ks, const at::Tensor & x, const at::Scalar & n) {
  auto& x_ = unpack(x, "x", 0);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_shifted_chebyshev_polynomial_w(ks & c10::after_autograd_keyset, x_, n);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_shifted_chebyshev_polynomial_w_n_scalar");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_shifted_chebyshev_polynomial_w_n_scalar");
  #endif
  return result;
}
at::Tensor special_spherical_bessel_j0(c10::DispatchKeySet ks, const at::Tensor & x) {
  auto& x_ = unpack(x, "x", 0);
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::special_spherical_bessel_j0(ks & c10::after_autograd_keyset, x_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(x_))
    TORCH_INTERNAL_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: special_spherical_bessel_j0");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: special_spherical_bessel_j0");
  #endif
  return result;
}
::std::vector<at::Tensor> split_Tensor(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymInt split_size, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SplitBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SplitBackward0>(new SplitBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_options = self.options();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
    grad_fn->split_size = split_size;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::split_symint(ks & c10::after_autograd_keyset, self_, split_size, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<::std::vector<at::Tensor>> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::split_symint(self_t, split_size, dim);
  }
  if (result_new_fw_grad_opt.has_value()) {
    auto result_new_fw_grad = result_new_fw_grad_opt.value();
    TORCH_INTERNAL_ASSERT(result.size() == result_new_fw_grad.size());
    for (const auto i : c10::irange(result.size())) {
      if (result_new_fw_grad[i].defined() && result[i].defined()) {
        // The hardcoded 0 here will need to be updated once we support multiple levels.
        result[i]._set_fw_grad(result_new_fw_grad[i], /* level */ 0, /* is_inplace_op */ false);
      }
    }
  }
  return result;
}
at::Tensor & sqrt_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<SqrtBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqrtBackward0>(new SqrtBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::sqrt_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      self_t = GradMode::is_enabled() ? self_t.clone() : self_t;
      self_new_fw_grad_opt = self_t_raw.defined() ? self_t_raw.copy_((self_t.conj() / (2 * self_p.conj())).conj()) : (self_t.conj() / (2 * self_p.conj())).conj();
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
at::Tensor squeeze(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SqueezeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqueezeBackward0>(new SqueezeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::squeeze(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: squeeze");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::squeeze(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor squeeze_dim(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SqueezeBackward1> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqueezeBackward1>(new SqueezeBackward1(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::squeeze(ks & c10::after_autograd_keyset, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: squeeze_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::squeeze(self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor squeeze_dims(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<SqueezeBackward2> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<SqueezeBackward2>(new SqueezeBackward2(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim.vec();
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::squeeze(ks & c10::after_autograd_keyset, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: squeeze_dims");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::squeeze(self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & take_out_out(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & index, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& index_ = unpack(index, "index", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("take");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("take");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> index__storage_saved =
    index_.has_storage() ? c10::optional<Storage>(index_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> index__impl_saved;
  if (index_.defined()) index__impl_saved = index_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::take_outf(ks & c10::after_autograd_keyset, self_, index_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (index__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__storage_saved.value().is_alias_of(index_.storage()));
  if (index__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(index_))
    TORCH_INTERNAL_ASSERT(index__impl_saved == index_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with take_out that does not support it because it is an out= function");
  return out;
}
at::Tensor tanh_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & output) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& output_ = unpack(output, "output", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(output));
  std::shared_ptr<TanhBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<TanhBackwardBackward0>(new TanhBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, output ));
    grad_fn->grad_output_ = SavedVariable(grad_output, false);
    grad_fn->output_ = SavedVariable(output, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> output__storage_saved =
    output_.has_storage() ? c10::optional<Storage>(output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> output__impl_saved;
  if (output_.defined()) output__impl_saved = output_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::tanh_backward(ks & c10::after_autograd_keyset, grad_output_, output_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__storage_saved.value().is_alias_of(output_.storage()));
  if (output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(output_))
    TORCH_INTERNAL_ASSERT(output__impl_saved == output_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: tanh_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: tanh_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto grad_output_p = toNonOptPrimal(grad_output);
      auto output_t_raw = toNonOptFwGrad(output);
      auto output_tensor = toNonOptTensor(output);
      auto output_t = (output_t_raw.defined() || !output_tensor.defined())
        ? output_t_raw : at::_efficientzerotensor(output_tensor.sizes(), output_tensor.options());
      auto output_p = toNonOptPrimal(output);
      result_new_fw_grad_opt = tanh_backward(grad_output_t, output_p) + output_t.conj() * (-2 * output_p.conj() * grad_output_p);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor threshold_backward(c10::DispatchKeySet ks, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& self_ = unpack(self, "self", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output, self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output) || isFwGradDefined(self));
  std::shared_ptr<ThresholdBackwardBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ThresholdBackwardBackward0>(new ThresholdBackwardBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( grad_output, self ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->threshold = threshold;
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::threshold_backward(ks & c10::after_autograd_keyset, grad_output_, self_, threshold);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: threshold_backward");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: threshold_backward");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "threshold_backward");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto grad_output_t_raw = toNonOptFwGrad(grad_output);
      auto grad_output_tensor = toNonOptTensor(grad_output);
      auto grad_output_t = (grad_output_t_raw.defined() || !grad_output_tensor.defined())
        ? grad_output_t_raw : at::_efficientzerotensor(grad_output_tensor.sizes(), grad_output_tensor.options());
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      result_new_fw_grad_opt = zeros_like(self_t) + threshold_backward(grad_output_t, self_p, threshold);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor to_mkldnn(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<ToMkldnnBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ToMkldnnBackward0>(new ToMkldnnBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::to_mkldnn", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("to_mkldnn", *opt_op, ks, self, dtype);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::to_mkldnn(ks & c10::after_autograd_keyset, self_, dtype);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: to_mkldnn");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: to_mkldnn");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "to_mkldnn");
  return result;
}
at::Tensor to_padded_tensor(c10::DispatchKeySet ks, const at::Tensor & self, double padding, at::OptionalSymIntArrayRef output_size) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<ToPaddedTensorBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ToPaddedTensorBackward0>(new ToPaddedTensorBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::to_padded_tensor", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("to_padded_tensor", *opt_op, ks, self, padding, output_size);
    } else {
      at::AutoDispatchBelowADInplaceOrView guard;
      return at::redispatch::to_padded_tensor_symint(ks & c10::after_autograd_keyset, self_, padding, output_size);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: to_padded_tensor");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: to_padded_tensor");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "to_padded_tensor");
  return result;
}
at::Tensor & tril_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("tril");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("tril");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::tril_outf(ks & c10::after_autograd_keyset, self_, diagonal, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with tril_out that does not support it because it is an out= function");
  return out;
}
::std::vector<at::Tensor> unbind_int(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UnbindBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnbindBackward0>(new UnbindBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::unbind(ks & c10::after_autograd_keyset, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<::std::vector<at::Tensor>> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::unbind(self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value()) {
    auto result_new_fw_grad = result_new_fw_grad_opt.value();
    TORCH_INTERNAL_ASSERT(result.size() == result_new_fw_grad.size());
    for (const auto i : c10::irange(result.size())) {
      if (result_new_fw_grad[i].defined() && result[i].defined()) {
        // The hardcoded 0 here will need to be updated once we support multiple levels.
        result[i]._set_fw_grad(result_new_fw_grad[i], /* level */ 0, /* is_inplace_op */ false);
      }
    }
  }
  return result;
}
void unbind_copy_out_int_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, at::TensorList out) {
  auto& self_ = unpack(self, "self", 0);
  auto out_ = unpack(out, "out", 2);
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  std::vector<c10::optional<Storage>> out__storage_saved(out_.size());
  for (const Tensor& tensor : out_)
    out__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> out__impl_saved(out_.size());
  for (size_t i=0; i<out_.size(); i++)
    if (out_[i].defined()) out__impl_saved[i] = out_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::unbind_copy_outf(ks & c10::after_autograd_keyset, self_, dim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__storage_saved[i].value().is_alias_of(out_[i].storage()));
  }
  for (size_t i=0; i<out_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (out__impl_saved[i] && !at::impl::tensorlist_has_dispatch(out_))
      TORCH_INTERNAL_ASSERT(out__impl_saved[i] == out_[i].getIntrusivePtr());
  }
  #endif
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefinedTensorList(out))), "Trying to use forward AD with unbind_copy_out that does not support it because it is an out= function");
}
at::Tensor unsqueeze_copy(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UnsqueezeBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UnsqueezeBackward0_copy>(new UnsqueezeBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->dim = dim;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::unsqueeze_copy(ks & c10::after_autograd_keyset, self_, dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: unsqueeze_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: unsqueeze_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "unsqueeze_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::unsqueeze(self_t, dim);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & upsample_bilinear2d_out_out(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("upsample_bilinear2d");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("upsample_bilinear2d");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::upsample_bilinear2d_symint_outf(ks & c10::after_autograd_keyset, self_, output_size, align_corners, scales_h, scales_w, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with upsample_bilinear2d_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & upsample_linear1d_backward_out_grad_input(c10::DispatchKeySet ks, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales, at::Tensor & grad_input) {
  auto& grad_output_ = unpack(grad_output, "grad_output", 0);
  auto& grad_input_ = unpack(grad_input, "grad_input", 5);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( grad_output );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(grad_output));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( grad_output )) {
    throw_error_out_requires_grad("upsample_linear1d_backward");
  }
  if (compute_requires_grad( grad_input )) {
    throw_error_out_requires_grad("upsample_linear1d_backward");
  }
  #ifndef NDEBUG
  c10::optional<Storage> grad_output__storage_saved =
    grad_output_.has_storage() ? c10::optional<Storage>(grad_output_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_output__impl_saved;
  if (grad_output_.defined()) grad_output__impl_saved = grad_output_.getIntrusivePtr();
  c10::optional<Storage> grad_input__storage_saved =
    grad_input_.has_storage() ? c10::optional<Storage>(grad_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> grad_input__impl_saved;
  if (grad_input_.defined()) grad_input__impl_saved = grad_input_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::upsample_linear1d_backward_symint_outf(ks & c10::after_autograd_keyset, grad_output_, output_size, input_size, align_corners, scales, grad_input_);
  }
  #ifndef NDEBUG
  if (grad_output__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__storage_saved.value().is_alias_of(grad_output_.storage()));
  if (grad_output__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_output_))
    TORCH_INTERNAL_ASSERT(grad_output__impl_saved == grad_output_.getIntrusivePtr());
  if (grad_input__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__storage_saved.value().is_alias_of(grad_input_.storage()));
  if (grad_input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_input_))
    TORCH_INTERNAL_ASSERT(grad_input__impl_saved == grad_input_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( grad_input ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(grad_output) || isFwGradDefined(grad_input))), "Trying to use forward AD with upsample_linear1d_backward_out that does not support it because it is an out= function");
  return grad_input;
}
at::Tensor upsample_nearest3d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UpsampleNearest3DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleNearest3DBackward0>(new UpsampleNearest3DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_d = scales_d;
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_nearest3d_symint(ks & c10::after_autograd_keyset, self_, output_size, scales_d, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_nearest3d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_nearest3d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_nearest3d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::upsample_nearest3d_symint(self_t, output_size, scales_d, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor upsample_trilinear3d(c10::DispatchKeySet ks, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<UpsampleTrilinear3DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<UpsampleTrilinear3DBackward0>(new UpsampleTrilinear3DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->align_corners = align_corners;
    grad_fn->output_size = output_size.vec();
    grad_fn->scales_d = scales_d;
    grad_fn->scales_h = scales_h;
    grad_fn->scales_w = scales_w;
    grad_fn->self_sym_sizes = self.sym_sizes().vec();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::upsample_trilinear3d_symint(ks & c10::after_autograd_keyset, self_, output_size, align_corners, scales_d, scales_h, scales_w);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: upsample_trilinear3d");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: upsample_trilinear3d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "upsample_trilinear3d");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::upsample_trilinear3d_symint(self_t, output_size, align_corners, scales_d, scales_h, scales_w);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor values(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  std::shared_ptr<ValuesBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ValuesBackward0>(new ValuesBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    if ((isFwGradDefined(self))) {
      static c10::OperatorName full_name("aten::values", "");
      static c10::optional<c10::OperatorHandle> opt_op = c10::Dispatcher::singleton().findSchema(full_name);
      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("values", *opt_op, ks, self);
    } else {
      at::AutoDispatchBelowAutograd guard;
      return at::redispatch::values(ks & c10::after_autograd_keyset, self_);
    }
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(result.storage()));
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: values");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  return result;
}
at::Tensor & var_out_correction_out(c10::DispatchKeySet ks, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 4);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("var");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("var");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::var_outf(ks & c10::after_autograd_keyset, self_, dim, correction, keepdim, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with var_out that does not support it because it is an out= function");
  return out;
}
at::Tensor vdot(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<VdotBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<VdotBackward0>(new VdotBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    if (grad_fn->should_compute_output(0)) {
      grad_fn->other_ = SavedVariable(other, false);
    }
    if (grad_fn->should_compute_output(1)) {
      grad_fn->self_ = SavedVariable(self, false);
    }
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::vdot(ks & c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: vdot");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: vdot");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto self_p = toNonOptPrimal(self);
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      auto other_p = toNonOptPrimal(other);
      result_new_fw_grad_opt = at::vdot(self_t, other_p) + at::vdot(self_p, other_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor view_as_complex_copy(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<ViewAsComplexBackward0_copy> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ViewAsComplexBackward0_copy>(new ViewAsComplexBackward0_copy(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::view_as_complex_copy(ks & c10::after_autograd_keyset, self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: view_as_complex_copy");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: view_as_complex_copy");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "view_as_complex_copy");
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      result_new_fw_grad_opt = at::view_as_complex(self_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor where_self(c10::DispatchKeySet ks, const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
  auto& condition_ = unpack(condition, "condition", 0);
  auto& self_ = unpack(self, "self", 1);
  auto& other_ = unpack(other, "other", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<WhereBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<WhereBackward0>(new WhereBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, other ));
    grad_fn->condition_ = SavedVariable(condition, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> condition__storage_saved =
    condition_.has_storage() ? c10::optional<Storage>(condition_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> condition__impl_saved;
  if (condition_.defined()) condition__impl_saved = condition_.getIntrusivePtr();
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::where(ks & c10::after_autograd_keyset, condition_, self_, other_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (condition__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(condition_))
    TORCH_INTERNAL_ASSERT(condition__storage_saved.value().is_alias_of(condition_.storage()));
  if (condition__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(condition_))
    TORCH_INTERNAL_ASSERT(condition__impl_saved == condition_.getIntrusivePtr());
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (result.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result)) {
    TORCH_INTERNAL_ASSERT(result.storage().use_count() == 1, "function: where_self");
  }
  if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result))
    TORCH_INTERNAL_ASSERT(result.use_count() <= 1, "function: where_self");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  c10::optional<at::Tensor> result_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_result && (result.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::_efficientzerotensor(self_tensor.sizes(), self_tensor.options());
      auto other_t_raw = toNonOptFwGrad(other);
      auto other_tensor = toNonOptTensor(other);
      auto other_t = (other_t_raw.defined() || !other_tensor.defined())
        ? other_t_raw : at::_efficientzerotensor(other_tensor.sizes(), other_tensor.options());
      result_new_fw_grad_opt = where(condition, self_t, other_t);
  }
  if (result_new_fw_grad_opt.has_value() && result_new_fw_grad_opt.value().defined() && result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
at::Tensor & xlogy_out_OutTensor(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self, other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self, other )) {
    throw_error_out_requires_grad("xlogy");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("xlogy");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::xlogy_outf(ks & c10::after_autograd_keyset, self_, other_, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with xlogy_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & xlogy_out_OutScalar_Self(c10::DispatchKeySet ks, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
  auto& other_ = unpack(other, "other", 1);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( other );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(other));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( other )) {
    throw_error_out_requires_grad("xlogy");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("xlogy");
  }
  #ifndef NDEBUG
  c10::optional<Storage> other__storage_saved =
    other_.has_storage() ? c10::optional<Storage>(other_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> other__impl_saved;
  if (other_.defined()) other__impl_saved = other_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::xlogy_outf(ks & c10::after_autograd_keyset, self, other_, out_);
  }
  #ifndef NDEBUG
  if (other__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__storage_saved.value().is_alias_of(other_.storage()));
  if (other__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(other_))
    TORCH_INTERNAL_ASSERT(other__impl_saved == other_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(other) || isFwGradDefined(out))), "Trying to use forward AD with xlogy_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & xlogy_out_OutScalar_Other(c10::DispatchKeySet ks, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto& self_ = unpack(self, "self", 0);
  auto& out_ = unpack(out, "out", 2);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_result = (isFwGradDefined(self));
  std::shared_ptr<Node> grad_fn;
  if (compute_requires_grad( self )) {
    throw_error_out_requires_grad("xlogy");
  }
  if (compute_requires_grad( out )) {
    throw_error_out_requires_grad("xlogy");
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> out__storage_saved =
    out_.has_storage() ? c10::optional<Storage>(out_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out__impl_saved;
  if (out_.defined()) out__impl_saved = out_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::xlogy_outf(ks & c10::after_autograd_keyset, self_, other, out_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (out__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__storage_saved.value().is_alias_of(out_.storage()));
  if (out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(out_))
    TORCH_INTERNAL_ASSERT(out__impl_saved == out_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( out ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!((isFwGradDefined(self) || isFwGradDefined(out))), "Trying to use forward AD with xlogy_out that does not support it because it is an out= function");
  return out;
}
at::Tensor & zero_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  [[maybe_unused]] auto _any_requires_grad = compute_requires_grad( self );
  
  [[maybe_unused]] auto _any_has_forward_grad_self = (isFwGradDefined(self));
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<ZeroBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<ZeroBackward0>(new ZeroBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::zero_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value() &&
      !at::impl::dispatch_mode_enabled() &&
      !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(self_))
    TORCH_INTERNAL_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  c10::optional<at::Tensor> self_new_fw_grad_opt = c10::nullopt;
  if (_any_has_forward_grad_self && (self.defined())) {
      auto self_t_raw = toNonOptFwGrad(self);
      auto self_tensor = toNonOptTensor(self);
      auto self_t = (self_t_raw.defined() || !self_tensor.defined())
        ? self_t_raw : at::zeros(self_tensor.sizes(), self_tensor.options());
      self_new_fw_grad_opt = at::zero_(self_t);
  }
  if (self_new_fw_grad_opt.has_value() && self_new_fw_grad_opt.value().defined() && self.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    self._set_fw_grad(self_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ true);
  }
  return self;
}
}
}

namespace {

TORCH_LIBRARY_IMPL(aten, AutogradCUDA, m) {
m.impl("_test_autograd_multiple_dispatch.fullcoverage",
       TORCH_FN(VariableType::_test_autograd_multiple_dispatch_fullcoverage_AutogradCUDA)
);
}

TORCH_LIBRARY_IMPL(aten, AutogradNestedTensor, m) {
m.impl("_test_autograd_multiple_dispatch.fullcoverage",
       TORCH_FN(VariableType::_test_autograd_multiple_dispatch_fullcoverage_AutogradNestedTensor)
);
m.impl("_test_autograd_multiple_dispatch.ntonly",
       TORCH_FN(VariableType::_test_autograd_multiple_dispatch_ntonly_AutogradNestedTensor)
);
m.impl("squeeze.dim",
       TORCH_FN(VariableType::squeeze_dim_AutogradNestedTensor)
);
m.impl("squeeze.dims",
       TORCH_FN(VariableType::squeeze_dims_AutogradNestedTensor)
);
m.impl("unbind.int",
       TORCH_FN(VariableType::unbind_int_AutogradNestedTensor)
);
m.impl("unbind_copy.int_out",
       TORCH_FN(VariableType::unbind_copy_out_int_out_AutogradNestedTensor)
);
m.impl("values",
       TORCH_FN(VariableType::values_AutogradNestedTensor)
);
}

TORCH_LIBRARY_IMPL(aten, Autograd, m) {
m.impl("__rshift__.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("__rshift__.Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("_adaptive_avg_pool3d_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_add_relu_.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("_add_relu_.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("_aminmax", torch::autograd::autogradNotImplementedFallback());
m.impl("_aminmax.dim", torch::autograd::autogradNotImplementedFallback());
m.impl("_aminmax.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_aminmax.dim_out", torch::autograd::autogradNotImplementedFallback());
m.impl("_amp_foreach_non_finite_check_and_unscale", torch::autograd::autogradNotImplementedFallback());
m.impl("_amp_foreach_non_finite_check_and_unscale_",
       TORCH_FN(VariableType::_amp_foreach_non_finite_check_and_unscale_)
);
m.impl("_amp_update_scale.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_cdist_forward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_conj",
       TORCH_FN(VariableType::_conj)
);
m.impl("_conj_physical",
       TORCH_FN(VariableType::_conj_physical)
);
m.impl("_conj_physical.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_conv_depthwise2d.out",
       TORCH_FN(VariableType::_conv_depthwise2d_out_out)
);
m.impl("_convolution",
       TORCH_FN(VariableType::_convolution)
);
m.impl("_cslt_sparse_mm", torch::autograd::autogradNotImplementedFallback());
m.impl("_ctc_loss",
       TORCH_FN(VariableType::_ctc_loss)
);
m.impl("_ctc_loss.Tensor",
       TORCH_FN(VariableType::_ctc_loss_Tensor)
);
m.impl("_cudnn_ctc_loss",
       TORCH_FN(VariableType::_cudnn_ctc_loss)
);
m.impl("_cudnn_ctc_loss.Tensor",
       TORCH_FN(VariableType::_cudnn_ctc_loss_Tensor)
);
m.impl("_cudnn_rnn",
       TORCH_FN(VariableType::_cudnn_rnn)
);
m.impl("_cudnn_rnn_backward.out",
       TORCH_FN(VariableType::_cudnn_rnn_backward_out_out)
);
m.impl("_embedding_bag",
       TORCH_FN(VariableType::_embedding_bag)
);
m.impl("_embedding_bag_forward_only.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_fft_r2c.out",
       TORCH_FN(VariableType::_fft_r2c_out_out)
);
m.impl("_flash_attention_forward",
       TORCH_FN(VariableType::_flash_attention_forward)
);
m.impl("_foreach_acos_",
       TORCH_FN(VariableType::_foreach_acos_)
);
m.impl("_foreach_add.Scalar",
       TORCH_FN(VariableType::_foreach_add_Scalar)
);
m.impl("_foreach_add.List",
       TORCH_FN(VariableType::_foreach_add_List)
);
m.impl("_foreach_add.ScalarList",
       TORCH_FN(VariableType::_foreach_add_ScalarList)
);
m.impl("_foreach_add.Tensor",
       TORCH_FN(VariableType::_foreach_add_Tensor)
);
m.impl("_foreach_add.Scalar_out",
       TORCH_FN(VariableType::_foreach_add_out_Scalar_out)
);
m.impl("_foreach_add.List_out",
       TORCH_FN(VariableType::_foreach_add_out_List_out)
);
m.impl("_foreach_add.ScalarList_out",
       TORCH_FN(VariableType::_foreach_add_out_ScalarList_out)
);
m.impl("_foreach_add.Tensor_out",
       TORCH_FN(VariableType::_foreach_add_out_Tensor_out)
);
m.impl("_foreach_asin_",
       TORCH_FN(VariableType::_foreach_asin_)
);
m.impl("_foreach_atan.out",
       TORCH_FN(VariableType::_foreach_atan_out_out)
);
m.impl("_foreach_clamp_min.Scalar",
       TORCH_FN(VariableType::_foreach_clamp_min_Scalar)
);
m.impl("_foreach_clamp_min.List",
       TORCH_FN(VariableType::_foreach_clamp_min_List)
);
m.impl("_foreach_clamp_min.ScalarList",
       TORCH_FN(VariableType::_foreach_clamp_min_ScalarList)
);
m.impl("_foreach_clamp_min_.Scalar",
       TORCH_FN(VariableType::_foreach_clamp_min__Scalar)
);
m.impl("_foreach_clamp_min_.List",
       TORCH_FN(VariableType::_foreach_clamp_min__List)
);
m.impl("_foreach_clamp_min_.ScalarList",
       TORCH_FN(VariableType::_foreach_clamp_min__ScalarList)
);
m.impl("_foreach_copy.out",
       TORCH_FN(VariableType::_foreach_copy_out_out)
);
m.impl("_foreach_cosh",
       TORCH_FN(VariableType::_foreach_cosh)
);
m.impl("_foreach_cosh_",
       TORCH_FN(VariableType::_foreach_cosh_)
);
m.impl("_foreach_erfc",
       TORCH_FN(VariableType::_foreach_erfc)
);
m.impl("_foreach_expm1_",
       TORCH_FN(VariableType::_foreach_expm1_)
);
m.impl("_foreach_frac",
       TORCH_FN(VariableType::_foreach_frac)
);
m.impl("_foreach_frac.out",
       TORCH_FN(VariableType::_foreach_frac_out_out)
);
m.impl("_foreach_lerp.List",
       TORCH_FN(VariableType::_foreach_lerp_List)
);
m.impl("_foreach_lerp.Scalar",
       TORCH_FN(VariableType::_foreach_lerp_Scalar)
);
m.impl("_foreach_lgamma",
       TORCH_FN(VariableType::_foreach_lgamma)
);
m.impl("_foreach_log_",
       TORCH_FN(VariableType::_foreach_log_)
);
m.impl("_foreach_log.out",
       TORCH_FN(VariableType::_foreach_log_out_out)
);
m.impl("_foreach_maximum.Scalar_out",
       TORCH_FN(VariableType::_foreach_maximum_out_Scalar_out)
);
m.impl("_foreach_maximum.List_out",
       TORCH_FN(VariableType::_foreach_maximum_out_List_out)
);
m.impl("_foreach_maximum.ScalarList_out",
       TORCH_FN(VariableType::_foreach_maximum_out_ScalarList_out)
);
m.impl("_foreach_minimum.Scalar",
       TORCH_FN(VariableType::_foreach_minimum_Scalar)
);
m.impl("_foreach_minimum.List",
       TORCH_FN(VariableType::_foreach_minimum_List)
);
m.impl("_foreach_minimum.ScalarList",
       TORCH_FN(VariableType::_foreach_minimum_ScalarList)
);
m.impl("_foreach_minimum_.Scalar",
       TORCH_FN(VariableType::_foreach_minimum__Scalar)
);
m.impl("_foreach_minimum_.List",
       TORCH_FN(VariableType::_foreach_minimum__List)
);
m.impl("_foreach_minimum_.ScalarList",
       TORCH_FN(VariableType::_foreach_minimum__ScalarList)
);
m.impl("_foreach_round",
       TORCH_FN(VariableType::_foreach_round)
);
m.impl("_foreach_sign.out",
       TORCH_FN(VariableType::_foreach_sign_out_out)
);
m.impl("_foreach_sub.Scalar_out",
       TORCH_FN(VariableType::_foreach_sub_out_Scalar_out)
);
m.impl("_foreach_sub.List_out",
       TORCH_FN(VariableType::_foreach_sub_out_List_out)
);
m.impl("_foreach_sub.ScalarList_out",
       TORCH_FN(VariableType::_foreach_sub_out_ScalarList_out)
);
m.impl("_foreach_tan_",
       TORCH_FN(VariableType::_foreach_tan_)
);
m.impl("_foreach_trunc",
       TORCH_FN(VariableType::_foreach_trunc)
);
m.impl("_foreach_trunc_",
       TORCH_FN(VariableType::_foreach_trunc_)
);
m.impl("_foreach_trunc.out",
       TORCH_FN(VariableType::_foreach_trunc_out_out)
);
m.impl("_fused_dropout",
       TORCH_FN(VariableType::_fused_dropout)
);
m.impl("_fused_dropout.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_fused_moving_avg_obs_fq_helper",
       TORCH_FN(VariableType::_fused_moving_avg_obs_fq_helper)
);
m.impl("_has_same_storage_numel",
       TORCH_FN(VariableType::_has_same_storage_numel)
);
m.impl("_index_put_impl",
       TORCH_FN(VariableType::_index_put_impl)
);
m.impl("_is_all_true",
       TORCH_FN(VariableType::_is_all_true)
);
m.impl("_linalg_eigh",
       TORCH_FN(VariableType::_linalg_eigh)
);
m.impl("_log_softmax",
       TORCH_FN(VariableType::_log_softmax)
);
m.impl("_log_softmax.out",
       TORCH_FN(VariableType::_log_softmax_out_out)
);
m.impl("_make_dual_copy", torch::autograd::autogradNotImplementedFallback());
m.impl("_make_per_tensor_quantized_tensor.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_masked_softmax_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_mixed_dtypes_linear", torch::autograd::autogradNotImplementedFallback());
m.impl("_mps_convolution.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_neg_view",
       TORCH_FN(VariableType::_neg_view)
);
m.impl("_neg_view_copy",
       TORCH_FN(VariableType::_neg_view_copy)
);
m.impl("_nested_tensor_softmax_with_shape", torch::autograd::autogradNotImplementedFallback());
m.impl("_nested_view_from_buffer_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_nnpack_spatial_convolution.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_pin_memory",
       TORCH_FN(VariableType::_pin_memory)
);
m.impl("_pin_memory.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_prelu_kernel_backward",
       TORCH_FN(VariableType::_prelu_kernel_backward)
);
m.impl("_sample_dirichlet", torch::autograd::autogradNotImplementedFallback());
m.impl("_segment_reduce_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("_slow_conv2d_backward.grad_input", torch::autograd::autogradNotImplementedFallback());
m.impl("_slow_conv2d_backward.output_mask_out", torch::autograd::autogradNotImplementedFallback());
m.impl("_slow_conv2d_forward",
       TORCH_FN(VariableType::_slow_conv2d_forward)
);
m.impl("_softmax_backward_data.out",
       TORCH_FN(VariableType::_softmax_backward_data_out_out)
);
m.impl("_sparse_addmm",
       TORCH_FN(VariableType::_sparse_addmm)
);
m.impl("_sparse_coo_tensor_with_dims.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_log_softmax.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_mm_reduce_impl_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("_sparse_sum.dim",
       TORCH_FN(VariableType::_sparse_sum_dim)
);
m.impl("_spdiags.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_standard_gamma",
       TORCH_FN(VariableType::_standard_gamma)
);
m.impl("_standard_gamma.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_test_autograd_multiple_dispatch.fullcoverage",
       TORCH_FN(VariableType::_test_autograd_multiple_dispatch_fullcoverage)
);
m.impl("_test_optional_intlist", torch::autograd::autogradNotImplementedFallback());
m.impl("_thnn_fused_gru_cell.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_to_copy",
       TORCH_FN(VariableType::_to_copy)
);
m.impl("_transform_bias_rescale_qkv", torch::autograd::autogradNotImplementedFallback());
m.impl("_unique",
       TORCH_FN(VariableType::_unique)
);
m.impl("_unsafe_index_put",
       TORCH_FN(VariableType::_unsafe_index_put)
);
m.impl("_unsafe_view.out", torch::autograd::autogradNotImplementedFallback());
m.impl("_upsample_bicubic2d_aa",
       TORCH_FN(VariableType::_upsample_bicubic2d_aa)
);
m.impl("_upsample_bicubic2d_aa_backward.grad_input",
       TORCH_FN(VariableType::_upsample_bicubic2d_aa_backward_out_grad_input)
);
m.impl("_upsample_bilinear2d_aa_backward",
       TORCH_FN(VariableType::_upsample_bilinear2d_aa_backward)
);
m.impl("_upsample_nearest_exact1d.out",
       TORCH_FN(VariableType::_upsample_nearest_exact1d_out_out)
);
m.impl("_use_cudnn_ctc_loss",
       TORCH_FN(VariableType::_use_cudnn_ctc_loss)
);
m.impl("_use_cudnn_ctc_loss.Tensor",
       TORCH_FN(VariableType::_use_cudnn_ctc_loss_Tensor)
);
m.impl("_weight_norm_interface",
       TORCH_FN(VariableType::_weight_norm_interface)
);
m.impl("abs.out",
       TORCH_FN(VariableType::abs_out_out)
);
m.impl("acosh",
       TORCH_FN(VariableType::acosh)
);
m.impl("adaptive_avg_pool3d_backward.grad_input", torch::autograd::autogradNotImplementedFallback());
m.impl("adaptive_avg_pool3d.out", torch::autograd::autogradNotImplementedFallback());
m.impl("addcmul",
       TORCH_FN(VariableType::addcmul)
);
m.impl("addcmul_",
       TORCH_FN(VariableType::addcmul_)
);
m.impl("addmm",
       TORCH_FN(VariableType::addmm)
);
m.impl("addmv_",
       TORCH_FN(VariableType::addmv_)
);
m.impl("allclose",
       TORCH_FN(VariableType::allclose)
);
m.impl("amin.out",
       TORCH_FN(VariableType::amin_out_out)
);
m.impl("aminmax", torch::autograd::autogradNotImplementedFallback());
m.impl("angle.out",
       TORCH_FN(VariableType::angle_out_out)
);
m.impl("argmax",
       TORCH_FN(VariableType::argmax)
);
m.impl("argsort.stable_out",
       TORCH_FN(VariableType::argsort_out_stable_out)
);
m.impl("as_strided",
       TORCH_FN(VariableType::as_strided)
);
m.impl("as_strided_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("as_strided_scatter",
       TORCH_FN(VariableType::as_strided_scatter)
);
m.impl("atanh_",
       TORCH_FN(VariableType::atanh_)
);
m.impl("avg_pool2d.out",
       TORCH_FN(VariableType::avg_pool2d_out_out)
);
m.impl("batch_norm_backward_elemt", torch::autograd::autogradNotImplementedFallback());
m.impl("batch_norm_backward_reduce.out", torch::autograd::autogradNotImplementedFallback());
m.impl("batch_norm_gather_stats_with_counts.out", torch::autograd::autogradNotImplementedFallback());
m.impl("batch_norm_update_stats.out", torch::autograd::autogradNotImplementedFallback());
m.impl("binary_cross_entropy.out",
       TORCH_FN(VariableType::binary_cross_entropy_out_out)
);
m.impl("binary_cross_entropy_with_logits.out", torch::autograd::autogradNotImplementedFallback());
m.impl("binomial", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_and.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_and.Scalar_Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_and.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_left_shift.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_left_shift.Tensor_Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_left_shift.Scalar_Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_left_shift.Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_left_shift.Tensor_Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_left_shift.Scalar_Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_or_.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_or_.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_right_shift.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_right_shift.Tensor_Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_right_shift.Scalar_Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_xor.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_xor.Scalar_Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_xor.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_xor_.Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("bitwise_xor_.Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("blackman_window", torch::autograd::autogradNotImplementedFallback());
m.impl("blackman_window.periodic", torch::autograd::autogradNotImplementedFallback());
m.impl("block_diag.out", torch::autograd::autogradNotImplementedFallback());
m.impl("cat",
       TORCH_FN(VariableType::cat)
);
m.impl("ccol_indices_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("cholesky",
       TORCH_FN(VariableType::cholesky)
);
m.impl("clamp_",
       TORCH_FN(VariableType::clamp_)
);
m.impl("clamp_.Tensor",
       TORCH_FN(VariableType::clamp__Tensor)
);
m.impl("clamp_min_",
       TORCH_FN(VariableType::clamp_min_)
);
m.impl("clamp_min_.Tensor",
       TORCH_FN(VariableType::clamp_min__Tensor)
);
m.impl("conj_physical.out", torch::autograd::autogradNotImplementedFallback());
m.impl("conv_tbc.out", torch::autograd::autogradNotImplementedFallback());
m.impl("convolution",
       TORCH_FN(VariableType::convolution)
);
m.impl("convolution_backward_overrideable",
       TORCH_FN(VariableType::convolution_backward_overrideable)
);
m.impl("copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("copy_sparse_to_sparse", torch::autograd::autogradNotImplementedFallback());
m.impl("copy_sparse_to_sparse_", torch::autograd::autogradNotImplementedFallback());
m.impl("copy_sparse_to_sparse.out", torch::autograd::autogradNotImplementedFallback());
m.impl("cos",
       TORCH_FN(VariableType::cos)
);
m.impl("cos.out",
       TORCH_FN(VariableType::cos_out_out)
);
m.impl("crow_indices_copy",
       TORCH_FN(VariableType::crow_indices_copy)
);
m.impl("cudnn_affine_grid_generator",
       TORCH_FN(VariableType::cudnn_affine_grid_generator)
);
m.impl("cudnn_affine_grid_generator.out", torch::autograd::autogradNotImplementedFallback());
m.impl("cudnn_batch_norm_backward",
       TORCH_FN(VariableType::cudnn_batch_norm_backward)
);
m.impl("cudnn_convolution_relu.out", torch::autograd::autogradNotImplementedFallback());
m.impl("cudnn_convolution_transpose",
       TORCH_FN(VariableType::cudnn_convolution_transpose)
);
m.impl("cudnn_grid_sampler_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("cudnn_grid_sampler_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("cumsum",
       TORCH_FN(VariableType::cumsum)
);
m.impl("diagonal_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("diagonal_scatter",
       TORCH_FN(VariableType::diagonal_scatter)
);
m.impl("digamma_",
       TORCH_FN(VariableType::digamma_)
);
m.impl("dist.out", torch::autograd::autogradNotImplementedFallback());
m.impl("div.out",
       TORCH_FN(VariableType::div_out_out)
);
m.impl("div.out_mode",
       TORCH_FN(VariableType::div_out_out_mode)
);
m.impl("div.Scalar_out", torch::autograd::autogradNotImplementedFallback());
m.impl("div.Scalar_mode_out", torch::autograd::autogradNotImplementedFallback());
m.impl("elu_backward",
       TORCH_FN(VariableType::elu_backward)
);
m.impl("embedding_dense_backward",
       TORCH_FN(VariableType::embedding_dense_backward)
);
m.impl("embedding_dense_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("empty_permuted.out", torch::autograd::autogradNotImplementedFallback());
m.impl("empty_strided.out", torch::autograd::autogradNotImplementedFallback());
m.impl("eq.Scalar",
       TORCH_FN(VariableType::eq_Scalar)
);
m.impl("eq.Tensor",
       TORCH_FN(VariableType::eq_Tensor)
);
m.impl("erfinv.out",
       TORCH_FN(VariableType::erfinv_out_out)
);
m.impl("exp_",
       TORCH_FN(VariableType::exp_)
);
m.impl("expand_copy",
       TORCH_FN(VariableType::expand_copy)
);
m.impl("expm1",
       TORCH_FN(VariableType::expm1)
);
m.impl("exponential",
       TORCH_FN(VariableType::exponential)
);
m.impl("exponential_",
       TORCH_FN(VariableType::exponential_)
);
m.impl("fake_quantize_per_channel_affine_cachemask",
       TORCH_FN(VariableType::fake_quantize_per_channel_affine_cachemask)
);
m.impl("fill.Scalar",
       TORCH_FN(VariableType::fill_Scalar)
);
m.impl("fill.Tensor",
       TORCH_FN(VariableType::fill_Tensor)
);
m.impl("floor_",
       TORCH_FN(VariableType::floor_)
);
m.impl("fmod.Scalar_out",
       TORCH_FN(VariableType::fmod_out_Scalar_out)
);
m.impl("fmod.Tensor_out",
       TORCH_FN(VariableType::fmod_out_Tensor_out)
);
m.impl("fractional_max_pool3d_backward",
       TORCH_FN(VariableType::fractional_max_pool3d_backward)
);
m.impl("full_like.out",
       TORCH_FN(VariableType::full_like_out_out)
);
m.impl("full.out", torch::autograd::autogradNotImplementedFallback());
m.impl("full.names_out", torch::autograd::autogradNotImplementedFallback());
m.impl("gelu_backward",
       TORCH_FN(VariableType::gelu_backward)
);
m.impl("gelu_backward.grad_input",
       TORCH_FN(VariableType::gelu_backward_out_grad_input)
);
m.impl("geometric",
       TORCH_FN(VariableType::geometric)
);
m.impl("geometric_",
       TORCH_FN(VariableType::geometric_)
);
m.impl("glu.out",
       TORCH_FN(VariableType::glu_out_out)
);
m.impl("grid_sampler_2d_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("grid_sampler_2d_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("grid_sampler_3d_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("hann_window.out", torch::autograd::autogradNotImplementedFallback());
m.impl("hann_window.periodic_out", torch::autograd::autogradNotImplementedFallback());
m.impl("hardsigmoid_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("histogram.bins_tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("histogram.bin_ct", torch::autograd::autogradNotImplementedFallback());
m.impl("hspmm.out", torch::autograd::autogradNotImplementedFallback());
m.impl("igamma",
       TORCH_FN(VariableType::igamma)
);
m.impl("index_copy",
       TORCH_FN(VariableType::index_copy)
);
m.impl("index_copy_",
       TORCH_FN(VariableType::index_copy_)
);
m.impl("index_select",
       TORCH_FN(VariableType::index_select)
);
m.impl("int_repr", torch::autograd::autogradNotImplementedFallback());
m.impl("is_pinned",
       TORCH_FN(VariableType::is_pinned)
);
m.impl("isnan.out",
       TORCH_FN(VariableType::isnan_out_out)
);
m.impl("kaiser_window.out", torch::autograd::autogradNotImplementedFallback());
m.impl("kaiser_window.periodic_out", torch::autograd::autogradNotImplementedFallback());
m.impl("kaiser_window.beta_out", torch::autograd::autogradNotImplementedFallback());
m.impl("kthvalue.values",
       TORCH_FN(VariableType::kthvalue_out_values)
);
m.impl("lcm", torch::autograd::autogradNotImplementedFallback());
m.impl("lcm_", torch::autograd::autogradNotImplementedFallback());
m.impl("le.Scalar",
       TORCH_FN(VariableType::le_Scalar)
);
m.impl("le.Tensor",
       TORCH_FN(VariableType::le_Tensor)
);
m.impl("le_.Scalar",
       TORCH_FN(VariableType::le__Scalar)
);
m.impl("le_.Tensor",
       TORCH_FN(VariableType::le__Tensor)
);
m.impl("leaky_relu_backward.grad_input",
       TORCH_FN(VariableType::leaky_relu_backward_out_grad_input)
);
m.impl("lerp.Scalar_out",
       TORCH_FN(VariableType::lerp_out_Scalar_out)
);
m.impl("lerp.Tensor_out",
       TORCH_FN(VariableType::lerp_out_Tensor_out)
);
m.impl("lgamma",
       TORCH_FN(VariableType::lgamma)
);
m.impl("lgamma_",
       TORCH_FN(VariableType::lgamma_)
);
m.impl("lgamma.out",
       TORCH_FN(VariableType::lgamma_out_out)
);
m.impl("lift",
       TORCH_FN(VariableType::lift)
);
m.impl("lift_fresh",
       TORCH_FN(VariableType::lift_fresh)
);
m.impl("lift_fresh_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("linalg_inv_ex.inverse",
       TORCH_FN(VariableType::linalg_inv_ex_out_inverse)
);
m.impl("linalg_ldl_factor_ex", torch::autograd::autogradNotImplementedFallback());
m.impl("linalg_ldl_solve", torch::autograd::autogradNotImplementedFallback());
m.impl("linalg_lstsq",
       TORCH_FN(VariableType::linalg_lstsq)
);
m.impl("linalg_lstsq.out",
       TORCH_FN(VariableType::linalg_lstsq_out_out)
);
m.impl("linalg_lu",
       TORCH_FN(VariableType::linalg_lu)
);
m.impl("linalg_matrix_exp",
       TORCH_FN(VariableType::linalg_matrix_exp)
);
m.impl("linear",
       TORCH_FN(VariableType::linear)
);
m.impl("linspace", torch::autograd::autogradNotImplementedFallback());
m.impl("linspace.Tensor_Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("linspace.Tensor_Scalar", torch::autograd::autogradNotImplementedFallback());
m.impl("linspace.Scalar_Tensor", torch::autograd::autogradNotImplementedFallback());
m.impl("log10",
       TORCH_FN(VariableType::log10)
);
m.impl("log10_",
       TORCH_FN(VariableType::log10_)
);
m.impl("log1p",
       TORCH_FN(VariableType::log1p)
);
m.impl("log1p.out",
       TORCH_FN(VariableType::log1p_out_out)
);
m.impl("log_",
       TORCH_FN(VariableType::log_)
);
m.impl("log_normal.out", torch::autograd::autogradNotImplementedFallback());
m.impl("logaddexp2",
       TORCH_FN(VariableType::logaddexp2)
);
m.impl("logaddexp2.out",
       TORCH_FN(VariableType::logaddexp2_out_out)
);
m.impl("logical_and_",
       TORCH_FN(VariableType::logical_and_)
);
m.impl("logical_or",
       TORCH_FN(VariableType::logical_or)
);
m.impl("logical_or_",
       TORCH_FN(VariableType::logical_or_)
);
m.impl("logit_backward.grad_input", torch::autograd::autogradNotImplementedFallback());
m.impl("logsumexp",
       TORCH_FN(VariableType::logsumexp)
);
m.impl("lu_unpack",
       TORCH_FN(VariableType::lu_unpack)
);
m.impl("masked_scatter.out", torch::autograd::autogradNotImplementedFallback());
m.impl("max.dim",
       TORCH_FN(VariableType::max_dim)
);
m.impl("max",
       TORCH_FN(VariableType::max)
);
m.impl("max_pool2d_backward.out", torch::autograd::autogradNotImplementedFallback());
m.impl("max_pool2d_with_indices",
       TORCH_FN(VariableType::max_pool2d_with_indices)
);
m.impl("max_unpool3d.out",
       TORCH_FN(VariableType::max_unpool3d_out_out)
);
m.impl("maximum.out",
       TORCH_FN(VariableType::maximum_out_out)
);
m.impl("minimum.out",
       TORCH_FN(VariableType::minimum_out_out)
);
m.impl("miopen_batch_norm_backward",
       TORCH_FN(VariableType::miopen_batch_norm_backward)
);
m.impl("miopen_convolution_relu", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_adaptive_avg_pool2d_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_convolution",
       TORCH_FN(VariableType::mkldnn_convolution)
);
m.impl("mkldnn_convolution.out", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_linear",
       TORCH_FN(VariableType::mkldnn_linear)
);
m.impl("mkldnn_linear_backward_weights", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_max_pool3d_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("mkldnn_reorder_conv3d_weight.out", torch::autograd::autogradNotImplementedFallback());
m.impl("mode",
       TORCH_FN(VariableType::mode)
);
m.impl("mse_loss_backward",
       TORCH_FN(VariableType::mse_loss_backward)
);
m.impl("mse_loss_backward.grad_input",
       TORCH_FN(VariableType::mse_loss_backward_out_grad_input)
);
m.impl("mul.Tensor",
       TORCH_FN(VariableType::mul_Tensor)
);
m.impl("mul.Scalar",
       TORCH_FN(VariableType::mul_Scalar)
);
m.impl("multi_margin_loss_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("multi_margin_loss.out",
       TORCH_FN(VariableType::multi_margin_loss_out_out)
);
m.impl("multilabel_margin_loss_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("multilabel_margin_loss_backward.grad_input", torch::autograd::autogradNotImplementedFallback());
m.impl("multinomial",
       TORCH_FN(VariableType::multinomial)
);
m.impl("multinomial.out",
       TORCH_FN(VariableType::multinomial_out_out)
);
m.impl("mvlgamma",
       TORCH_FN(VariableType::mvlgamma)
);
m.impl("mvlgamma.out",
       TORCH_FN(VariableType::mvlgamma_out_out)
);
m.impl("narrow", torch::autograd::autogradNotImplementedFallback());
m.impl("narrow_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("native_norm.out", torch::autograd::autogradNotImplementedFallback());
m.impl("native_norm.ScalarOpt_dim_dtype_out", torch::autograd::autogradNotImplementedFallback());
m.impl("neg",
       TORCH_FN(VariableType::neg)
);
m.impl("new_empty",
       TORCH_FN(VariableType::new_empty)
);
m.impl("new_zeros.out",
       TORCH_FN(VariableType::new_zeros_out_out)
);
m.impl("nextafter_",
       TORCH_FN(VariableType::nextafter_)
);
m.impl("nonzero",
       TORCH_FN(VariableType::nonzero)
);
m.impl("normal_",
       TORCH_FN(VariableType::normal_)
);
m.impl("normal_functional", torch::autograd::autogradNotImplementedFallback());
m.impl("permute_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("pixel_unshuffle",
       TORCH_FN(VariableType::pixel_unshuffle)
);
m.impl("poisson.out", torch::autograd::autogradNotImplementedFallback());
m.impl("pow.Tensor_Tensor",
       TORCH_FN(VariableType::pow_Tensor_Tensor)
);
m.impl("pow.Scalar",
       TORCH_FN(VariableType::pow_Scalar)
);
m.impl("pow.Tensor_Scalar",
       TORCH_FN(VariableType::pow_Tensor_Scalar)
);
m.impl("prod.int_out",
       TORCH_FN(VariableType::prod_out_int_out)
);
m.impl("prod.out", torch::autograd::autogradNotImplementedFallback());
m.impl("qscheme",
       TORCH_FN(VariableType::qscheme)
);
m.impl("quantize_per_tensor.out",
       TORCH_FN(VariableType::quantize_per_tensor_out_out)
);
m.impl("quantize_per_tensor.tensor_qparams_out",
       TORCH_FN(VariableType::quantize_per_tensor_out_tensor_qparams_out)
);
m.impl("quantize_per_tensor.tensors_out",
       TORCH_FN(VariableType::quantize_per_tensor_out_tensors_out)
);
m.impl("quantized_max_pool1d", torch::autograd::autogradNotImplementedFallback());
m.impl("quantized_max_pool1d.out", torch::autograd::autogradNotImplementedFallback());
m.impl("rad2deg.out",
       TORCH_FN(VariableType::rad2deg_out_out)
);
m.impl("randn", torch::autograd::autogradNotImplementedFallback());
m.impl("randn.generator", torch::autograd::autogradNotImplementedFallback());
m.impl("randn.names", torch::autograd::autogradNotImplementedFallback());
m.impl("randn.generator_with_names", torch::autograd::autogradNotImplementedFallback());
m.impl("range.step", torch::autograd::autogradNotImplementedFallback());
m.impl("range", torch::autograd::autogradNotImplementedFallback());
m.impl("reciprocal",
       TORCH_FN(VariableType::reciprocal)
);
m.impl("reflection_pad1d",
       TORCH_FN(VariableType::reflection_pad1d)
);
m.impl("reflection_pad2d.out",
       TORCH_FN(VariableType::reflection_pad2d_out_out)
);
m.impl("reflection_pad3d_backward.grad_input",
       TORCH_FN(VariableType::reflection_pad3d_backward_out_grad_input)
);
m.impl("remainder.Scalar_out",
       TORCH_FN(VariableType::remainder_out_Scalar_out)
);
m.impl("remainder.Tensor_out",
       TORCH_FN(VariableType::remainder_out_Tensor_out)
);
m.impl("remainder.Scalar_Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("repeat_interleave.Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("replication_pad2d.out",
       TORCH_FN(VariableType::replication_pad2d_out_out)
);
m.impl("row_indices",
       TORCH_FN(VariableType::row_indices)
);
m.impl("rrelu_with_noise_backward",
       TORCH_FN(VariableType::rrelu_with_noise_backward)
);
m.impl("scalar_tensor.out", torch::autograd::autogradNotImplementedFallback());
m.impl("segment_reduce",
       TORCH_FN(VariableType::segment_reduce)
);
m.impl("select_copy.int_out", torch::autograd::autogradNotImplementedFallback());
m.impl("select_scatter.out", torch::autograd::autogradNotImplementedFallback());
m.impl("set_.source_Storage",
       TORCH_FN(VariableType::set__source_Storage)
);
m.impl("set_.source_Storage_storage_offset",
       TORCH_FN(VariableType::set__source_Storage_storage_offset)
);
m.impl("set_.source_Tensor",
       TORCH_FN(VariableType::set__source_Tensor)
);
m.impl("set_",
       TORCH_FN(VariableType::set_)
);
m.impl("set.source_Storage_out", torch::autograd::autogradNotImplementedFallback());
m.impl("set.source_Storage_storage_offset_out", torch::autograd::autogradNotImplementedFallback());
m.impl("set.source_Tensor_out", torch::autograd::autogradNotImplementedFallback());
m.impl("set.out", torch::autograd::autogradNotImplementedFallback());
m.impl("sgn",
       TORCH_FN(VariableType::sgn)
);
m.impl("sigmoid_",
       TORCH_FN(VariableType::sigmoid_)
);
m.impl("sigmoid_backward",
       TORCH_FN(VariableType::sigmoid_backward)
);
m.impl("signbit.out",
       TORCH_FN(VariableType::signbit_out_out)
);
m.impl("silu_backward", torch::autograd::autogradNotImplementedFallback());
m.impl("sin",
       TORCH_FN(VariableType::sin)
);
m.impl("sin.out",
       TORCH_FN(VariableType::sin_out_out)
);
m.impl("slice_scatter.out", torch::autograd::autogradNotImplementedFallback());
m.impl("softplus_backward",
       TORCH_FN(VariableType::softplus_backward)
);
m.impl("sparse_mask",
       TORCH_FN(VariableType::sparse_mask)
);
m.impl("sparse_resize", torch::autograd::autogradNotImplementedFallback());
m.impl("sparse_resize_and_clear_", torch::autograd::autogradNotImplementedFallback());
m.impl("special_bessel_j0",
       TORCH_FN(VariableType::special_bessel_j0)
);
m.impl("special_bessel_y0",
       TORCH_FN(VariableType::special_bessel_y0)
);
m.impl("special_chebyshev_polynomial_u",
       TORCH_FN(VariableType::special_chebyshev_polynomial_u)
);
m.impl("special_chebyshev_polynomial_u.x_scalar",
       TORCH_FN(VariableType::special_chebyshev_polynomial_u_x_scalar)
);
m.impl("special_chebyshev_polynomial_u.n_scalar",
       TORCH_FN(VariableType::special_chebyshev_polynomial_u_n_scalar)
);
m.impl("special_entr",
       TORCH_FN(VariableType::special_entr)
);
m.impl("special_hermite_polynomial_he",
       TORCH_FN(VariableType::special_hermite_polynomial_he)
);
m.impl("special_hermite_polynomial_he.x_scalar",
       TORCH_FN(VariableType::special_hermite_polynomial_he_x_scalar)
);
m.impl("special_hermite_polynomial_he.n_scalar",
       TORCH_FN(VariableType::special_hermite_polynomial_he_n_scalar)
);
m.impl("special_i0e.out",
       TORCH_FN(VariableType::special_i0e_out_out)
);
m.impl("special_i1e",
       TORCH_FN(VariableType::special_i1e)
);
m.impl("special_modified_bessel_i1",
       TORCH_FN(VariableType::special_modified_bessel_i1)
);
m.impl("special_modified_bessel_k0.out",
       TORCH_FN(VariableType::special_modified_bessel_k0_out_out)
);
m.impl("special_ndtri",
       TORCH_FN(VariableType::special_ndtri)
);
m.impl("special_scaled_modified_bessel_k0.out",
       TORCH_FN(VariableType::special_scaled_modified_bessel_k0_out_out)
);
m.impl("special_shifted_chebyshev_polynomial_t.out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_t_out_out)
);
m.impl("special_shifted_chebyshev_polynomial_t.x_scalar_out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_t_out_x_scalar_out)
);
m.impl("special_shifted_chebyshev_polynomial_t.n_scalar_out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_t_out_n_scalar_out)
);
m.impl("special_shifted_chebyshev_polynomial_u.out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_u_out_out)
);
m.impl("special_shifted_chebyshev_polynomial_u.x_scalar_out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_u_out_x_scalar_out)
);
m.impl("special_shifted_chebyshev_polynomial_u.n_scalar_out",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_u_out_n_scalar_out)
);
m.impl("special_shifted_chebyshev_polynomial_v",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_v)
);
m.impl("special_shifted_chebyshev_polynomial_v.x_scalar",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_v_x_scalar)
);
m.impl("special_shifted_chebyshev_polynomial_v.n_scalar",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_v_n_scalar)
);
m.impl("special_shifted_chebyshev_polynomial_w",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_w)
);
m.impl("special_shifted_chebyshev_polynomial_w.x_scalar",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_w_x_scalar)
);
m.impl("special_shifted_chebyshev_polynomial_w.n_scalar",
       TORCH_FN(VariableType::special_shifted_chebyshev_polynomial_w_n_scalar)
);
m.impl("special_spherical_bessel_j0",
       TORCH_FN(VariableType::special_spherical_bessel_j0)
);
m.impl("split.Tensor",
       TORCH_FN(VariableType::split_Tensor)
);
m.impl("sqrt_",
       TORCH_FN(VariableType::sqrt_)
);
m.impl("squeeze",
       TORCH_FN(VariableType::squeeze)
);
m.impl("squeeze.dim",
       TORCH_FN(VariableType::squeeze_dim)
);
m.impl("squeeze.dims",
       TORCH_FN(VariableType::squeeze_dims)
);
m.impl("take.out",
       TORCH_FN(VariableType::take_out_out)
);
m.impl("tanh_backward",
       TORCH_FN(VariableType::tanh_backward)
);
m.impl("threshold_backward",
       TORCH_FN(VariableType::threshold_backward)
);
m.impl("to_mkldnn",
       TORCH_FN(VariableType::to_mkldnn)
);
m.impl("to_padded_tensor",
       TORCH_FN(VariableType::to_padded_tensor)
);
m.impl("to_padded_tensor.out", torch::autograd::autogradNotImplementedFallback());
m.impl("tril.out",
       TORCH_FN(VariableType::tril_out_out)
);
m.impl("unbind.int",
       TORCH_FN(VariableType::unbind_int)
);
m.impl("unbind_copy.int_out",
       TORCH_FN(VariableType::unbind_copy_out_int_out)
);
m.impl("unfold_copy.out", torch::autograd::autogradNotImplementedFallback());
m.impl("unique_dim.out", torch::autograd::autogradNotImplementedFallback());
m.impl("unsqueeze_copy",
       TORCH_FN(VariableType::unsqueeze_copy)
);
m.impl("upsample_bilinear2d.out",
       TORCH_FN(VariableType::upsample_bilinear2d_out_out)
);
m.impl("upsample_linear1d_backward.grad_input",
       TORCH_FN(VariableType::upsample_linear1d_backward_out_grad_input)
);
m.impl("upsample_nearest3d",
       TORCH_FN(VariableType::upsample_nearest3d)
);
m.impl("upsample_trilinear3d",
       TORCH_FN(VariableType::upsample_trilinear3d)
);
m.impl("values",
       TORCH_FN(VariableType::values)
);
m.impl("var.correction_out",
       TORCH_FN(VariableType::var_out_correction_out)
);
m.impl("vdot",
       TORCH_FN(VariableType::vdot)
);
m.impl("view_as_complex_copy",
       TORCH_FN(VariableType::view_as_complex_copy)
);
m.impl("where.self",
       TORCH_FN(VariableType::where_self)
);
m.impl("xlogy.OutTensor",
       TORCH_FN(VariableType::xlogy_out_OutTensor)
);
m.impl("xlogy.OutScalar_Self",
       TORCH_FN(VariableType::xlogy_out_OutScalar_Self)
);
m.impl("xlogy.OutScalar_Other",
       TORCH_FN(VariableType::xlogy_out_OutScalar_Other)
);
m.impl("zero_",
       TORCH_FN(VariableType::zero_)
);
m.impl("zeros.names", torch::autograd::autogradNotImplementedFallback());
m.impl("zeros", torch::autograd::autogradNotImplementedFallback());
}

}

} // namespace torch::autograd
